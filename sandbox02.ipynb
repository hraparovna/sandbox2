{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "from joblib import dump, load\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "from math import isnan\n",
    "import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24376\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f282</th>\n",
       "      <th>f283</th>\n",
       "      <th>f284</th>\n",
       "      <th>f285</th>\n",
       "      <th>f286</th>\n",
       "      <th>f287</th>\n",
       "      <th>f288</th>\n",
       "      <th>f289</th>\n",
       "      <th>f290</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>69</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>9500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2014-04-07</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3837949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-05-18</td>\n",
       "      <td>30</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>11</td>\n",
       "      <td>6250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>44</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>2000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>45</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>6700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2014-07-17</td>\n",
       "      <td>116</td>\n",
       "      <td>116.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8523460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id          f1   f2     f3   f4    f5   f6      f7   f8    f9  ...  f282  \\\n",
       "0   1  2014-01-29   69   38.0  7.0  10.0  1.0  2001.0  2.0  11.0  ...     1   \n",
       "1   2  2014-04-07   55    NaN  2.0   1.0  4.0     NaN  2.0  10.0  ...     0   \n",
       "2   3  2012-05-18   30   16.0  2.0   NaN  NaN     NaN  NaN   NaN  ...    46   \n",
       "3   4  2013-02-08   44   43.0  1.0   NaN  NaN     NaN  NaN   NaN  ...    17   \n",
       "4   5  2014-01-10   45   28.0  3.0   5.0  2.0  1960.0  2.0   5.0  ...    20   \n",
       "5   6  2014-07-17  116  116.0  8.0  12.0  6.0  2016.0  3.0   1.0  ...     2   \n",
       "\n",
       "  f283 f284  f285  f286  f287  f288  f289  f290   target  \n",
       "0    0    0     2     8     1     0    19     2  9500000  \n",
       "1    0    0     0     4     0     0     2     0  3837949  \n",
       "2    9    2    11    38     1     8    97    11  6250000  \n",
       "3    4    1    12    12     0     1    55     7  2000000  \n",
       "4    2    0     4    16     1     4    47     5  6700000  \n",
       "5    2    0     1    13     1     0     8     0  8523460  \n",
       "\n",
       "[6 rows x 292 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mlcoursemm2021v2/train.csv')\n",
    "print(len(df))\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f281</th>\n",
       "      <th>f282</th>\n",
       "      <th>f283</th>\n",
       "      <th>f284</th>\n",
       "      <th>f285</th>\n",
       "      <th>f286</th>\n",
       "      <th>f287</th>\n",
       "      <th>f288</th>\n",
       "      <th>f289</th>\n",
       "      <th>f290</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-29</td>\n",
       "      <td>69</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-04-07</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-05-18</td>\n",
       "      <td>30</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>46</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>44</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>45</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014-07-17</td>\n",
       "      <td>116</td>\n",
       "      <td>116.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1   f2     f3   f4    f5   f6      f7   f8    f9  f10  ... f281  \\\n",
       "0  2014-01-29   69   38.0  7.0  10.0  1.0  2001.0  2.0  11.0  2.0  ...    9   \n",
       "1  2014-04-07   55    NaN  2.0   1.0  4.0     NaN  2.0  10.0  NaN  ...    1   \n",
       "2  2012-05-18   30   16.0  2.0   NaN  NaN     NaN  NaN   NaN  NaN  ...  114   \n",
       "3  2013-02-08   44   43.0  1.0   NaN  NaN     NaN  NaN   NaN  NaN  ...   48   \n",
       "4  2014-01-10   45   28.0  3.0   5.0  2.0  1960.0  2.0   5.0  NaN  ...   39   \n",
       "5  2014-07-17  116  116.0  8.0  12.0  6.0  2016.0  3.0   1.0  1.0  ...    6   \n",
       "\n",
       "  f282  f283  f284  f285  f286  f287  f288  f289  f290  \n",
       "0    1     0     0     2     8     1     0    19     2  \n",
       "1    0     0     0     0     4     0     0     2     0  \n",
       "2   46     9     2    11    38     1     8    97    11  \n",
       "3   17     4     1    12    12     0     1    55     7  \n",
       "4   20     2     0     4    16     1     4    47     5  \n",
       "5    2     2     0     1    13     1     0     8     0  \n",
       "\n",
       "[6 rows x 290 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nan = df.iloc[ : ,1 : - 1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train_nan.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.1587e+04, 2.4520e+03, 2.2300e+02, 6.4000e+01, 2.8000e+01,\n",
       "        1.3000e+01, 4.0000e+00, 2.0000e+00, 2.0000e+00, 1.0000e+00]),\n",
       " array([1.00000000e+05, 1.12011112e+07, 2.23022224e+07, 3.34033336e+07,\n",
       "        4.45044448e+07, 5.56055560e+07, 6.67066672e+07, 7.78077784e+07,\n",
       "        8.89088896e+07, 1.00010001e+08, 1.11111112e+08]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f281</th>\n",
       "      <th>f282</th>\n",
       "      <th>f283</th>\n",
       "      <th>f284</th>\n",
       "      <th>f285</th>\n",
       "      <th>f286</th>\n",
       "      <th>f287</th>\n",
       "      <th>f288</th>\n",
       "      <th>f289</th>\n",
       "      <th>f290</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-03-29</td>\n",
       "      <td>63</td>\n",
       "      <td>63.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-19</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-18</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>636</td>\n",
       "      <td>371</td>\n",
       "      <td>141</td>\n",
       "      <td>26</td>\n",
       "      <td>150</td>\n",
       "      <td>249</td>\n",
       "      <td>2</td>\n",
       "      <td>105</td>\n",
       "      <td>203</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-04</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-09-24</td>\n",
       "      <td>127</td>\n",
       "      <td>58.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-03-04</td>\n",
       "      <td>43</td>\n",
       "      <td>29.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           f1   f2    f3    f4    f5   f6      f7   f8    f9  f10  ... f281  \\\n",
       "0  2014-03-29   63  63.0  11.0  17.0  1.0     NaN  2.0   1.0  NaN  ...    5   \n",
       "1  2013-02-19   64   NaN   2.0   NaN  NaN     NaN  NaN   NaN  NaN  ...    5   \n",
       "2  2013-09-18   40   NaN   4.0  17.0  1.0     NaN  1.0   1.0  NaN  ...  636   \n",
       "3  2014-07-04   37   1.0  25.0   1.0  1.0     1.0  1.0   1.0  1.0  ...   28   \n",
       "4  2014-09-24  127  58.0  11.0  20.0  1.0  2006.0  3.0  33.0  4.0  ...   95   \n",
       "5  2013-03-04   43  29.0   9.0   NaN  NaN     NaN  NaN   NaN  NaN  ...   33   \n",
       "\n",
       "  f282  f283  f284  f285  f286  f287  f288  f289  f290  \n",
       "0    1     1     0     2    12     0     1    10     0  \n",
       "1    1     1     0     2    12     0     0     9     0  \n",
       "2  371   141    26   150   249     2   105   203    13  \n",
       "3   17     6     2     4    16     0     0    47     3  \n",
       "4   37     5     1     5    33     1     6    85     5  \n",
       "5   23    15     0     9    20     1     3    68     1  \n",
       "\n",
       "[6 rows x 290 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('mlcoursemm2021v2/test.csv')\n",
    "X_test_nan = df_test.iloc[ : ,1 : ]\n",
    "X_test_nan.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "object\n",
      "object\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "object\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "object\n",
      "object\n",
      "object\n",
      "object\n",
      "object\n",
      "object\n",
      "object\n",
      "object\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "object\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "object\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "object\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "object\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "float64\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_train_nan.columns)):\n",
    "    print(X_train_nan.dtypes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f13</th>\n",
       "      <th>...</th>\n",
       "      <th>f281</th>\n",
       "      <th>f282</th>\n",
       "      <th>f283</th>\n",
       "      <th>f284</th>\n",
       "      <th>f285</th>\n",
       "      <th>f286</th>\n",
       "      <th>f287</th>\n",
       "      <th>f288</th>\n",
       "      <th>f289</th>\n",
       "      <th>f290</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24376.000000</td>\n",
       "      <td>19265.000000</td>\n",
       "      <td>24242.000000</td>\n",
       "      <td>16726.000000</td>\n",
       "      <td>16726.000000</td>\n",
       "      <td>1.349100e+04</td>\n",
       "      <td>16726.000000</td>\n",
       "      <td>16726.000000</td>\n",
       "      <td>13534.000000</td>\n",
       "      <td>2.437600e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "      <td>24376.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.240811</td>\n",
       "      <td>34.492759</td>\n",
       "      <td>7.684061</td>\n",
       "      <td>12.555064</td>\n",
       "      <td>1.834091</td>\n",
       "      <td>3.365373e+03</td>\n",
       "      <td>1.909781</td>\n",
       "      <td>6.229702</td>\n",
       "      <td>2.108911</td>\n",
       "      <td>1.773943e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>63.592837</td>\n",
       "      <td>32.123031</td>\n",
       "      <td>10.798121</td>\n",
       "      <td>1.774532</td>\n",
       "      <td>15.096899</td>\n",
       "      <td>30.352068</td>\n",
       "      <td>0.443879</td>\n",
       "      <td>8.673490</td>\n",
       "      <td>52.838407</td>\n",
       "      <td>5.993354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.939055</td>\n",
       "      <td>57.527014</td>\n",
       "      <td>5.325196</td>\n",
       "      <td>6.765722</td>\n",
       "      <td>1.487298</td>\n",
       "      <td>1.726222e+05</td>\n",
       "      <td>0.846089</td>\n",
       "      <td>22.318067</td>\n",
       "      <td>0.886838</td>\n",
       "      <td>2.081008e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>124.484597</td>\n",
       "      <td>73.664193</td>\n",
       "      <td>28.438960</td>\n",
       "      <td>5.428966</td>\n",
       "      <td>29.221547</td>\n",
       "      <td>47.525878</td>\n",
       "      <td>0.609236</td>\n",
       "      <td>20.653677</td>\n",
       "      <td>46.373546</td>\n",
       "      <td>4.886120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.081628e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.967000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.307411e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.979000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.050803e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.005000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.803644e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5326.000000</td>\n",
       "      <td>7478.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.005201e+07</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.060718e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>641.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 274 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 f2            f3            f4            f5            f6  \\\n",
       "count  24376.000000  19265.000000  24242.000000  16726.000000  16726.000000   \n",
       "mean      54.240811     34.492759      7.684061     12.555064      1.834091   \n",
       "std       40.939055     57.527014      5.325196      6.765722      1.487298   \n",
       "min        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "25%       38.000000     20.000000      3.000000      9.000000      1.000000   \n",
       "50%       49.000000     30.000000      7.000000     12.000000      1.000000   \n",
       "75%       63.000000     43.000000     11.000000     17.000000      2.000000   \n",
       "max     5326.000000   7478.000000     77.000000    117.000000      6.000000   \n",
       "\n",
       "                 f7            f8            f9           f10           f13  \\\n",
       "count  1.349100e+04  16726.000000  16726.000000  13534.000000  2.437600e+04   \n",
       "mean   3.365373e+03      1.909781      6.229702      2.108911  1.773943e+07   \n",
       "std    1.726222e+05      0.846089     22.318067      0.886838  2.081008e+07   \n",
       "min    0.000000e+00      0.000000      0.000000      1.000000  2.081628e+06   \n",
       "25%    1.967000e+03      1.000000      1.000000      1.000000  7.307411e+06   \n",
       "50%    1.979000e+03      2.000000      6.000000      2.000000  1.050803e+07   \n",
       "75%    2.005000e+03      2.000000      9.000000      3.000000  1.803644e+07   \n",
       "max    2.005201e+07     19.000000   2013.000000     33.000000  2.060718e+08   \n",
       "\n",
       "       ...          f281          f282          f283          f284  \\\n",
       "count  ...  24376.000000  24376.000000  24376.000000  24376.000000   \n",
       "mean   ...     63.592837     32.123031     10.798121      1.774532   \n",
       "std    ...    124.484597     73.664193     28.438960      5.428966   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      6.000000      2.000000      1.000000      0.000000   \n",
       "50%    ...     24.000000      8.000000      2.000000      0.000000   \n",
       "75%    ...     51.000000     21.000000      5.000000      0.000000   \n",
       "max    ...    641.000000    377.000000    147.000000     29.000000   \n",
       "\n",
       "               f285          f286          f287          f288          f289  \\\n",
       "count  24376.000000  24376.000000  24376.000000  24376.000000  24376.000000   \n",
       "mean      15.096899     30.352068      0.443879      8.673490     52.838407   \n",
       "std       29.221547     47.525878      0.609236     20.653677     46.373546   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        2.000000      9.000000      0.000000      0.000000     11.000000   \n",
       "50%        7.000000     16.000000      0.000000      2.000000     48.000000   \n",
       "75%       12.000000     28.000000      1.000000      7.000000     76.000000   \n",
       "max      151.000000    250.000000      2.000000    106.000000    218.000000   \n",
       "\n",
       "               f290  \n",
       "count  24376.000000  \n",
       "mean       5.993354  \n",
       "std        4.886120  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        5.000000  \n",
       "75%       10.000000  \n",
       "max       20.000000  \n",
       "\n",
       "[8 rows x 274 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30471, 290)\n"
     ]
    }
   ],
   "source": [
    "X_all_nan = pd.concat((X_train_nan, X_test_nan), axis = 0)\n",
    "print(X_all_nan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan['f1'] = pd.to_datetime(X_all_nan['f1'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>f291</th>\n",
       "      <th>f292</th>\n",
       "      <th>f293</th>\n",
       "      <th>f294</th>\n",
       "      <th>f295</th>\n",
       "      <th>f296</th>\n",
       "      <th>f297</th>\n",
       "      <th>f298</th>\n",
       "      <th>f299</th>\n",
       "      <th>...</th>\n",
       "      <th>f318</th>\n",
       "      <th>f319</th>\n",
       "      <th>f320</th>\n",
       "      <th>f321</th>\n",
       "      <th>f322</th>\n",
       "      <th>f323</th>\n",
       "      <th>f324</th>\n",
       "      <th>f325</th>\n",
       "      <th>f326</th>\n",
       "      <th>f327</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40658</td>\n",
       "      <td>4649</td>\n",
       "      <td>7.54</td>\n",
       "      <td>27.9012</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1158870</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1132982</td>\n",
       "      <td>114.80</td>\n",
       "      <td>111.80</td>\n",
       "      <td>101.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40659</td>\n",
       "      <td>4649</td>\n",
       "      <td>7.54</td>\n",
       "      <td>27.8072</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1158870</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.21</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1132982</td>\n",
       "      <td>115.16</td>\n",
       "      <td>115.42</td>\n",
       "      <td>101.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40660</td>\n",
       "      <td>4649</td>\n",
       "      <td>7.54</td>\n",
       "      <td>27.7391</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1158870</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.16</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1132982</td>\n",
       "      <td>115.13</td>\n",
       "      <td>116.35</td>\n",
       "      <td>102.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40661</td>\n",
       "      <td>4649</td>\n",
       "      <td>7.54</td>\n",
       "      <td>27.5152</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1158870</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.19</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1158870</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1132982</td>\n",
       "      <td>117.36</td>\n",
       "      <td>114.79</td>\n",
       "      <td>101.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40662</td>\n",
       "      <td>4649</td>\n",
       "      <td>7.54</td>\n",
       "      <td>27.3808</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.72</td>\n",
       "      <td>1158870</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.24</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1158870</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1132982</td>\n",
       "      <td>118.70</td>\n",
       "      <td>115.97</td>\n",
       "      <td>99.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40663</td>\n",
       "      <td>4765</td>\n",
       "      <td>7.77</td>\n",
       "      <td>27.3808</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.72</td>\n",
       "      <td>1178915</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.14</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1158870</td>\n",
       "      <td>1139159</td>\n",
       "      <td>1132982</td>\n",
       "      <td>118.70</td>\n",
       "      <td>115.97</td>\n",
       "      <td>99.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    date  f291  f292     f293  f294  f295  f296  f297  f298     f299  ...  \\\n",
       "0  40658  4649  7.54  27.9012   0.1   0.0   7.1   3.1  3.28  1158870  ...   \n",
       "1  40659  4649  7.54  27.8072   0.1   0.0   7.1   3.1  3.42  1158870  ...   \n",
       "2  40660  4649  7.54  27.7391   0.1   0.0   7.1   3.1  3.37  1158870  ...   \n",
       "3  40661  4649  7.54  27.5152   0.1   0.0   7.1   3.1  3.60  1158870  ...   \n",
       "4  40662  4649  7.54  27.3808   0.1   0.0   7.1   3.1  4.72  1158870  ...   \n",
       "5  40663  4765  7.77  27.3808   0.1   0.0   7.2   3.3  4.72  1178915  ...   \n",
       "\n",
       "   f318  f319  f320  f321     f322     f323     f324    f325    f326    f327  \n",
       "0   2.7  3.26  3.35  2.85  1139159  1139159  1132982  114.80  111.80  101.01  \n",
       "1   2.7  3.26  3.21  2.71  1139159  1139159  1132982  115.16  115.42  101.74  \n",
       "2   2.7  3.24  3.16  2.80  1139159  1139159  1132982  115.13  116.35  102.34  \n",
       "3   2.7  3.27  3.19  2.95  1158870  1139159  1132982  117.36  114.79  101.76  \n",
       "4   2.7  3.14  3.24  3.01  1158870  1139159  1132982  118.70  115.97   99.83  \n",
       "5   2.7  3.14  3.22  3.01  1158870  1139159  1132982  118.70  115.97   99.83  \n",
       "\n",
       "[6 rows x 38 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra = pd.read_csv('extra2.csv')\n",
    "print(len(extra))\n",
    "extra.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nan['f1'] = pd.to_datetime(X_train_nan['f1'], format='%Y-%m-%d')\n",
    "X_test_nan['f1'] = pd.to_datetime(X_test_nan['f1'], format='%Y-%m-%d')\n",
    "df['f1'] = pd.to_datetime(df['f1'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "extra['f1'] = pd.TimedeltaIndex(extra['date'] - 2, unit='d') + dt.datetime(1900,1,1)\n",
    "extra = extra.drop(columns = ['date'])\n",
    "extra['f1'] = pd.to_datetime(extra['f1'], format='%Y-%m-%d')\n",
    "X_all_nan = X_all_nan.join(extra.set_index('f1'), on = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan['f0'] = X_all_nan['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = np.sort(pd.Series.unique(X_all_nan['f1']))\n",
    "dates_dummy = np.arange(len(all_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan['f1'] = dates_dummy[[np.where(d == all_dates)[0][0] for d in X_all_nan['f0']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = X_all_nan['f0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan = X_all_nan.drop(columns = ['f0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan['dy'] = np.array([i.year for i in f0]) - ([j for j in X_all_nan['f7']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>...</th>\n",
       "      <th>f319</th>\n",
       "      <th>f320</th>\n",
       "      <th>f321</th>\n",
       "      <th>f322</th>\n",
       "      <th>f323</th>\n",
       "      <th>f324</th>\n",
       "      <th>f325</th>\n",
       "      <th>f326</th>\n",
       "      <th>f327</th>\n",
       "      <th>dy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>726</td>\n",
       "      <td>69</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Investment</td>\n",
       "      <td>...</td>\n",
       "      <td>4.95</td>\n",
       "      <td>6.44</td>\n",
       "      <td>6.26</td>\n",
       "      <td>2648859</td>\n",
       "      <td>2560014</td>\n",
       "      <td>2481457</td>\n",
       "      <td>110.80</td>\n",
       "      <td>111.88</td>\n",
       "      <td>105.24</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>785</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OwnerOccupier</td>\n",
       "      <td>...</td>\n",
       "      <td>7.68</td>\n",
       "      <td>5.75</td>\n",
       "      <td>6.12</td>\n",
       "      <td>2746088</td>\n",
       "      <td>2682513</td>\n",
       "      <td>2648859</td>\n",
       "      <td>108.08</td>\n",
       "      <td>108.63</td>\n",
       "      <td>106.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>207</td>\n",
       "      <td>30</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Investment</td>\n",
       "      <td>...</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.30</td>\n",
       "      <td>4.48</td>\n",
       "      <td>1539880</td>\n",
       "      <td>1501303</td>\n",
       "      <td>1478755</td>\n",
       "      <td>118.76</td>\n",
       "      <td>125.13</td>\n",
       "      <td>125.47</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1  f2    f3   f4    f5   f6   f8    f9  f10            f11  ...  f319  \\\n",
       "0  726  69  38.0  7.0  10.0  1.0  2.0  11.0  2.0     Investment  ...  4.95   \n",
       "1  785  55   NaN  2.0   1.0  4.0  2.0  10.0  NaN  OwnerOccupier  ...  7.68   \n",
       "2  207  30  16.0  2.0   NaN  NaN  NaN   NaN  NaN     Investment  ...  5.74   \n",
       "\n",
       "   f320  f321     f322     f323     f324    f325    f326    f327    dy  \n",
       "0  6.44  6.26  2648859  2560014  2481457  110.80  111.88  105.24  13.0  \n",
       "1  5.75  6.12  2746088  2682513  2648859  108.08  108.63  106.75   NaN  \n",
       "2  5.30  4.48  1539880  1501303  1478755  118.76  125.13  125.47   NaN  \n",
       "\n",
       "[3 rows x 327 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_nan = X_all_nan.drop(columns = ['f7'])\n",
    "X_all_nan.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan = pd.concat((X_all_nan, f0.dt.year, f0.dt.month, f0.dt.quarter, f0.dt.dayofweek), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan.columns.values[-4] = 'year'\n",
    "X_all_nan.columns.values[-3] = 'month'\n",
    "X_all_nan.columns.values[-2] = 'quarter'\n",
    "X_all_nan.columns.values[-1] = 'dayofweek'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy = pd.get_dummies(X_all_nan[X_all_nan.columns[X_all_nan.dtypes == 'object']], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_nan = pd.concat((X_all_nan, df_dummy), axis = 1)\n",
    "X_all_nan = X_all_nan.drop(columns = X_all_nan.columns[X_all_nan.dtypes == 'object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all_nan = X_all_nan.apply(pd.to_numeric, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a602dfbbb0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_all_nan.index, X_all_nan.iloc[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_median = X_train_nan.groupby(['f1']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(X_median.index, X_median['f13']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_median_2 = X_test_nan.groupby(['f1']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(X_median_2.index, X_median_2['f13']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1584x1296 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = X_all_nan.iloc[:,:100].corr(method = 'pearson')\n",
    "f, ax = plt.subplots(figsize=(22, 18))\n",
    "cmap = sns.diverging_palette(10, 275, as_cmap = True)\n",
    "sns.heatmap(corr, cmap = cmap, square = True,\n",
    "            linewidths = 0.5, cbar_kws = {\"shrink\": 0.5}, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f141</th>\n",
       "      <th>f142</th>\n",
       "      <th>f143</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f141</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.829544</td>\n",
       "      <td>0.694233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f142</th>\n",
       "      <td>0.829544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.512168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f143</th>\n",
       "      <td>0.694233</td>\n",
       "      <td>0.512168</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f141      f142      f143\n",
       "f141  1.000000  0.829544  0.694233\n",
       "f142  0.829544  1.000000  0.512168\n",
       "f143  0.694233  0.512168  1.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_nan[['f141', 'f142', 'f143']].corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler().fit(X_all_nan)\n",
    "X_all_nan_transformed = scaler.transform(X_all_nan)\n",
    "X_all_nan_transformed = pd.DataFrame(X_all_nan_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045872</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.458390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.089450</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.5625</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.352489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.236239</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.608696</td>\n",
       "      <td>-0.5625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.669363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.727064</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>-0.6875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.244230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.084862</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.086957</td>\n",
       "      <td>-0.4375</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30466</th>\n",
       "      <td>0.584862</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.245061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30467</th>\n",
       "      <td>0.247706</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.542827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30468</th>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.267973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30469</th>\n",
       "      <td>0.314220</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.521739</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.458390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30470</th>\n",
       "      <td>0.447248</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>-0.5625</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.672560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30471 rows Ã— 478 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1         2       3      4    5    6      7    8    \\\n",
       "0     -0.045872  0.80  0.347826  0.0625 -0.250  0.0  0.0  0.625  0.0   \n",
       "1      0.089450  0.24  0.000000 -0.5625 -1.375  3.0  0.0  0.500  0.0   \n",
       "2     -1.236239 -0.76 -0.608696 -0.5625  0.000  0.0  0.0  0.000  0.0   \n",
       "3     -0.727064 -0.20  0.565217 -0.6875  0.000  0.0  0.0  0.000  0.0   \n",
       "4     -0.084862 -0.16 -0.086957 -0.4375 -0.875  1.0  0.0 -0.125  0.0   \n",
       "...         ...   ...       ...     ...    ...  ...  ...    ...  ...   \n",
       "30466  0.584862  0.40  0.260870  0.9375 -1.375  0.0  0.0  0.375  0.0   \n",
       "30467  0.247706  0.64  0.565217  0.4375  0.000  0.0  1.0  0.125  0.5   \n",
       "30468  0.779817  0.16  0.000000  0.5625  0.000  0.0  0.0  0.250  0.5   \n",
       "30469  0.314220 -0.48 -0.521739 -0.1875 -0.875  0.0 -1.0  0.125  0.5   \n",
       "30470  0.447248  1.00  0.695652 -0.5625  0.500  0.0  1.0  0.500  0.0   \n",
       "\n",
       "            9    ...  468  469  470  471  472  473  474  475  476  477  \n",
       "0      1.458390  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "1      2.352489  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "2     -0.669363  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "3     -0.244230  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "4      0.654158  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "30466 -0.245061  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "30467 -0.542827  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "30468 -0.267973  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "30469  1.458390  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "30470 -0.672560  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[30471 rows x 478 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_transformed = X_all_nan_transformed.fillna(X_all_nan_transformed.median())\n",
    "X_all_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_transformed.columns = X_all_nan.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    f1    0\n",
      "1    f2    0\n",
      "2    f3    6383\n",
      "3    f4    167\n",
      "4    f5    9572\n",
      "5    f6    9572\n",
      "6    f8    9572\n",
      "7    f9    9572\n",
      "8    f10    13559\n",
      "9    f13    0\n",
      "10    f14    0\n",
      "11    f15    0\n",
      "12    f16    0\n",
      "13    f17    0\n",
      "14    f18    6688\n",
      "15    f19    0\n",
      "16    f20    0\n",
      "17    f21    6685\n",
      "18    f22    0\n",
      "19    f23    0\n",
      "20    f24    14441\n",
      "21    f25    0\n",
      "22    f26    0\n",
      "23    f27    0\n",
      "24    f28    0\n",
      "25    f30    0\n",
      "26    f31    0\n",
      "27    f32    0\n",
      "28    f41    0\n",
      "29    f42    0\n",
      "30    f43    0\n",
      "31    f44    0\n",
      "32    f45    0\n",
      "33    f46    0\n",
      "34    f47    0\n",
      "35    f48    0\n",
      "36    f49    0\n",
      "37    f50    0\n",
      "38    f51    0\n",
      "39    f52    0\n",
      "40    f53    0\n",
      "41    f54    0\n",
      "42    f55    0\n",
      "43    f56    0\n",
      "44    f57    0\n",
      "45    f58    0\n",
      "46    f59    0\n",
      "47    f60    0\n",
      "48    f61    0\n",
      "49    f62    0\n",
      "50    f63    0\n",
      "51    f64    0\n",
      "52    f65    0\n",
      "53    f66    0\n",
      "54    f67    0\n",
      "55    f68    4991\n",
      "56    f69    4991\n",
      "57    f70    4991\n",
      "58    f71    4991\n",
      "59    f72    4991\n",
      "60    f73    4991\n",
      "61    f74    4991\n",
      "62    f75    4991\n",
      "63    f76    4991\n",
      "64    f77    4991\n",
      "65    f78    4991\n",
      "66    f79    4991\n",
      "67    f80    4991\n",
      "68    f81    4991\n",
      "69    f82    4991\n",
      "70    f83    4991\n",
      "71    f84    0\n",
      "72    f85    0\n",
      "73    f86    0\n",
      "74    f87    25\n",
      "75    f88    25\n",
      "76    f89    0\n",
      "77    f90    0\n",
      "78    f91    0\n",
      "79    f92    0\n",
      "80    f93    0\n",
      "81    f94    0\n",
      "82    f95    0\n",
      "83    f96    0\n",
      "84    f97    25\n",
      "85    f98    25\n",
      "86    f99    25\n",
      "87    f100    0\n",
      "88    f101    0\n",
      "89    f102    0\n",
      "90    f103    0\n",
      "91    f104    0\n",
      "92    f105    0\n",
      "93    f107    0\n",
      "94    f108    0\n",
      "95    f109    0\n",
      "96    f110    0\n",
      "97    f111    0\n",
      "98    f112    0\n",
      "99    f113    0\n",
      "100    f115    0\n",
      "101    f116    0\n",
      "102    f117    0\n",
      "103    f119    0\n",
      "104    f120    0\n",
      "105    f121    0\n",
      "106    f122    0\n",
      "107    f123    0\n",
      "108    f124    0\n",
      "109    f125    0\n",
      "110    f126    0\n",
      "111    f127    0\n",
      "112    f128    0\n",
      "113    f129    0\n",
      "114    f130    0\n",
      "115    f131    0\n",
      "116    f132    0\n",
      "117    f133    0\n",
      "118    f134    0\n",
      "119    f135    0\n",
      "120    f136    0\n",
      "121    f137    0\n",
      "122    f138    0\n",
      "123    f139    0\n",
      "124    f140    0\n",
      "125    f141    0\n",
      "126    f142    0\n",
      "127    f143    0\n",
      "128    f144    0\n",
      "129    f145    0\n",
      "130    f146    0\n",
      "131    f147    0\n",
      "132    f148    0\n",
      "133    f149    0\n",
      "134    f150    0\n",
      "135    f151    0\n",
      "136    f153    0\n",
      "137    f154    0\n",
      "138    f155    0\n",
      "139    f156    0\n",
      "140    f157    0\n",
      "141    f158    0\n",
      "142    f159    0\n",
      "143    f160    13281\n",
      "144    f161    13281\n",
      "145    f162    13281\n",
      "146    f163    0\n",
      "147    f164    0\n",
      "148    f165    0\n",
      "149    f166    0\n",
      "150    f167    0\n",
      "151    f168    0\n",
      "152    f169    0\n",
      "153    f170    0\n",
      "154    f171    0\n",
      "155    f172    0\n",
      "156    f173    0\n",
      "157    f174    0\n",
      "158    f175    0\n",
      "159    f176    0\n",
      "160    f177    0\n",
      "161    f178    0\n",
      "162    f179    0\n",
      "163    f180    0\n",
      "164    f181    0\n",
      "165    f182    0\n",
      "166    f183    6524\n",
      "167    f184    6524\n",
      "168    f185    6524\n",
      "169    f186    0\n",
      "170    f187    0\n",
      "171    f188    0\n",
      "172    f189    0\n",
      "173    f190    0\n",
      "174    f191    0\n",
      "175    f192    0\n",
      "176    f193    0\n",
      "177    f194    0\n",
      "178    f195    0\n",
      "179    f196    0\n",
      "180    f197    0\n",
      "181    f198    0\n",
      "182    f199    0\n",
      "183    f200    0\n",
      "184    f201    0\n",
      "185    f202    0\n",
      "186    f203    0\n",
      "187    f204    0\n",
      "188    f205    0\n",
      "189    f206    4199\n",
      "190    f207    4199\n",
      "191    f208    4199\n",
      "192    f209    0\n",
      "193    f210    0\n",
      "194    f211    0\n",
      "195    f212    0\n",
      "196    f213    0\n",
      "197    f214    0\n",
      "198    f215    0\n",
      "199    f216    0\n",
      "200    f217    0\n",
      "201    f218    0\n",
      "202    f219    0\n",
      "203    f220    0\n",
      "204    f221    0\n",
      "205    f222    0\n",
      "206    f223    0\n",
      "207    f224    0\n",
      "208    f225    0\n",
      "209    f226    0\n",
      "210    f227    0\n",
      "211    f228    0\n",
      "212    f229    1725\n",
      "213    f230    1725\n",
      "214    f231    1725\n",
      "215    f232    0\n",
      "216    f233    0\n",
      "217    f234    0\n",
      "218    f235    0\n",
      "219    f236    0\n",
      "220    f237    0\n",
      "221    f238    0\n",
      "222    f239    0\n",
      "223    f240    0\n",
      "224    f241    0\n",
      "225    f242    0\n",
      "226    f243    0\n",
      "227    f244    0\n",
      "228    f245    0\n",
      "229    f246    0\n",
      "230    f247    0\n",
      "231    f248    0\n",
      "232    f249    0\n",
      "233    f250    0\n",
      "234    f251    0\n",
      "235    f252    991\n",
      "236    f253    991\n",
      "237    f254    991\n",
      "238    f255    0\n",
      "239    f256    0\n",
      "240    f257    0\n",
      "241    f258    0\n",
      "242    f259    0\n",
      "243    f260    0\n",
      "244    f261    0\n",
      "245    f262    0\n",
      "246    f263    0\n",
      "247    f264    0\n",
      "248    f265    0\n",
      "249    f266    0\n",
      "250    f267    0\n",
      "251    f268    0\n",
      "252    f269    178\n",
      "253    f270    0\n",
      "254    f271    0\n",
      "255    f272    0\n",
      "256    f273    0\n",
      "257    f274    0\n",
      "258    f275    297\n",
      "259    f276    297\n",
      "260    f277    297\n",
      "261    f278    0\n",
      "262    f279    0\n",
      "263    f280    0\n",
      "264    f281    0\n",
      "265    f282    0\n",
      "266    f283    0\n",
      "267    f284    0\n",
      "268    f285    0\n",
      "269    f286    0\n",
      "270    f287    0\n",
      "271    f288    0\n",
      "272    f289    0\n",
      "273    f290    0\n",
      "274    f291    0\n",
      "275    f292    0\n",
      "276    f293    0\n",
      "277    f294    0\n",
      "278    f295    0\n",
      "279    f296    0\n",
      "280    f297    0\n",
      "281    f298    0\n",
      "282    f299    0\n",
      "283    f300    0\n",
      "284    f301    0\n",
      "285    f302    0\n",
      "286    f303    0\n",
      "287    f304    0\n",
      "288    f305    0\n",
      "289    f306    0\n",
      "290    f307    0\n",
      "291    f308    0\n",
      "292    f309    0\n",
      "293    f310    0\n",
      "294    f311    0\n",
      "295    f312    0\n",
      "296    f313    0\n",
      "297    f314    0\n",
      "298    f315    0\n",
      "299    f316    0\n",
      "300    f317    0\n",
      "301    f318    0\n",
      "302    f319    0\n",
      "303    f320    0\n",
      "304    f321    0\n",
      "305    f322    0\n",
      "306    f323    0\n",
      "307    f324    0\n",
      "308    f325    0\n",
      "309    f326    0\n",
      "310    f327    0\n",
      "311    dy    13605\n",
      "312    year    0\n",
      "313    month    0\n",
      "314    quarter    0\n",
      "315    dayofweek    0\n",
      "316    f11_OwnerOccupier    0\n",
      "317    f12_Akademicheskoe    0\n",
      "318    f12_Alekseevskoe    0\n",
      "319    f12_Altuf'evskoe    0\n",
      "320    f12_Arbat    0\n",
      "321    f12_Babushkinskoe    0\n",
      "322    f12_Basmannoe    0\n",
      "323    f12_Begovoe    0\n",
      "324    f12_Beskudnikovskoe    0\n",
      "325    f12_Bibirevo    0\n",
      "326    f12_Birjulevo Vostochnoe    0\n",
      "327    f12_Birjulevo Zapadnoe    0\n",
      "328    f12_Bogorodskoe    0\n",
      "329    f12_Brateevo    0\n",
      "330    f12_Butyrskoe    0\n",
      "331    f12_Caricyno    0\n",
      "332    f12_Cheremushki    0\n",
      "333    f12_Chertanovo Central'noe    0\n",
      "334    f12_Chertanovo Juzhnoe    0\n",
      "335    f12_Chertanovo Severnoe    0\n",
      "336    f12_Danilovskoe    0\n",
      "337    f12_Dmitrovskoe    0\n",
      "338    f12_Donskoe    0\n",
      "339    f12_Dorogomilovo    0\n",
      "340    f12_Filevskij Park    0\n",
      "341    f12_Fili Davydkovo    0\n",
      "342    f12_Gagarinskoe    0\n",
      "343    f12_Gol'janovo    0\n",
      "344    f12_Golovinskoe    0\n",
      "345    f12_Hamovniki    0\n",
      "346    f12_Horoshevo-Mnevniki    0\n",
      "347    f12_Horoshevskoe    0\n",
      "348    f12_Hovrino    0\n",
      "349    f12_Ivanovskoe    0\n",
      "350    f12_Izmajlovo    0\n",
      "351    f12_Jakimanka    0\n",
      "352    f12_Jaroslavskoe    0\n",
      "353    f12_Jasenevo    0\n",
      "354    f12_Juzhnoe Butovo    0\n",
      "355    f12_Juzhnoe Medvedkovo    0\n",
      "356    f12_Juzhnoe Tushino    0\n",
      "357    f12_Juzhnoportovoe    0\n",
      "358    f12_Kapotnja    0\n",
      "359    f12_Kon'kovo    0\n",
      "360    f12_Koptevo    0\n",
      "361    f12_Kosino-Uhtomskoe    0\n",
      "362    f12_Kotlovka    0\n",
      "363    f12_Krasnosel'skoe    0\n",
      "364    f12_Krjukovo    0\n",
      "365    f12_Krylatskoe    0\n",
      "366    f12_Kuncevo    0\n",
      "367    f12_Kurkino    0\n",
      "368    f12_Kuz'minki    0\n",
      "369    f12_Lefortovo    0\n",
      "370    f12_Levoberezhnoe    0\n",
      "371    f12_Lianozovo    0\n",
      "372    f12_Ljublino    0\n",
      "373    f12_Lomonosovskoe    0\n",
      "374    f12_Losinoostrovskoe    0\n",
      "375    f12_Mar'ina Roshha    0\n",
      "376    f12_Mar'ino    0\n",
      "377    f12_Marfino    0\n",
      "378    f12_Matushkino    0\n",
      "379    f12_Meshhanskoe    0\n",
      "380    f12_Metrogorodok    0\n",
      "381    f12_Mitino    0\n",
      "382    f12_Molzhaninovskoe    0\n",
      "383    f12_Moskvorech'e-Saburovo    0\n",
      "384    f12_Mozhajskoe    0\n",
      "385    f12_Nagatino-Sadovniki    0\n",
      "386    f12_Nagatinskij Zaton    0\n",
      "387    f12_Nagornoe    0\n",
      "388    f12_Nekrasovka    0\n",
      "389    f12_Nizhegorodskoe    0\n",
      "390    f12_Novo-Peredelkino    0\n",
      "391    f12_Novogireevo    0\n",
      "392    f12_Novokosino    0\n",
      "393    f12_Obruchevskoe    0\n",
      "394    f12_Ochakovo-Matveevskoe    0\n",
      "395    f12_Orehovo-Borisovo Juzhnoe    0\n",
      "396    f12_Orehovo-Borisovo Severnoe    0\n",
      "397    f12_Ostankinskoe    0\n",
      "398    f12_Otradnoe    0\n",
      "399    f12_Pechatniki    0\n",
      "400    f12_Perovo    0\n",
      "401    f12_Pokrovskoe Streshnevo    0\n",
      "402    f12_Poselenie Desjonovskoe    0\n",
      "403    f12_Poselenie Filimonkovskoe    0\n",
      "404    f12_Poselenie Kievskij    0\n",
      "405    f12_Poselenie Klenovskoe    0\n",
      "406    f12_Poselenie Kokoshkino    0\n",
      "407    f12_Poselenie Krasnopahorskoe    0\n",
      "408    f12_Poselenie Marushkinskoe    0\n",
      "409    f12_Poselenie Mihajlovo-Jarcevskoe    0\n",
      "410    f12_Poselenie Moskovskij    0\n",
      "411    f12_Poselenie Mosrentgen    0\n",
      "412    f12_Poselenie Novofedorovskoe    0\n",
      "413    f12_Poselenie Pervomajskoe    0\n",
      "414    f12_Poselenie Rjazanovskoe    0\n",
      "415    f12_Poselenie Rogovskoe    0\n",
      "416    f12_Poselenie Shhapovskoe    0\n",
      "417    f12_Poselenie Shherbinka    0\n",
      "418    f12_Poselenie Sosenskoe    0\n",
      "419    f12_Poselenie Vnukovskoe    0\n",
      "420    f12_Poselenie Voronovskoe    0\n",
      "421    f12_Poselenie Voskresenskoe    0\n",
      "422    f12_Preobrazhenskoe    0\n",
      "423    f12_Presnenskoe    0\n",
      "424    f12_Prospekt Vernadskogo    0\n",
      "425    f12_Ramenki    0\n",
      "426    f12_Rjazanskij    0\n",
      "427    f12_Rostokino    0\n",
      "428    f12_Savelki    0\n",
      "429    f12_Savelovskoe    0\n",
      "430    f12_Severnoe    0\n",
      "431    f12_Severnoe Butovo    0\n",
      "432    f12_Severnoe Izmajlovo    0\n",
      "433    f12_Severnoe Medvedkovo    0\n",
      "434    f12_Severnoe Tushino    0\n",
      "435    f12_Shhukino    0\n",
      "436    f12_Silino    0\n",
      "437    f12_Sokol    0\n",
      "438    f12_Sokol'niki    0\n",
      "439    f12_Sokolinaja Gora    0\n",
      "440    f12_Solncevo    0\n",
      "441    f12_Staroe Krjukovo    0\n",
      "442    f12_Strogino    0\n",
      "443    f12_Sviblovo    0\n",
      "444    f12_Taganskoe    0\n",
      "445    f12_Tekstil'shhiki    0\n",
      "446    f12_Teplyj Stan    0\n",
      "447    f12_Timirjazevskoe    0\n",
      "448    f12_Troickij okrug    0\n",
      "449    f12_Troparevo-Nikulino    0\n",
      "450    f12_Tverskoe    0\n",
      "451    f12_Veshnjaki    0\n",
      "452    f12_Vnukovo    0\n",
      "453    f12_Vojkovskoe    0\n",
      "454    f12_Vostochnoe    0\n",
      "455    f12_Vostochnoe Degunino    0\n",
      "456    f12_Vostochnoe Izmajlovo    0\n",
      "457    f12_Vyhino-Zhulebino    0\n",
      "458    f12_Zamoskvorech'e    0\n",
      "459    f12_Zapadnoe Degunino    0\n",
      "460    f12_Zjablikovo    0\n",
      "461    f12_Zjuzino    0\n",
      "462    f29_yes    0\n",
      "463    f33_yes    0\n",
      "464    f34_yes    0\n",
      "465    f35_yes    0\n",
      "466    f36_yes    0\n",
      "467    f37_yes    0\n",
      "468    f38_yes    0\n",
      "469    f39_yes    0\n",
      "470    f40_yes    0\n",
      "471    f106_yes    0\n",
      "472    f114_yes    0\n",
      "473    f118_yes    0\n",
      "474    f152_good    0\n",
      "475    f152_no data    0\n",
      "476    f152_poor    0\n",
      "477    f152_satisfactory    0\n"
     ]
    }
   ],
   "source": [
    "nan_values = X_all_nan.isna().sum()\n",
    "i=0\n",
    "for n in nan_values:\n",
    "    print(i, '  ', X_all_nan.columns[i] ,'  ',n)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f3', 'f4', 'f5', 'f6', 'f8', 'f9', 'f10', 'f18', 'f21', 'f24', 'f68',\n",
       "       'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78',\n",
       "       'f79', 'f80', 'f81', 'f82', 'f83', 'f87', 'f88', 'f97', 'f98', 'f99',\n",
       "       'f160', 'f161', 'f162', 'f183', 'f184', 'f185', 'f206', 'f207', 'f208',\n",
       "       'f229', 'f230', 'f231', 'f252', 'f253', 'f254', 'f269', 'f275', 'f276',\n",
       "       'f277', 'dy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_nan.columns[nan_values != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24376"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...   41   42   43   44  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "    45   46   47   48   49   50  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_factors = np.zeros((len(X_all_nan), len(X_all_nan.columns[nan_values != 0])))\n",
    "for i in range(len(X_all_nan)):\n",
    "    for j in range(len(X_all_nan.columns[nan_values != 0])):\n",
    "        if isnan(X_all_nan.iloc[i,j]):\n",
    "            nan_factors[i,j] = 1\n",
    "\n",
    "nan_factors = pd.DataFrame(nan_factors)\n",
    "nan_factors.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_factors = nan_factors.loc[:, (nan_factors != 0).any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_factors.index = X_all_transformed.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f13</th>\n",
       "      <th>...</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>14</th>\n",
       "      <th>17</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045872</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.458390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.089450</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.5625</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.352489</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.236239</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-0.608696</td>\n",
       "      <td>-0.5625</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.669363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.727064</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>-0.6875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.244230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.084862</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.086957</td>\n",
       "      <td>-0.4375</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.286697</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.739130</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>5.244131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 488 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1    f2        f3      f4     f5   f6   f8     f9  f10       f13  \\\n",
       "0 -0.045872  0.80  0.347826  0.0625 -0.250  0.0  0.0  0.625  0.0  1.458390   \n",
       "1  0.089450  0.24  0.000000 -0.5625 -1.375  3.0  0.0  0.500  0.0  2.352489   \n",
       "2 -1.236239 -0.76 -0.608696 -0.5625  0.000  0.0  0.0  0.000  0.0 -0.669363   \n",
       "3 -0.727064 -0.20  0.565217 -0.6875  0.000  0.0  0.0  0.000  0.0 -0.244230   \n",
       "4 -0.084862 -0.16 -0.086957 -0.4375 -0.875  1.0  0.0 -0.125  0.0  0.654158   \n",
       "5  0.286697  2.68  3.739130  0.1875  0.000  5.0  1.0 -0.625 -0.5  5.244131   \n",
       "\n",
       "   ...    2    3    4    5    6    7    8   14   17   20  \n",
       "0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "1  ...  1.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  \n",
       "2  ...  0.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  \n",
       "3  ...  0.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  1.0  \n",
       "4  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  1.0  \n",
       "\n",
       "[6 rows x 488 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_transformed_expanded = pd.concat((X_all_transformed, nan_factors), axis = 1)\n",
    "X_all_transformed_expanded.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f8', 'f9', 'f10', 'f13',\n",
       "       ...\n",
       "       'f323', 'f324', 'f325', 'f326', 'f327', 'dy', 'year', 'month',\n",
       "       'quarter', 'dayofweek'],\n",
       "      dtype='object', length=316)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_transformed_expanded.columns[:316]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_transformed_expanded = pd.concat((X_all_transformed_expanded, X_all_transformed_expanded[X_all_transformed_expanded.columns[:316]].apply(lambda x: x ** 2)), axis = 1)\n",
    "X_all_transformed_expanded.columns.values[-316:] = X_all_transformed_expanded.columns[-316:] + 'sqr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all_transformed_expanded = pd.concat((X_all_transformed_expanded, X_all_transformed_expanded[X_all_transformed_expanded.columns[:316]].apply(lambda x: (x - min(x)) ** 0.5)), axis = 1)\n",
    "X_all_transformed_expanded.columns.values[-316:] = X_all_transformed_expanded.columns[-316:] + 'sqrt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all_transformed_expanded = pd.concat((X_all_transformed_expanded, X_all_transformed_expanded[X_all_transformed_expanded.columns[:316]].apply(lambda x: np.log(x - min(x) + 1))), axis = 1)\n",
    "X_all_transformed_expanded.columns.values[-316:] = X_all_transformed_expanded.columns[-316:] + 'log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_all_transformed_expanded = pd.concat((X_all_transformed_expanded, X_all_transformed_expanded[X_all_transformed_expanded.columns[:316]].apply(lambda x: np.exp(x))), axis = 1)\n",
    "#X_all_transformed_expanded.columns.values[-316:] = X_all_transformed_expanded.columns[-316:] + 'exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_all_transformed_expanded = pd.concat((X_all_transformed_expanded, X_all_transformed_expanded[X_all_transformed_expanded.columns[:316]].apply(lambda x: np.exp(-x**2))), axis = 1)\n",
    "X_all_transformed_expanded.columns.values[-316:] = X_all_transformed_expanded.columns[-316:] + 'exp2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_transformed_expanded = pd.concat((X_all_transformed_expanded, X_all_transformed_expanded[X_all_transformed_expanded.columns[:316]].apply(lambda x: (x - min(x) + 1)**(-1))), axis = 1)\n",
    "X_all_transformed_expanded.columns.values[-316:] = X_all_transformed_expanded.columns[-316:] + 'hip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_all_transformed_expanded = pd.concat((X_all_transformed_expanded, X_all_transformed_expanded[X_all_transformed_expanded.columns[:316]].apply(lambda x: x ** 3)), axis = 1)\n",
    "#X_all_transformed_expanded.columns.values[-316:] = X_all_transformed_expanded.columns[-316:] + 'cube'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30471, 2068)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_all_transformed_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24376, 2068)\n",
      "(6095, 2068)\n"
     ]
    }
   ],
   "source": [
    "X = X_all_transformed_expanded.loc[ : 24375, : ]\n",
    "X_test = X_all_transformed_expanded.loc[24376 : , : ]\n",
    "X_test.reset_index(inplace = True, drop = True)\n",
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "#from sklearn.linear_model import QuantileRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor   \n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.isotonic import IsotonicRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = make_scorer(mean_squared_error, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clean = np.log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y_clean, test_size = 6100, random_state = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_models(model, hyps, name_save):\n",
    "    md = GridSearchCV(model, \\\n",
    "                     hyps, cv = 3, scoring = score, n_jobs = 7, verbose = 4)\n",
    "    md_cv = md.fit(X_train, y_train)\n",
    "\n",
    "    print(md.best_score_)\n",
    "    print(' ')\n",
    "    print(md.best_params_)\n",
    "    dump(md_cv, name_save)\n",
    "    return md_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_huber = {'epsilon': np.arange(1.5, 2, 0.2), \n",
    "              'alpha': [10, 20, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huver_cv = cv_models(HuberRegressor(max_iter=20000), hyps_huber, 'huver_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "huver_cv = load('huver_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_ridge = {'alpha': [10, 100, 10000, 20000, 30000, 40000, 50000, 60000, 70000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = cv_models(Ridge(max_iter=20000), hyps_ridge, 'ridge_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = load('ridge_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_lsvr = {'C': [0.1, 1, 10, 100, 1000, 10000, 50000, 100000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvr_cv = cv_models(LinearSVR(random_state = 10, dual = False, loss='squared_epsilon_insensitive'), hyps_lsvr, 'lsvr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvr_cv = load('lsvr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_svr = {'kernel': ['poly', 'rbf'],\n",
    "            'C': [0.1, 1, 10, 100, 1000, 10000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svr_cv = cv_models(SVR(), hyps_svr, 'svr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_cv = load('svr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_knr = {'n_neighbors': [5, 10, 15, 20],\n",
    "            'weights': ['uniform', 'distance']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knr_cv = cv_models(KNeighborsRegressor(algorithm = 'brute'), hyps_knr, 'knr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "knr_cv = load('knr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_dtr = {'max_depth': [3, 5, 10, 20, None],\n",
    "           'min_samples_split': [2, 200, 300]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtr_cv = cv_models(DecisionTreeRegressor(random_state = 40), hyps_dtr, 'dtr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr_cv = load('dtr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_rfr = {'max_samples': [0.8, 0.9, 0.95, 1.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfr_cv = cv_models(RandomForestRegressor(n_estimators = 100, \\\n",
    "                                         n_jobs = 4, random_state = 40, verbose = 4), hyps_rfr, 'rfr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_cv = load('rfr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_etr = {'max_depth': [5, 10, None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efr_cv = cv_models(ExtraTreesRegressor(n_estimators = 100, \\\n",
    "                                         n_jobs = 4, random_state = 50, verbose = 4), hyps_etr, 'efr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "efr_cv = load('efr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_gbr = {'learning_rate': [0.01, 0.1, 0.2],\n",
    "           'subsample': [0.3, 0.5, 0.8, 1.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbr_cv = cv_models(GradientBoostingRegressor(n_estimators = 100, validation_fraction = 0.1, max_depth = 5, \\\n",
    "                                             random_state = 50, verbose = 4), hyps_gbr, 'gbr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_cv = load('gbr_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps_xgb = {'learning_rate': [0.1, 0.2, 0.3],\n",
    "           'colsample_bytree': [0.3],\n",
    "           'alpha': [0, 10, 100, 1000],\n",
    "           'lambda': [0, 10, 100, 1000],\n",
    "            'subsample': [0.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_cv = cv_models(xgb.XGBRegressor(n_estimators = 100), hyps_xgb, 'xgb_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv = load('xgb_cv.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_model = HuberRegressor(max_iter=20000, **huver_cv.best_params_)\n",
    "ridge_model = Ridge(max_iter=20000, **ridge_cv.best_params_)\n",
    "lsvr_model = LinearSVR(random_state = 10, dual = False, loss='squared_epsilon_insensitive', **lsvr_cv.best_params_)\n",
    "svr_model = SVR(**svr_cv.best_params_, max_iter = 1000)\n",
    "knr_model = KNeighborsRegressor(algorithm = 'brute', **knr_cv.best_params_)\n",
    "dtr_model = DecisionTreeRegressor(random_state = 40, **dtr_cv.best_params_)\n",
    "rfr_model = RandomForestRegressor(n_estimators = 100, n_jobs = 4, random_state = 40,\\\n",
    "                                  verbose = 4, **rfr_cv.best_params_, max_features = 50)\n",
    "efr_model = ExtraTreesRegressor(n_estimators = 100, n_jobs = 4, random_state = 50, \\\n",
    "                                verbose = 4,  **efr_cv.best_params_, max_features = 50)\n",
    "gbr_model = GradientBoostingRegressor(n_estimators = 100, validation_fraction = 0.1,  max_depth = 5, \\\n",
    "                                             random_state = 50, verbose = 4, max_features = 50,  **gbr_cv.best_params_)\n",
    "xgb_model = xgb.XGBRegressor(n_estimators = 100, **xgb_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_huber_model = AdaBoostRegressor(base_estimator = huber_model, n_estimators = 50, random_state = 71)\n",
    "ada_huber_model = ada_huber_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_huber_model, 'ada_huber_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_ridge_model = AdaBoostRegressor(base_estimator = ridge_model, n_estimators = 50, random_state = 72)\n",
    "ada_ridge_model = ada_ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_ridge_model, 'ada_ridge_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_lsvr_model = AdaBoostRegressor(base_estimator = lsvr_model, n_estimators = 50, random_state = 73)\n",
    "ada_lsvr_model = ada_lsvr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_lsvr_model, 'ada_lsvr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ada_knr_model = AdaBoostRegressor(base_estimator = knr_model, n_estimators = 10, random_state = 75)\n",
    "ada_knr_model = ada_knr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_knr_model, 'ada_knr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_dtr_model = AdaBoostRegressor(base_estimator = dtr_model, n_estimators = 50, random_state = 76)\n",
    "ada_dtr_model = ada_dtr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_dtr_model, 'ada_dtr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_rfr_model = AdaBoostRegressor(base_estimator = rfr_model, n_estimators = 10, random_state = 77)\n",
    "ada_rfr_model = ada_rfr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_rfr_model, 'ada_rfr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_efr_model = AdaBoostRegressor(base_estimator = efr_model, n_estimators = 10, random_state = 78)\n",
    "ada_efr_model = ada_efr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_efr_model, 'ada_efr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_gbr_model = AdaBoostRegressor(base_estimator = gbr_model, n_estimators = 10, random_state = 79)\n",
    "ada_gbr_model = ada_gbr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_gbr_model, 'ada_gbr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_xgb_model = AdaBoostRegressor(base_estimator = xgb_model, n_estimators = 10, random_state = 80)\n",
    "ada_xgb_model = ada_xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(ada_xgb_model, 'ada_xgb_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = svr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_ridge_model = BaggingRegressor(base_estimator = ridge_model, n_estimators = 50,\\\n",
    "                                   verbose = 4, n_jobs = 6, random_state = 82)\n",
    "bag_ridge_model = bag_ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(bag_ridge_model, 'bag_ridge_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_lsvr_model = BaggingRegressor(base_estimator = lsvr_model, n_estimators = 50,\\\n",
    "                                   verbose = 4, n_jobs = 6, random_state = 83)\n",
    "bag_lsvr_model = bag_lsvr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(bag_lsvr_model, 'bag_lsvr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bag_knr_model = BaggingRegressor(base_estimator = knr_model, n_estimators = 10,\\\n",
    "                                verbose = 4, n_jobs = 6, random_state = 85)\n",
    "bag_knr_model = bag_knr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(bag_knr_model, 'bag_knr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_dtr_model = BaggingRegressor(base_estimator = dtr_model, n_estimators = 50,\\\n",
    "                                 verbose = 4, n_jobs = 6, random_state = 86)\n",
    "bag_dtr_model = bag_dtr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(bag_dtr_model, 'bag_dtr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_rfr_model = BaggingRegressor(base_estimator = rfr_model, n_estimators = 10,\\\n",
    "                                 verbose = 4, n_jobs = 6, random_state = 87)\n",
    "bag_rfr_model = bag_rfr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(bag_rfr_model, 'bag_rfr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_efr_model = BaggingRegressor(base_estimator = efr_model, n_estimators = 10,\\\n",
    "                                   verbose = 4, n_jobs = 6, random_state = 88)\n",
    "bag_efr_model = bag_efr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(bag_efr_model, 'bag_efr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_gbr_model = BaggingRegressor(base_estimator = gbr_model, n_estimators = 10,\\\n",
    "                                verbose = 4, n_jobs = 6, random_state = 89)\n",
    "bag_gbr_model = bag_gbr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(bag_gbr_model, 'bag_gbr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_xgb_model = BaggingRegressor(base_estimator = xgb_model, n_estimators = 10,\\\n",
    "                                verbose = 4, n_jobs = 6, random_state = 90)\n",
    "bag_xgb_model = bag_xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(bag_xgb_model, 'bag_xgb_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('ridge', ridge_model), \\\n",
    "                                \n",
    "                                ('lsvr', lsvr_model), \\\n",
    "                                      \n",
    "                                ('svr', svr_model), \\\n",
    "                                \n",
    "                                ('knr', knr_model), \\\n",
    "                                \n",
    "                                ('dtr', dtr_model), \\\n",
    "                                \n",
    "                                ('rfr', rfr_model), \\\n",
    "                                                                \n",
    "                                ('efr', efr_model), \\\n",
    "                                      \n",
    "                                ('gbr', gbr_model), \\\n",
    "             ('xgb', xgb_model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrens = VotingRegressor(estimators,\\\n",
    "                                verbose = 4, n_jobs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrens = vrens.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(vrens, 'vrens.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacks = StackingRegressor(estimators, cv = 3, \\\n",
    "                                verbose = 4, n_jobs = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacks = stacks.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(stacks, 'stacks.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_train = [huver_cv, ridge_cv, lsvr_cv, svr_cv, knr_cv, dtr_cv, rfr_cv, efr_cv, gbr_cv, xgb_cv, \\\n",
    "                   ada_huber_model, ada_ridge_model, ada_lsvr_model, ada_knr_model, ada_dtr_model, \\\n",
    "                    ada_rfr_model, ada_efr_model, ada_gbr_model, ada_xgb_model, \\\n",
    "                   bag_huber_model, bag_ridge_model, bag_lsvr_model, bag_knr_model, bag_dtr_model, \\\n",
    "                    bag_rfr_model, bag_efr_model, bag_gbr_model, bag_xgb_model,\\\n",
    "                   vrens, stacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_val_preds = []\n",
    "i = 0 \n",
    "for m in all_models_train:\n",
    "    print(i, '  ', m)\n",
    "    i += 1\n",
    "    y_val_preds.append(m.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(y_val_preds, 'y_val_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_preds = load('y_val_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_preds_array = np.zeros((6100, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    y_val_preds_array[:,i] = y_val_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_weights(w):\n",
    "        y_to_do = y_val_preds_array\n",
    "        y_true = y_val\n",
    "        w_arit = w\n",
    "        #w_geom =  w[30:60]\n",
    "        #w_harm = w[60:90]\n",
    "        #w1 = w[90]\n",
    "        #w2 = w[91]\n",
    "        #w3 = w[93]\n",
    "        arit_preds = np.mean(w_arit * y_to_do, axis = 1)\n",
    "        #geom_preds = np.mean(w_geom * y_to_do, axis = 0)\n",
    "        #harm_preds = np.mean(harm_preds * y_to_do, axis = 0)\n",
    "        #all_pred = w1 * arit_preds + w2 * geom_preds + w3 * harm_preds\n",
    "        target_score = mean_squared_error(y_true, arit_preds)\n",
    "        return target_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mealpy.bio_based import SMA\n",
    "from mealpy.evolutionary_based import DE\n",
    "from mealpy.math_based import HC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dict1 = {\n",
    "    \"obj_func\": get_weights,\n",
    "    \"lb\": [-10.0, ] * 30,\n",
    "    \"ub\": [10.0, ] * 30,\n",
    "    \"minmax\": \"min\",\n",
    "    \"verbose\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optmod = SMA.BaseSMA(problem_dict1, epoch = 1000, pop_size = 1000)\n",
    "optmod.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = optmod.solution[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_huber = HuberRegressor(max_iter=20000, **huver_cv.best_params_)\n",
    "final_huber = final_huber.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_huber.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_huber, 'final_huber.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ridge = Ridge(max_iter=20000, **ridge_cv.best_params_)\n",
    "final_ridge = final_ridge.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_ridge.joblib']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_ridge, 'final_ridge.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lsvr = LinearSVR(random_state = 10, dual = False, loss='squared_epsilon_insensitive', **lsvr_cv.best_params_)\n",
    "final_lsvr = final_lsvr.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_lsvr.joblib']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_lsvr, 'final_lsvr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svr = SVR(**svr_cv.best_params_)\n",
    "final_svr = final_svr.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_svr.joblib']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_svr, 'final_svr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knr = KNeighborsRegressor(algorithm = 'brute', **knr_cv.best_params_)\n",
    "final_knr = final_knr.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_knr.joblib']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_knr, 'final_knr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dtr = DecisionTreeRegressor(random_state = 40, **dtr_cv.best_params_)\n",
    "final_dtr = final_dtr.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_dtr.joblib']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_dtr, 'final_dtr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100building tree 3 of 100\n",
      "\n",
      "building tree 4 of 100\n",
      "\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed: 20.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed: 21.9min finished\n"
     ]
    }
   ],
   "source": [
    "final_rfr = RandomForestRegressor(n_estimators = 100, n_jobs = 4, random_state = 40,\\\n",
    "                                  verbose = 4, **rfr_cv.best_params_)\n",
    "final_rfr = final_rfr.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_rfr.joblib']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_rfr, 'final_rfr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed: 12.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed: 13.1min finished\n"
     ]
    }
   ],
   "source": [
    "final_efr = ExtraTreesRegressor(n_estimators = 100, n_jobs = 4, random_state = 50, \\\n",
    "                                verbose = 4,  **efr_cv.best_params_)\n",
    "final_efr = final_efr.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_efr.joblib']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_efr, 'final_efr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3397           13.63m\n",
      "         2           0.3189           13.94m\n",
      "         3           0.3017           13.64m\n",
      "         4           0.2873           13.56m\n",
      "         5           0.2751           13.44m\n",
      "         6           0.2651           13.19m\n",
      "         7           0.2566           12.86m\n",
      "         8           0.2493           12.76m\n",
      "         9           0.2432           12.63m\n",
      "        10           0.2377           12.60m\n",
      "        11           0.2328           12.82m\n",
      "        12           0.2285           13.26m\n",
      "        13           0.2250           13.15m\n",
      "        14           0.2218           12.83m\n",
      "        15           0.2188           12.55m\n",
      "        16           0.2159           12.30m\n",
      "        17           0.2136           12.07m\n",
      "        18           0.2115           11.85m\n",
      "        19           0.2095           11.63m\n",
      "        20           0.2075           11.46m\n",
      "        21           0.2057           11.25m\n",
      "        22           0.2043           11.09m\n",
      "        23           0.2028           11.01m\n",
      "        24           0.2015           10.84m\n",
      "        25           0.2002           10.68m\n",
      "        26           0.1989           10.60m\n",
      "        27           0.1979           10.45m\n",
      "        28           0.1967           10.29m\n",
      "        29           0.1952           10.16m\n",
      "        30           0.1941           10.02m\n",
      "        31           0.1930            9.90m\n",
      "        32           0.1918            9.78m\n",
      "        33           0.1910            9.67m\n",
      "        34           0.1900            9.53m\n",
      "        35           0.1893            9.37m\n",
      "        36           0.1884            9.26m\n",
      "        37           0.1877            9.14m\n",
      "        38           0.1867            9.02m\n",
      "        39           0.1861            8.93m\n",
      "        40           0.1852            8.83m\n",
      "        41           0.1842            8.67m\n",
      "        42           0.1832            8.51m\n",
      "        43           0.1827            8.37m\n",
      "        44           0.1820            8.29m\n",
      "        45           0.1813            8.14m\n",
      "        46           0.1809            7.99m\n",
      "        47           0.1799            7.87m\n",
      "        48           0.1795            7.75m\n",
      "        49           0.1790            7.58m\n",
      "        50           0.1785            7.43m\n",
      "        51           0.1780            7.28m\n",
      "        52           0.1776            7.13m\n",
      "        53           0.1773            6.96m\n",
      "        54           0.1766            6.79m\n",
      "        55           0.1758            6.62m\n",
      "        56           0.1755            6.46m\n",
      "        57           0.1747            6.29m\n",
      "        58           0.1743            6.13m\n",
      "        59           0.1736            5.97m\n",
      "        60           0.1733            5.81m\n",
      "        61           0.1730            5.65m\n",
      "        62           0.1723            5.49m\n",
      "        63           0.1717            5.33m\n",
      "        64           0.1713            5.18m\n",
      "        65           0.1704            5.02m\n",
      "        66           0.1701            4.88m\n",
      "        67           0.1698            4.75m\n",
      "        68           0.1694            4.60m\n",
      "        69           0.1690            4.46m\n",
      "        70           0.1688            4.32m\n",
      "        71           0.1684            4.18m\n",
      "        72           0.1682            4.03m\n",
      "        73           0.1677            3.88m\n",
      "        74           0.1673            3.74m\n",
      "        75           0.1670            3.60m\n",
      "        76           0.1667            3.45m\n",
      "        77           0.1663            3.30m\n",
      "        78           0.1653            3.16m\n",
      "        79           0.1648            3.02m\n",
      "        80           0.1646            2.88m\n",
      "        81           0.1643            2.74m\n",
      "        82           0.1635            2.60m\n",
      "        83           0.1632            2.46m\n",
      "        84           0.1627            2.32m\n",
      "        85           0.1619            2.17m\n",
      "        86           0.1616            2.03m\n",
      "        87           0.1614            1.89m\n",
      "        88           0.1611            1.74m\n",
      "        89           0.1608            1.60m\n",
      "        90           0.1606            1.45m\n",
      "        91           0.1597            1.31m\n",
      "        92           0.1594            1.16m\n",
      "        93           0.1591            1.02m\n",
      "        94           0.1585           52.67s\n",
      "        95           0.1583           43.98s\n",
      "        96           0.1578           35.21s\n",
      "        97           0.1574           26.41s\n",
      "        98           0.1570           17.66s\n",
      "        99           0.1566            8.83s\n",
      "       100           0.1562            0.00s\n"
     ]
    }
   ],
   "source": [
    "final_gbr = GradientBoostingRegressor(n_estimators = 100, validation_fraction = 0.1,  max_depth = 5, \\\n",
    "                                             random_state = 50, verbose = 4,  **gbr_cv.best_params_)\n",
    "final_gbr = final_gbr.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_gbr.joblib']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_gbr, 'final_gbr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_xgb = xgb.XGBRegressor(n_estimators = 100, **xgb_cv.best_params_)\n",
    "final_xgb = final_xgb.fit(X, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_xgb.joblib']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(final_xgb, 'final_xgb.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_ada_huber.joblib']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_huber = AdaBoostRegressor(base_estimator = huber_model, n_estimators = 50, random_state = 71)\n",
    "final_ada_huber = final_ada_huber.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_huber, 'final_ada_huber.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_ada_ridge.joblib']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_ridge = AdaBoostRegressor(base_estimator = ridge_model, n_estimators = 50, random_state = 72)\n",
    "final_ada_ridge = final_ada_ridge.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_ridge, 'final_ada_ridge.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_ada_lsvr.joblib']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_lsvr = AdaBoostRegressor(base_estimator = lsvr_model, n_estimators = 50, random_state = 73)\n",
    "final_ada_lsvr = final_ada_lsvr.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_lsvr, 'final_ada_lsvr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_ada_knr.joblib']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_knr = AdaBoostRegressor(base_estimator = knr_model, n_estimators = 10, random_state = 75)\n",
    "final_ada_knr = final_ada_knr.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_knr, 'final_ada_knr.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_ada_dtr.joblib']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_dtr = AdaBoostRegressor(base_estimator = dtr_model, n_estimators = 50, random_state = 76)\n",
    "final_ada_dtr = final_ada_dtr.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_dtr, 'final_ada_dtr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   25.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   28.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   26.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   29.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    6.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   29.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   32.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100building tree 30 of 100\n",
      "\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   29.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   31.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   29.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   32.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   26.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   28.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   26.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   28.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   25.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   28.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "\n",
      "\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   26.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   29.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100building tree 10 of 100\n",
      "\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    5.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   27.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   29.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_ada_rfr.joblib']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_rfr = AdaBoostRegressor(base_estimator = rfr_model, n_estimators = 10, random_state = 77)\n",
    "final_ada_rfr = final_ada_rfr.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_rfr, 'final_ada_rfr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   18.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   19.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    3.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   15.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   17.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    4.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100building tree 58 of 100\n",
      "\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   19.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   21.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "\n",
      "\n",
      "building tree 5 of 100building tree 6 of 100\n",
      "\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    4.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100building tree 62 of 100\n",
      "\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   19.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   21.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    3.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   18.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   19.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100building tree 11 of 100\n",
      "\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    4.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   22.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   24.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    4.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   19.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   21.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    3.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   17.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   18.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    4.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100building tree 58 of 100\n",
      "\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   20.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   22.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100building tree 8 of 100\n",
      "\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    3.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   18.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   20.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_ada_efr.joblib']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_efr = AdaBoostRegressor(base_estimator = efr_model, n_estimators = 10, random_state = 78)\n",
    "final_ada_efr = final_ada_efr.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_efr, 'final_ada_efr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3462           33.66s\n",
      "         2           0.3310           34.34s\n",
      "         3           0.3146           32.55s\n",
      "         4           0.3036           30.33s\n",
      "         5           0.2905           29.06s\n",
      "         6           0.2785           28.10s\n",
      "         7           0.2699           27.21s\n",
      "         8           0.2614           26.55s\n",
      "         9           0.2560           26.06s\n",
      "        10           0.2504           25.68s\n",
      "        11           0.2438           25.29s\n",
      "        12           0.2390           24.92s\n",
      "        13           0.2350           24.76s\n",
      "        14           0.2303           24.62s\n",
      "        15           0.2272           24.71s\n",
      "        16           0.2231           24.50s\n",
      "        17           0.2197           24.17s\n",
      "        18           0.2171           23.83s\n",
      "        19           0.2135           23.52s\n",
      "        20           0.2105           23.37s\n",
      "        21           0.2075           23.50s\n",
      "        22           0.2053           23.70s\n",
      "        23           0.2029           23.87s\n",
      "        24           0.2013           23.80s\n",
      "        25           0.1999           23.57s\n",
      "        26           0.1976           23.27s\n",
      "        27           0.1964           23.02s\n",
      "        28           0.1950           22.71s\n",
      "        29           0.1936           22.40s\n",
      "        30           0.1918           22.02s\n",
      "        31           0.1905           21.64s\n",
      "        32           0.1892           21.29s\n",
      "        33           0.1877           20.97s\n",
      "        34           0.1864           20.66s\n",
      "        35           0.1854           20.36s\n",
      "        36           0.1846           20.06s\n",
      "        37           0.1837           19.80s\n",
      "        38           0.1830           19.57s\n",
      "        39           0.1816           19.33s\n",
      "        40           0.1807           19.16s\n",
      "        41           0.1798           18.90s\n",
      "        42           0.1787           18.67s\n",
      "        43           0.1777           18.33s\n",
      "        44           0.1770           17.96s\n",
      "        45           0.1760           17.59s\n",
      "        46           0.1752           17.21s\n",
      "        47           0.1745           16.87s\n",
      "        48           0.1734           16.50s\n",
      "        49           0.1730           16.14s\n",
      "        50           0.1719           15.78s\n",
      "        51           0.1709           15.41s\n",
      "        52           0.1706           15.06s\n",
      "        53           0.1697           14.71s\n",
      "        54           0.1686           14.40s\n",
      "        55           0.1677           14.08s\n",
      "        56           0.1672           13.73s\n",
      "        57           0.1666           13.41s\n",
      "        58           0.1659           13.08s\n",
      "        59           0.1652           12.74s\n",
      "        60           0.1648           12.41s\n",
      "        61           0.1641           12.08s\n",
      "        62           0.1630           11.74s\n",
      "        63           0.1620           11.41s\n",
      "        64           0.1616           11.09s\n",
      "        65           0.1610           10.80s\n",
      "        66           0.1603           10.48s\n",
      "        67           0.1595           10.15s\n",
      "        68           0.1588            9.84s\n",
      "        69           0.1580            9.52s\n",
      "        70           0.1577            9.21s\n",
      "        71           0.1569            8.88s\n",
      "        72           0.1563            8.57s\n",
      "        73           0.1558            8.24s\n",
      "        74           0.1555            7.91s\n",
      "        75           0.1548            7.60s\n",
      "        76           0.1544            7.29s\n",
      "        77           0.1537            6.98s\n",
      "        78           0.1532            6.67s\n",
      "        79           0.1530            6.35s\n",
      "        80           0.1524            6.04s\n",
      "        81           0.1516            5.73s\n",
      "        82           0.1510            5.43s\n",
      "        83           0.1502            5.12s\n",
      "        84           0.1494            4.81s\n",
      "        85           0.1490            4.50s\n",
      "        86           0.1485            4.20s\n",
      "        87           0.1480            3.89s\n",
      "        88           0.1472            3.58s\n",
      "        89           0.1466            3.27s\n",
      "        90           0.1463            2.97s\n",
      "        91           0.1458            2.67s\n",
      "        92           0.1455            2.37s\n",
      "        93           0.1451            2.07s\n",
      "        94           0.1446            1.78s\n",
      "        95           0.1442            1.48s\n",
      "        96           0.1435            1.18s\n",
      "        97           0.1432            0.88s\n",
      "        98           0.1424            0.59s\n",
      "        99           0.1420            0.29s\n",
      "       100           0.1416            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.5222           28.37s\n",
      "         2           0.5029           25.67s\n",
      "         3           0.4884           24.32s\n",
      "         4           0.4749           23.74s\n",
      "         5           0.4618           23.34s\n",
      "         6           0.4525           23.03s\n",
      "         7           0.4442           22.80s\n",
      "         8           0.4329           22.58s\n",
      "         9           0.4238           22.42s\n",
      "        10           0.4175           22.15s\n",
      "        11           0.4106           22.05s\n",
      "        12           0.4048           21.77s\n",
      "        13           0.3984           21.40s\n",
      "        14           0.3919           21.11s\n",
      "        15           0.3863           20.82s\n",
      "        16           0.3791           20.56s\n",
      "        17           0.3751           20.27s\n",
      "        18           0.3706           20.00s\n",
      "        19           0.3661           19.69s\n",
      "        20           0.3622           19.41s\n",
      "        21           0.3598           19.11s\n",
      "        22           0.3569           18.85s\n",
      "        23           0.3526           18.62s\n",
      "        24           0.3500           18.39s\n",
      "        25           0.3468           18.14s\n",
      "        26           0.3435           17.89s\n",
      "        27           0.3408           17.66s\n",
      "        28           0.3381           17.44s\n",
      "        29           0.3357           17.23s\n",
      "        30           0.3337           17.02s\n",
      "        31           0.3317           16.81s\n",
      "        32           0.3295           16.57s\n",
      "        33           0.3275           16.32s\n",
      "        34           0.3257           16.08s\n",
      "        35           0.3232           15.87s\n",
      "        36           0.3214           15.60s\n",
      "        37           0.3195           15.33s\n",
      "        38           0.3172           15.08s\n",
      "        39           0.3153           14.83s\n",
      "        40           0.3143           14.58s\n",
      "        41           0.3126           14.31s\n",
      "        42           0.3107           14.06s\n",
      "        43           0.3097           13.85s\n",
      "        44           0.3070           13.60s\n",
      "        45           0.3050           13.38s\n",
      "        46           0.3038           13.15s\n",
      "        47           0.3026           12.90s\n",
      "        48           0.3018           12.66s\n",
      "        49           0.3007           12.41s\n",
      "        50           0.2990           12.17s\n",
      "        51           0.2975           11.94s\n",
      "        52           0.2958           11.68s\n",
      "        53           0.2950           11.43s\n",
      "        54           0.2939           11.18s\n",
      "        55           0.2914           10.94s\n",
      "        56           0.2896           10.69s\n",
      "        57           0.2879           10.44s\n",
      "        58           0.2863           10.19s\n",
      "        59           0.2850            9.95s\n",
      "        60           0.2833            9.70s\n",
      "        61           0.2821            9.46s\n",
      "        62           0.2811            9.21s\n",
      "        63           0.2796            8.96s\n",
      "        64           0.2783            8.72s\n",
      "        65           0.2773            8.48s\n",
      "        66           0.2764            8.24s\n",
      "        67           0.2749            8.00s\n",
      "        68           0.2738            7.76s\n",
      "        69           0.2730            7.52s\n",
      "        70           0.2726            7.28s\n",
      "        71           0.2707            7.05s\n",
      "        72           0.2695            6.82s\n",
      "        73           0.2679            6.57s\n",
      "        74           0.2660            6.33s\n",
      "        75           0.2641            6.09s\n",
      "        76           0.2625            5.84s\n",
      "        77           0.2614            5.59s\n",
      "        78           0.2604            5.34s\n",
      "        79           0.2598            5.10s\n",
      "        80           0.2587            4.85s\n",
      "        81           0.2581            4.61s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        82           0.2566            4.37s\n",
      "        83           0.2555            4.13s\n",
      "        84           0.2547            3.88s\n",
      "        85           0.2533            3.64s\n",
      "        86           0.2528            3.40s\n",
      "        87           0.2521            3.16s\n",
      "        88           0.2507            2.92s\n",
      "        89           0.2497            2.67s\n",
      "        90           0.2490            2.43s\n",
      "        91           0.2477            2.19s\n",
      "        92           0.2465            1.95s\n",
      "        93           0.2457            1.70s\n",
      "        94           0.2445            1.46s\n",
      "        95           0.2435            1.22s\n",
      "        96           0.2430            0.97s\n",
      "        97           0.2419            0.73s\n",
      "        98           0.2411            0.49s\n",
      "        99           0.2401            0.24s\n",
      "       100           0.2393            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.7033           22.50s\n",
      "         2           0.6859           22.86s\n",
      "         3           0.6698           22.78s\n",
      "         4           0.6574           23.82s\n",
      "         5           0.6452           23.72s\n",
      "         6           0.6338           23.52s\n",
      "         7           0.6190           23.41s\n",
      "         8           0.6074           23.00s\n",
      "         9           0.5962           22.78s\n",
      "        10           0.5874           22.42s\n",
      "        11           0.5776           22.13s\n",
      "        12           0.5679           21.78s\n",
      "        13           0.5590           21.38s\n",
      "        14           0.5523           21.06s\n",
      "        15           0.5444           20.74s\n",
      "        16           0.5353           20.45s\n",
      "        17           0.5297           20.11s\n",
      "        18           0.5243           19.84s\n",
      "        19           0.5190           19.55s\n",
      "        20           0.5153           19.37s\n",
      "        21           0.5105           19.12s\n",
      "        22           0.5055           18.88s\n",
      "        23           0.5022           18.68s\n",
      "        24           0.4977           18.48s\n",
      "        25           0.4933           18.25s\n",
      "        26           0.4898           17.99s\n",
      "        27           0.4864           17.70s\n",
      "        28           0.4825           17.42s\n",
      "        29           0.4774           17.19s\n",
      "        30           0.4729           16.95s\n",
      "        31           0.4688           16.75s\n",
      "        32           0.4658           16.48s\n",
      "        33           0.4629           16.22s\n",
      "        34           0.4600           15.99s\n",
      "        35           0.4586           15.75s\n",
      "        36           0.4553           15.52s\n",
      "        37           0.4519           15.30s\n",
      "        38           0.4475           15.05s\n",
      "        39           0.4451           14.77s\n",
      "        40           0.4431           14.57s\n",
      "        41           0.4397           14.31s\n",
      "        42           0.4389           14.05s\n",
      "        43           0.4353           13.81s\n",
      "        44           0.4318           13.57s\n",
      "        45           0.4296           13.32s\n",
      "        46           0.4266           13.07s\n",
      "        47           0.4237           12.81s\n",
      "        48           0.4209           12.56s\n",
      "        49           0.4189           12.32s\n",
      "        50           0.4167           12.09s\n",
      "        51           0.4135           11.84s\n",
      "        52           0.4122           11.60s\n",
      "        53           0.4102           11.36s\n",
      "        54           0.4082           11.13s\n",
      "        55           0.4068           10.89s\n",
      "        56           0.4041           10.65s\n",
      "        57           0.4016           10.39s\n",
      "        58           0.3980           10.14s\n",
      "        59           0.3952            9.90s\n",
      "        60           0.3937            9.65s\n",
      "        61           0.3922            9.41s\n",
      "        62           0.3897            9.16s\n",
      "        63           0.3869            8.91s\n",
      "        64           0.3850            8.67s\n",
      "        65           0.3840            8.43s\n",
      "        66           0.3819            8.18s\n",
      "        67           0.3790            7.93s\n",
      "        68           0.3766            7.71s\n",
      "        69           0.3743            7.48s\n",
      "        70           0.3729            7.26s\n",
      "        71           0.3723            7.02s\n",
      "        72           0.3706            6.79s\n",
      "        73           0.3680            6.55s\n",
      "        74           0.3672            6.32s\n",
      "        75           0.3657            6.07s\n",
      "        76           0.3648            5.83s\n",
      "        77           0.3630            5.58s\n",
      "        78           0.3617            5.34s\n",
      "        79           0.3603            5.09s\n",
      "        80           0.3581            4.85s\n",
      "        81           0.3572            4.60s\n",
      "        82           0.3561            4.36s\n",
      "        83           0.3539            4.11s\n",
      "        84           0.3524            3.87s\n",
      "        85           0.3509            3.63s\n",
      "        86           0.3489            3.39s\n",
      "        87           0.3473            3.14s\n",
      "        88           0.3464            2.90s\n",
      "        89           0.3448            2.66s\n",
      "        90           0.3426            2.42s\n",
      "        91           0.3414            2.18s\n",
      "        92           0.3396            1.93s\n",
      "        93           0.3374            1.69s\n",
      "        94           0.3366            1.45s\n",
      "        95           0.3341            1.21s\n",
      "        96           0.3328            0.97s\n",
      "        97           0.3317            0.72s\n",
      "        98           0.3312            0.48s\n",
      "        99           0.3308            0.24s\n",
      "       100           0.3300            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.8331           22.50s\n",
      "         2           0.8122           22.66s\n",
      "         3           0.7956           22.82s\n",
      "         4           0.7781           22.55s\n",
      "         5           0.7629           22.26s\n",
      "         6           0.7511           21.82s\n",
      "         7           0.7324           21.71s\n",
      "         8           0.7208           21.48s\n",
      "         9           0.7095           21.19s\n",
      "        10           0.6991           20.95s\n",
      "        11           0.6893           20.73s\n",
      "        12           0.6775           20.46s\n",
      "        13           0.6712           20.20s\n",
      "        14           0.6639           20.02s\n",
      "        15           0.6561           19.78s\n",
      "        16           0.6500           19.56s\n",
      "        17           0.6431           19.36s\n",
      "        18           0.6329           19.40s\n",
      "        19           0.6293           19.28s\n",
      "        20           0.6217           19.03s\n",
      "        21           0.6156           18.84s\n",
      "        22           0.6113           18.59s\n",
      "        23           0.6050           18.37s\n",
      "        24           0.5992           18.17s\n",
      "        25           0.5929           17.95s\n",
      "        26           0.5872           17.75s\n",
      "        27           0.5802           17.53s\n",
      "        28           0.5767           17.32s\n",
      "        29           0.5738           17.05s\n",
      "        30           0.5706           16.85s\n",
      "        31           0.5664           16.62s\n",
      "        32           0.5624           16.38s\n",
      "        33           0.5589           16.13s\n",
      "        34           0.5560           15.93s\n",
      "        35           0.5503           15.71s\n",
      "        36           0.5482           15.45s\n",
      "        37           0.5448           15.18s\n",
      "        38           0.5417           14.95s\n",
      "        39           0.5384           14.70s\n",
      "        40           0.5365           14.48s\n",
      "        41           0.5318           14.23s\n",
      "        42           0.5267           13.96s\n",
      "        43           0.5235           13.72s\n",
      "        44           0.5209           13.49s\n",
      "        45           0.5167           13.25s\n",
      "        46           0.5129           13.01s\n",
      "        47           0.5101           12.78s\n",
      "        48           0.5054           12.56s\n",
      "        49           0.5019           12.34s\n",
      "        50           0.4965           12.11s\n",
      "        51           0.4924           11.88s\n",
      "        52           0.4909           11.63s\n",
      "        53           0.4880           11.38s\n",
      "        54           0.4865           11.14s\n",
      "        55           0.4822           10.90s\n",
      "        56           0.4789           10.65s\n",
      "        57           0.4744           10.40s\n",
      "        58           0.4702           10.15s\n",
      "        59           0.4675            9.91s\n",
      "        60           0.4641            9.67s\n",
      "        61           0.4606            9.43s\n",
      "        62           0.4592            9.18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        63           0.4563            8.93s\n",
      "        64           0.4549            8.70s\n",
      "        65           0.4539            8.47s\n",
      "        66           0.4518            8.24s\n",
      "        67           0.4478            8.00s\n",
      "        68           0.4468            7.76s\n",
      "        69           0.4449            7.52s\n",
      "        70           0.4430            7.28s\n",
      "        71           0.4417            7.03s\n",
      "        72           0.4394            6.82s\n",
      "        73           0.4379            6.59s\n",
      "        74           0.4364            6.35s\n",
      "        75           0.4354            6.12s\n",
      "        76           0.4324            5.87s\n",
      "        77           0.4303            5.63s\n",
      "        78           0.4281            5.38s\n",
      "        79           0.4257            5.13s\n",
      "        80           0.4230            4.89s\n",
      "        81           0.4215            4.64s\n",
      "        82           0.4195            4.40s\n",
      "        83           0.4184            4.15s\n",
      "        84           0.4173            3.91s\n",
      "        85           0.4150            3.67s\n",
      "        86           0.4137            3.42s\n",
      "        87           0.4121            3.18s\n",
      "        88           0.4097            2.94s\n",
      "        89           0.4081            2.69s\n",
      "        90           0.4067            2.45s\n",
      "        91           0.4055            2.20s\n",
      "        92           0.4038            1.96s\n",
      "        93           0.4012            1.71s\n",
      "        94           0.3987            1.47s\n",
      "        95           0.3964            1.22s\n",
      "        96           0.3946            0.98s\n",
      "        97           0.3937            0.73s\n",
      "        98           0.3924            0.49s\n",
      "        99           0.3904            0.24s\n",
      "       100           0.3866            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9213           24.09s\n",
      "         2           0.8972           23.84s\n",
      "         3           0.8800           23.61s\n",
      "         4           0.8646           23.23s\n",
      "         5           0.8463           22.95s\n",
      "         6           0.8326           22.55s\n",
      "         7           0.8157           22.44s\n",
      "         8           0.8037           22.32s\n",
      "         9           0.7930           22.05s\n",
      "        10           0.7807           22.08s\n",
      "        11           0.7688           22.73s\n",
      "        12           0.7579           23.10s\n",
      "        13           0.7490           23.51s\n",
      "        14           0.7433           23.76s\n",
      "        15           0.7319           23.75s\n",
      "        16           0.7220           23.71s\n",
      "        17           0.7174           23.87s\n",
      "        18           0.7081           24.13s\n",
      "        19           0.7004           24.58s\n",
      "        20           0.6914           24.79s\n",
      "        21           0.6839           24.76s\n",
      "        22           0.6774           24.79s\n",
      "        23           0.6724           24.55s\n",
      "        24           0.6664           24.29s\n",
      "        25           0.6622           24.07s\n",
      "        26           0.6589           23.83s\n",
      "        27           0.6511           23.53s\n",
      "        28           0.6465           23.19s\n",
      "        29           0.6407           22.81s\n",
      "        30           0.6365           22.82s\n",
      "        31           0.6329           22.71s\n",
      "        32           0.6277           22.48s\n",
      "        33           0.6234           22.36s\n",
      "        34           0.6200           22.11s\n",
      "        35           0.6166           21.83s\n",
      "        36           0.6097           21.57s\n",
      "        37           0.6066           21.43s\n",
      "        38           0.6025           21.19s\n",
      "        39           0.5957           20.94s\n",
      "        40           0.5919           20.77s\n",
      "        41           0.5885           20.53s\n",
      "        42           0.5836           20.22s\n",
      "        43           0.5792           19.86s\n",
      "        44           0.5765           19.43s\n",
      "        45           0.5745           18.99s\n",
      "        46           0.5701           18.54s\n",
      "        47           0.5653           18.10s\n",
      "        48           0.5588           17.66s\n",
      "        49           0.5561           17.24s\n",
      "        50           0.5525           16.83s\n",
      "        51           0.5507           16.40s\n",
      "        52           0.5484           15.99s\n",
      "        53           0.5452           15.60s\n",
      "        54           0.5432           15.22s\n",
      "        55           0.5390           14.83s\n",
      "        56           0.5376           14.45s\n",
      "        57           0.5329           14.06s\n",
      "        58           0.5294           13.69s\n",
      "        59           0.5260           13.31s\n",
      "        60           0.5220           12.94s\n",
      "        61           0.5173           12.57s\n",
      "        62           0.5150           12.23s\n",
      "        63           0.5110           11.88s\n",
      "        64           0.5079           11.52s\n",
      "        65           0.5060           11.17s\n",
      "        66           0.5026           10.82s\n",
      "        67           0.5014           10.48s\n",
      "        68           0.4994           10.14s\n",
      "        69           0.4982            9.80s\n",
      "        70           0.4967            9.46s\n",
      "        71           0.4927            9.11s\n",
      "        72           0.4913            8.78s\n",
      "        73           0.4899            8.45s\n",
      "        74           0.4886            8.13s\n",
      "        75           0.4846            7.80s\n",
      "        76           0.4826            7.47s\n",
      "        77           0.4812            7.15s\n",
      "        78           0.4796            6.82s\n",
      "        79           0.4775            6.50s\n",
      "        80           0.4758            6.18s\n",
      "        81           0.4733            5.86s\n",
      "        82           0.4684            5.55s\n",
      "        83           0.4668            5.24s\n",
      "        84           0.4642            4.93s\n",
      "        85           0.4623            4.62s\n",
      "        86           0.4591            4.31s\n",
      "        87           0.4562            4.00s\n",
      "        88           0.4540            3.68s\n",
      "        89           0.4508            3.37s\n",
      "        90           0.4485            3.06s\n",
      "        91           0.4451            2.75s\n",
      "        92           0.4432            2.44s\n",
      "        93           0.4409            2.13s\n",
      "        94           0.4384            1.82s\n",
      "        95           0.4368            1.52s\n",
      "        96           0.4340            1.21s\n",
      "        97           0.4329            0.91s\n",
      "        98           0.4314            0.60s\n",
      "        99           0.4302            0.30s\n",
      "       100           0.4279            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.9939           25.46s\n",
      "         2           0.9662           25.20s\n",
      "         3           0.9433           25.33s\n",
      "         4           0.9233           25.43s\n",
      "         5           0.9105           25.72s\n",
      "         6           0.8935           25.75s\n",
      "         7           0.8773           25.86s\n",
      "         8           0.8620           25.73s\n",
      "         9           0.8489           25.30s\n",
      "        10           0.8341           24.83s\n",
      "        11           0.8175           24.46s\n",
      "        12           0.8068           24.07s\n",
      "        13           0.7965           23.72s\n",
      "        14           0.7885           23.38s\n",
      "        15           0.7814           23.06s\n",
      "        16           0.7738           22.76s\n",
      "        17           0.7670           22.52s\n",
      "        18           0.7601           22.27s\n",
      "        19           0.7527           22.18s\n",
      "        20           0.7480           21.98s\n",
      "        21           0.7436           21.72s\n",
      "        22           0.7388           21.41s\n",
      "        23           0.7310           21.17s\n",
      "        24           0.7250           20.89s\n",
      "        25           0.7188           20.61s\n",
      "        26           0.7133           20.35s\n",
      "        27           0.7112           20.04s\n",
      "        28           0.7052           19.75s\n",
      "        29           0.6985           19.44s\n",
      "        30           0.6915           19.13s\n",
      "        31           0.6874           18.85s\n",
      "        32           0.6833           18.58s\n",
      "        33           0.6797           18.31s\n",
      "        34           0.6750           18.01s\n",
      "        35           0.6720           17.73s\n",
      "        36           0.6675           17.44s\n",
      "        37           0.6624           17.14s\n",
      "        38           0.6602           16.84s\n",
      "        39           0.6543           16.56s\n",
      "        40           0.6487           16.27s\n",
      "        41           0.6429           15.97s\n",
      "        42           0.6379           15.66s\n",
      "        43           0.6311           15.37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        44           0.6285           15.08s\n",
      "        45           0.6244           14.80s\n",
      "        46           0.6230           14.53s\n",
      "        47           0.6172           14.23s\n",
      "        48           0.6134           13.94s\n",
      "        49           0.6105           13.66s\n",
      "        50           0.6074           13.38s\n",
      "        51           0.6011           13.10s\n",
      "        52           0.5962           12.82s\n",
      "        53           0.5933           12.53s\n",
      "        54           0.5906           12.26s\n",
      "        55           0.5840           11.98s\n",
      "        56           0.5810           11.69s\n",
      "        57           0.5759           11.43s\n",
      "        58           0.5729           11.15s\n",
      "        59           0.5689           10.88s\n",
      "        60           0.5653           10.61s\n",
      "        61           0.5588           10.33s\n",
      "        62           0.5537           10.06s\n",
      "        63           0.5509            9.80s\n",
      "        64           0.5476            9.53s\n",
      "        65           0.5444            9.26s\n",
      "        66           0.5425            8.99s\n",
      "        67           0.5410            8.73s\n",
      "        68           0.5376            8.46s\n",
      "        69           0.5324            8.19s\n",
      "        70           0.5300            7.92s\n",
      "        71           0.5277            7.66s\n",
      "        72           0.5242            7.39s\n",
      "        73           0.5222            7.12s\n",
      "        74           0.5191            6.87s\n",
      "        75           0.5170            6.61s\n",
      "        76           0.5140            6.35s\n",
      "        77           0.5125            6.09s\n",
      "        78           0.5090            5.84s\n",
      "        79           0.5060            5.58s\n",
      "        80           0.5030            5.33s\n",
      "        81           0.4987            5.08s\n",
      "        82           0.4937            4.82s\n",
      "        83           0.4908            4.56s\n",
      "        84           0.4884            4.30s\n",
      "        85           0.4857            4.03s\n",
      "        86           0.4834            3.76s\n",
      "        87           0.4809            3.49s\n",
      "        88           0.4793            3.22s\n",
      "        89           0.4760            2.95s\n",
      "        90           0.4724            2.68s\n",
      "        91           0.4703            2.42s\n",
      "        92           0.4659            2.15s\n",
      "        93           0.4642            1.88s\n",
      "        94           0.4615            1.61s\n",
      "        95           0.4600            1.34s\n",
      "        96           0.4560            1.07s\n",
      "        97           0.4533            0.80s\n",
      "        98           0.4492            0.53s\n",
      "        99           0.4464            0.27s\n",
      "       100           0.4453            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0241           27.03s\n",
      "         2           1.0029           25.64s\n",
      "         3           0.9817           25.27s\n",
      "         4           0.9646           25.14s\n",
      "         5           0.9467           24.94s\n",
      "         6           0.9355           24.39s\n",
      "         7           0.9205           23.96s\n",
      "         8           0.9084           23.65s\n",
      "         9           0.8937           23.46s\n",
      "        10           0.8850           23.42s\n",
      "        11           0.8784           23.22s\n",
      "        12           0.8671           22.92s\n",
      "        13           0.8553           22.57s\n",
      "        14           0.8434           22.46s\n",
      "        15           0.8319           22.67s\n",
      "        16           0.8247           22.69s\n",
      "        17           0.8173           22.54s\n",
      "        18           0.8125           22.25s\n",
      "        19           0.8064           22.01s\n",
      "        20           0.7976           21.76s\n",
      "        21           0.7881           21.45s\n",
      "        22           0.7820           21.24s\n",
      "        23           0.7749           21.03s\n",
      "        24           0.7660           20.75s\n",
      "        25           0.7603           20.43s\n",
      "        26           0.7555           20.11s\n",
      "        27           0.7497           19.81s\n",
      "        28           0.7413           19.51s\n",
      "        29           0.7388           19.24s\n",
      "        30           0.7332           18.89s\n",
      "        31           0.7294           18.63s\n",
      "        32           0.7236           18.34s\n",
      "        33           0.7207           18.03s\n",
      "        34           0.7176           17.75s\n",
      "        35           0.7136           17.44s\n",
      "        36           0.7116           17.15s\n",
      "        37           0.7052           16.90s\n",
      "        38           0.7015           16.61s\n",
      "        39           0.6965           16.40s\n",
      "        40           0.6927           16.12s\n",
      "        41           0.6906           15.85s\n",
      "        42           0.6875           15.57s\n",
      "        43           0.6826           15.29s\n",
      "        44           0.6767           15.03s\n",
      "        45           0.6732           14.75s\n",
      "        46           0.6716           14.47s\n",
      "        47           0.6676           14.22s\n",
      "        48           0.6618           14.02s\n",
      "        49           0.6559           13.78s\n",
      "        50           0.6518           13.52s\n",
      "        51           0.6493           13.23s\n",
      "        52           0.6464           12.95s\n",
      "        53           0.6441           12.68s\n",
      "        54           0.6381           12.41s\n",
      "        55           0.6333           12.13s\n",
      "        56           0.6299           11.86s\n",
      "        57           0.6287           11.59s\n",
      "        58           0.6252           11.32s\n",
      "        59           0.6211           11.05s\n",
      "        60           0.6179           10.77s\n",
      "        61           0.6131           10.49s\n",
      "        62           0.6076           10.22s\n",
      "        63           0.6027            9.94s\n",
      "        64           0.5988            9.66s\n",
      "        65           0.5972            9.40s\n",
      "        66           0.5941            9.13s\n",
      "        67           0.5896            8.86s\n",
      "        68           0.5861            8.58s\n",
      "        69           0.5848            8.31s\n",
      "        70           0.5824            8.04s\n",
      "        71           0.5784            7.76s\n",
      "        72           0.5738            7.50s\n",
      "        73           0.5705            7.23s\n",
      "        74           0.5684            6.96s\n",
      "        75           0.5661            6.69s\n",
      "        76           0.5617            6.42s\n",
      "        77           0.5566            6.16s\n",
      "        78           0.5530            5.89s\n",
      "        79           0.5516            5.63s\n",
      "        80           0.5499            5.36s\n",
      "        81           0.5464            5.09s\n",
      "        82           0.5423            4.82s\n",
      "        83           0.5407            4.55s\n",
      "        84           0.5370            4.28s\n",
      "        85           0.5357            4.01s\n",
      "        86           0.5343            3.75s\n",
      "        87           0.5321            3.48s\n",
      "        88           0.5295            3.21s\n",
      "        89           0.5261            2.94s\n",
      "        90           0.5232            2.67s\n",
      "        91           0.5212            2.40s\n",
      "        92           0.5184            2.14s\n",
      "        93           0.5148            1.87s\n",
      "        94           0.5123            1.60s\n",
      "        95           0.5089            1.33s\n",
      "        96           0.5066            1.07s\n",
      "        97           0.5048            0.80s\n",
      "        98           0.5019            0.53s\n",
      "        99           0.4990            0.27s\n",
      "       100           0.4968            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0780           43.37s\n",
      "         2           1.0509           39.75s\n",
      "         3           1.0341           38.67s\n",
      "         4           1.0138           39.34s\n",
      "         5           1.0020           36.65s\n",
      "         6           0.9830           35.48s\n",
      "         7           0.9610           33.70s\n",
      "         8           0.9485           33.12s\n",
      "         9           0.9286           32.00s\n",
      "        10           0.9139           30.96s\n",
      "        11           0.9009           30.77s\n",
      "        12           0.8930           30.30s\n",
      "        13           0.8809           29.68s\n",
      "        14           0.8709           29.13s\n",
      "        15           0.8604           28.52s\n",
      "        16           0.8531           27.99s\n",
      "        17           0.8435           27.63s\n",
      "        18           0.8366           27.23s\n",
      "        19           0.8284           26.68s\n",
      "        20           0.8180           26.19s\n",
      "        21           0.8080           25.67s\n",
      "        22           0.8011           25.12s\n",
      "        23           0.7959           24.54s\n",
      "        24           0.7892           23.99s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        25           0.7819           23.51s\n",
      "        26           0.7747           23.05s\n",
      "        27           0.7663           22.56s\n",
      "        28           0.7603           22.16s\n",
      "        29           0.7526           21.68s\n",
      "        30           0.7442           21.22s\n",
      "        31           0.7375           20.79s\n",
      "        32           0.7309           20.41s\n",
      "        33           0.7245           20.03s\n",
      "        34           0.7211           19.68s\n",
      "        35           0.7166           19.50s\n",
      "        36           0.7129           19.17s\n",
      "        37           0.7086           18.77s\n",
      "        38           0.7063           18.41s\n",
      "        39           0.7017           18.06s\n",
      "        40           0.6939           17.70s\n",
      "        41           0.6894           17.35s\n",
      "        42           0.6861           16.98s\n",
      "        43           0.6827           16.62s\n",
      "        44           0.6758           16.29s\n",
      "        45           0.6702           16.00s\n",
      "        46           0.6672           15.70s\n",
      "        47           0.6625           15.40s\n",
      "        48           0.6592           15.12s\n",
      "        49           0.6535           14.80s\n",
      "        50           0.6477           14.55s\n",
      "        51           0.6458           14.27s\n",
      "        52           0.6397           13.97s\n",
      "        53           0.6378           13.66s\n",
      "        54           0.6331           13.35s\n",
      "        55           0.6313           13.08s\n",
      "        56           0.6288           12.79s\n",
      "        57           0.6249           12.49s\n",
      "        58           0.6197           12.18s\n",
      "        59           0.6163           11.86s\n",
      "        60           0.6137           11.62s\n",
      "        61           0.6126           11.38s\n",
      "        62           0.6093           11.11s\n",
      "        63           0.6049           10.84s\n",
      "        64           0.6009           10.55s\n",
      "        65           0.5946           10.27s\n",
      "        66           0.5925           10.02s\n",
      "        67           0.5905            9.73s\n",
      "        68           0.5885            9.44s\n",
      "        69           0.5820            9.16s\n",
      "        70           0.5805            8.89s\n",
      "        71           0.5782            8.61s\n",
      "        72           0.5745            8.32s\n",
      "        73           0.5706            8.03s\n",
      "        74           0.5677            7.74s\n",
      "        75           0.5656            7.43s\n",
      "        76           0.5609            7.14s\n",
      "        77           0.5577            6.83s\n",
      "        78           0.5553            6.53s\n",
      "        79           0.5493            6.23s\n",
      "        80           0.5471            5.94s\n",
      "        81           0.5454            5.64s\n",
      "        82           0.5429            5.35s\n",
      "        83           0.5421            5.08s\n",
      "        84           0.5389            4.82s\n",
      "        85           0.5372            4.53s\n",
      "        86           0.5347            4.23s\n",
      "        87           0.5317            3.92s\n",
      "        88           0.5302            3.62s\n",
      "        89           0.5285            3.31s\n",
      "        90           0.5260            3.00s\n",
      "        91           0.5240            2.70s\n",
      "        92           0.5198            2.40s\n",
      "        93           0.5158            2.10s\n",
      "        94           0.5150            1.80s\n",
      "        95           0.5112            1.50s\n",
      "        96           0.5076            1.20s\n",
      "        97           0.5045            0.90s\n",
      "        98           0.5027            0.60s\n",
      "        99           0.5019            0.30s\n",
      "       100           0.4978            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.0790           40.86s\n",
      "         2           1.0606           36.74s\n",
      "         3           1.0360           33.63s\n",
      "         4           1.0185           32.33s\n",
      "         5           1.0026           33.32s\n",
      "         6           0.9888           34.01s\n",
      "         7           0.9701           34.75s\n",
      "         8           0.9527           34.87s\n",
      "         9           0.9410           34.59s\n",
      "        10           0.9258           34.00s\n",
      "        11           0.9130           33.38s\n",
      "        12           0.9028           33.00s\n",
      "        13           0.8936           32.48s\n",
      "        14           0.8830           31.92s\n",
      "        15           0.8728           31.57s\n",
      "        16           0.8640           31.17s\n",
      "        17           0.8581           30.69s\n",
      "        18           0.8503           30.15s\n",
      "        19           0.8398           29.36s\n",
      "        20           0.8304           28.56s\n",
      "        21           0.8230           27.76s\n",
      "        22           0.8179           27.01s\n",
      "        23           0.8110           26.28s\n",
      "        24           0.8046           25.64s\n",
      "        25           0.7960           25.03s\n",
      "        26           0.7902           24.52s\n",
      "        27           0.7872           23.96s\n",
      "        28           0.7795           23.43s\n",
      "        29           0.7707           22.90s\n",
      "        30           0.7647           22.38s\n",
      "        31           0.7606           21.89s\n",
      "        32           0.7563           21.42s\n",
      "        33           0.7524           20.93s\n",
      "        34           0.7475           20.46s\n",
      "        35           0.7428           19.99s\n",
      "        36           0.7396           19.58s\n",
      "        37           0.7373           19.15s\n",
      "        38           0.7336           18.74s\n",
      "        39           0.7291           18.37s\n",
      "        40           0.7231           17.97s\n",
      "        41           0.7181           17.58s\n",
      "        42           0.7131           17.19s\n",
      "        43           0.7101           16.82s\n",
      "        44           0.7068           16.45s\n",
      "        45           0.7006           16.09s\n",
      "        46           0.6987           15.74s\n",
      "        47           0.6926           15.40s\n",
      "        48           0.6896           15.07s\n",
      "        49           0.6862           14.74s\n",
      "        50           0.6804           14.42s\n",
      "        51           0.6751           14.07s\n",
      "        52           0.6719           13.74s\n",
      "        53           0.6651           13.41s\n",
      "        54           0.6624           13.09s\n",
      "        55           0.6578           12.76s\n",
      "        56           0.6547           12.45s\n",
      "        57           0.6519           12.12s\n",
      "        58           0.6488           11.81s\n",
      "        59           0.6440           11.51s\n",
      "        60           0.6387           11.19s\n",
      "        61           0.6354           10.89s\n",
      "        62           0.6335           10.59s\n",
      "        63           0.6318           10.29s\n",
      "        64           0.6270            9.98s\n",
      "        65           0.6249            9.68s\n",
      "        66           0.6174            9.39s\n",
      "        67           0.6126            9.09s\n",
      "        68           0.6095            8.80s\n",
      "        69           0.6061            8.50s\n",
      "        70           0.6014            8.21s\n",
      "        71           0.5981            7.92s\n",
      "        72           0.5910            7.64s\n",
      "        73           0.5862            7.35s\n",
      "        74           0.5850            7.07s\n",
      "        75           0.5823            6.79s\n",
      "        76           0.5801            6.51s\n",
      "        77           0.5767            6.23s\n",
      "        78           0.5733            5.95s\n",
      "        79           0.5690            5.67s\n",
      "        80           0.5659            5.39s\n",
      "        81           0.5634            5.12s\n",
      "        82           0.5593            4.84s\n",
      "        83           0.5554            4.56s\n",
      "        84           0.5524            4.29s\n",
      "        85           0.5507            4.02s\n",
      "        86           0.5488            3.75s\n",
      "        87           0.5463            3.48s\n",
      "        88           0.5453            3.21s\n",
      "        89           0.5423            2.94s\n",
      "        90           0.5396            2.67s\n",
      "        91           0.5365            2.40s\n",
      "        92           0.5322            2.13s\n",
      "        93           0.5285            1.86s\n",
      "        94           0.5268            1.60s\n",
      "        95           0.5244            1.33s\n",
      "        96           0.5213            1.06s\n",
      "        97           0.5180            0.80s\n",
      "        98           0.5170            0.53s\n",
      "        99           0.5160            0.27s\n",
      "       100           0.5142            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1162           23.88s\n",
      "         2           1.0859           23.34s\n",
      "         3           1.0666           23.14s\n",
      "         4           1.0492           22.72s\n",
      "         5           1.0293           22.42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         6           1.0076           22.20s\n",
      "         7           0.9947           21.80s\n",
      "         8           0.9800           21.53s\n",
      "         9           0.9617           21.30s\n",
      "        10           0.9498           21.10s\n",
      "        11           0.9336           20.85s\n",
      "        12           0.9208           20.60s\n",
      "        13           0.9109           20.25s\n",
      "        14           0.9022           20.04s\n",
      "        15           0.8935           19.89s\n",
      "        16           0.8848           19.69s\n",
      "        17           0.8770           19.42s\n",
      "        18           0.8657           19.18s\n",
      "        19           0.8571           18.99s\n",
      "        20           0.8543           18.81s\n",
      "        21           0.8489           18.61s\n",
      "        22           0.8432           18.45s\n",
      "        23           0.8339           18.24s\n",
      "        24           0.8274           18.08s\n",
      "        25           0.8179           17.89s\n",
      "        26           0.8107           17.70s\n",
      "        27           0.8035           17.47s\n",
      "        28           0.7995           17.26s\n",
      "        29           0.7921           16.99s\n",
      "        30           0.7837           16.74s\n",
      "        31           0.7759           16.46s\n",
      "        32           0.7690           16.21s\n",
      "        33           0.7642           15.96s\n",
      "        34           0.7574           15.71s\n",
      "        35           0.7538           15.44s\n",
      "        36           0.7466           15.19s\n",
      "        37           0.7398           14.96s\n",
      "        38           0.7359           14.72s\n",
      "        39           0.7344           14.47s\n",
      "        40           0.7301           14.24s\n",
      "        41           0.7276           14.03s\n",
      "        42           0.7235           13.81s\n",
      "        43           0.7199           13.61s\n",
      "        44           0.7169           13.41s\n",
      "        45           0.7124           13.18s\n",
      "        46           0.7055           12.96s\n",
      "        47           0.7031           12.73s\n",
      "        48           0.7009           12.49s\n",
      "        49           0.6965           12.26s\n",
      "        50           0.6907           12.02s\n",
      "        51           0.6844           11.76s\n",
      "        52           0.6777           11.51s\n",
      "        53           0.6715           11.26s\n",
      "        54           0.6700           11.02s\n",
      "        55           0.6627           10.76s\n",
      "        56           0.6600           10.52s\n",
      "        57           0.6573           10.27s\n",
      "        58           0.6531           10.04s\n",
      "        59           0.6484            9.79s\n",
      "        60           0.6456            9.55s\n",
      "        61           0.6390            9.31s\n",
      "        62           0.6372            9.06s\n",
      "        63           0.6319            8.82s\n",
      "        64           0.6254            8.58s\n",
      "        65           0.6204            8.35s\n",
      "        66           0.6160            8.12s\n",
      "        67           0.6133            7.89s\n",
      "        68           0.6079            7.65s\n",
      "        69           0.6046            7.42s\n",
      "        70           0.6012            7.18s\n",
      "        71           0.5999            6.94s\n",
      "        72           0.5948            6.70s\n",
      "        73           0.5936            6.47s\n",
      "        74           0.5879            6.23s\n",
      "        75           0.5828            5.99s\n",
      "        76           0.5781            5.75s\n",
      "        77           0.5753            5.52s\n",
      "        78           0.5717            5.28s\n",
      "        79           0.5674            5.04s\n",
      "        80           0.5646            4.80s\n",
      "        81           0.5608            4.56s\n",
      "        82           0.5592            4.32s\n",
      "        83           0.5540            4.08s\n",
      "        84           0.5515            3.84s\n",
      "        85           0.5482            3.60s\n",
      "        86           0.5473            3.37s\n",
      "        87           0.5431            3.13s\n",
      "        88           0.5386            2.89s\n",
      "        89           0.5365            2.64s\n",
      "        90           0.5337            2.40s\n",
      "        91           0.5315            2.16s\n",
      "        92           0.5307            1.92s\n",
      "        93           0.5275            1.68s\n",
      "        94           0.5249            1.44s\n",
      "        95           0.5234            1.20s\n",
      "        96           0.5205            0.96s\n",
      "        97           0.5184            0.72s\n",
      "        98           0.5165            0.48s\n",
      "        99           0.5141            0.24s\n",
      "       100           0.5115            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_ada_gbr.joblib']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_gbr = AdaBoostRegressor(base_estimator = gbr_model, n_estimators = 10, random_state = 79)\n",
    "final_ada_gbr = final_ada_gbr.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_gbr, 'final_ada_gbr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_ada_xgb.joblib']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ada_xgb = AdaBoostRegressor(base_estimator = xgb_model, n_estimators = 10, random_state = 80)\n",
    "final_ada_xgb = final_ada_xgb.fit(X, y_clean)\n",
    "\n",
    "dump(final_ada_xgb, 'final_ada_xgb.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  7.1min remaining:  7.1min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_huber.joblib']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_huber = BaggingRegressor(base_estimator=HuberRegressor(alpha=20, epsilon=1.7,\n",
    "                                               max_iter=20000),\n",
    "                 n_estimators=50, n_jobs=6, random_state=81, verbose=4)\n",
    "final_bag_huber = final_bag_huber.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_huber, 'final_bag_huber.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  1.7min remaining:  1.7min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_ridge.joblib']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_ridge = BaggingRegressor(base_estimator = ridge_model, n_estimators = 50,\\\n",
    "                                   verbose = 4, n_jobs = 6, random_state = 82)\n",
    "final_bag_ridge = final_bag_ridge.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_ridge, 'final_bag_ridge.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  1.0min remaining:  1.0min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_lsvr.joblib']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_lsvr = BaggingRegressor(base_estimator = lsvr_model, n_estimators = 50,\\\n",
    "                                   verbose = 4, n_jobs = 6, random_state = 83)\n",
    "final_bag_lsvr = final_bag_lsvr.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_lsvr, 'final_bag_lsvr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    8.5s remaining:    8.5s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_knr.joblib']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_knr = BaggingRegressor(base_estimator = knr_model, n_estimators = 8,\\\n",
    "                                verbose = 4, n_jobs = 6, random_state = 85)\n",
    "final_bag_knr = final_bag_knr.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_knr, 'final_bag_knr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  3.5min remaining:  3.5min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_dtr.joblib']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_dtr = BaggingRegressor(base_estimator = dtr_model, n_estimators = 50,\\\n",
    "                                 verbose = 4, n_jobs = 6, random_state = 86)\n",
    "final_bag_dtr = final_bag_dtr.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_dtr, 'final_bag_dtr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  2.4min remaining:  2.4min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_rfr.joblib']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_rfr = BaggingRegressor(base_estimator = rfr_model, n_estimators = 10,\\\n",
    "                                 verbose = 4, n_jobs = 6, random_state = 87)\n",
    "final_bag_rfr = final_bag_rfr.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_rfr, 'final_bag_rfr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  1.9min remaining:  1.9min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_efr.joblib']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_efr = BaggingRegressor(base_estimator = efr_model, n_estimators = 10,\\\n",
    "                                   verbose = 4, n_jobs = 6, random_state = 88)\n",
    "final_bag_efr = final_bag_efr.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_efr, 'final_bag_efr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  1.2min remaining:  1.2min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_gbr.joblib']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_gbr = BaggingRegressor(base_estimator = gbr_model, n_estimators = 10,\\\n",
    "                                verbose = 4, n_jobs = 6, random_state = 89)\n",
    "final_bag_gbr = final_bag_gbr.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_gbr, 'final_bag_gbr.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  3.8min remaining:  3.8min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['final_bag_xgb.joblib']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bag_xgb = BaggingRegressor(base_estimator = xgb_model, n_estimators = 6,\\\n",
    "                                verbose = 4, n_jobs = 6, random_state = 90)\n",
    "final_bag_xgb = final_bag_xgb.fit(X, y_clean)\n",
    "\n",
    "dump(final_bag_xgb, 'final_bag_xgb.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_vote.joblib']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vote = VotingRegressor(estimators,\\\n",
    "                                verbose = 4, n_jobs = 6)\n",
    "\n",
    "final_vote = final_vote.fit(X, y_clean)\n",
    "\n",
    "dump(final_vote, 'final_vote.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_stacks.joblib']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_stacks = StackingRegressor(estimators, cv = 2, \\\n",
    "                                verbose = 4, n_jobs = 6)\n",
    "\n",
    "final_stacks = final_stacks.fit(X, y_clean)\n",
    "\n",
    "dump(final_stacks, 'final_stacks.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_final = [final_huber, final_ridge, final_lsvr, final_svr, final_knr, final_dtr, final_rfr,\\\n",
    "                    final_efr, final_gbr, final_xgb, \\\n",
    "                   final_ada_huber, final_ada_ridge, final_ada_lsvr, final_ada_knr, final_ada_dtr, \\\n",
    "                    final_ada_rfr, final_ada_efr, final_ada_gbr, final_ada_xgb, \\\n",
    "                   final_bag_huber, final_bag_ridge, final_bag_lsvr, final_bag_knr, final_bag_dtr, \\\n",
    "                    final_bag_rfr, final_bag_efr, final_bag_gbr, final_bag_xgb,\\\n",
    "                   final_vote, final_stacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    HuberRegressor(alpha=20, epsilon=1.7, max_iter=20000)\n",
      "1    Ridge(alpha=20000, max_iter=20000)\n",
      "2    LinearSVR(C=10000, dual=False, loss='squared_epsilon_insensitive',\n",
      "          random_state=10)\n",
      "3    SVR(C=10000)\n",
      "4    KNeighborsRegressor(algorithm='brute', n_neighbors=15, weights='distance')\n",
      "5    DecisionTreeRegressor(max_depth=10, min_samples_split=200, random_state=40)\n",
      "6    RandomForestRegressor(max_samples=0.95, n_jobs=4, random_state=40, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    ExtraTreesRegressor(n_jobs=4, random_state=50, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8    GradientBoostingRegressor(max_depth=5, random_state=50, verbose=4)\n",
      "9    XGBRegressor(alpha=100, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='', lambda=10,\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "             reg_alpha=100, reg_lambda=10, scale_pos_weight=1, subsample=0.5,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "10    AdaBoostRegressor(base_estimator=HuberRegressor(alpha=20, epsilon=1.7,\n",
      "                                                max_iter=20000),\n",
      "                  random_state=71)\n",
      "11    AdaBoostRegressor(base_estimator=Ridge(alpha=20000, max_iter=20000),\n",
      "                  random_state=72)\n",
      "12    AdaBoostRegressor(base_estimator=LinearSVR(C=10000, dual=False,\n",
      "                                           loss='squared_epsilon_insensitive',\n",
      "                                           random_state=10),\n",
      "                  random_state=73)\n",
      "13    AdaBoostRegressor(base_estimator=KNeighborsRegressor(algorithm='brute',\n",
      "                                                     n_neighbors=15,\n",
      "                                                     weights='distance'),\n",
      "                  n_estimators=10, random_state=75)\n",
      "14    AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10,\n",
      "                                                       min_samples_split=200,\n",
      "                                                       random_state=40),\n",
      "                  random_state=76)\n",
      "15    AdaBoostRegressor(base_estimator=RandomForestRegressor(max_features=50,\n",
      "                                                       max_samples=0.95,\n",
      "                                                       n_jobs=4,\n",
      "                                                       random_state=40,\n",
      "                                                       verbose=4),\n",
      "                  n_estimators=10, random_state=77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16    AdaBoostRegressor(base_estimator=ExtraTreesRegressor(max_features=50, n_jobs=4,\n",
      "                                                     random_state=50,\n",
      "                                                     verbose=4),\n",
      "                  n_estimators=10, random_state=78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17    AdaBoostRegressor(base_estimator=GradientBoostingRegressor(max_depth=5,\n",
      "                                                           max_features=50,\n",
      "                                                           random_state=50,\n",
      "                                                           verbose=4),\n",
      "                  n_estimators=10, random_state=79)\n",
      "18    AdaBoostRegressor(base_estimator=XGBRegressor(alpha=100, base_score=None,\n",
      "                                              booster=None,\n",
      "                                              colsample_bylevel=None,\n",
      "                                              colsample_bynode=None,\n",
      "                                              colsample_bytree=0.3, gamma=None,\n",
      "                                              gpu_id=None,\n",
      "                                              importance_type='gain',\n",
      "                                              interaction_constraints=None,\n",
      "                                              lambda=10, learning_rate=0.1,\n",
      "                                              max_delta_step=None,\n",
      "                                              max_depth=None,\n",
      "                                              min_child_weight=None,\n",
      "                                              missing=nan,\n",
      "                                              monotone_constraints=None,\n",
      "                                              n_estimators=100, n_jobs=None,\n",
      "                                              num_parallel_tree=None,\n",
      "                                              random_state=None, reg_alpha=None,\n",
      "                                              reg_lambda=None,\n",
      "                                              scale_pos_weight=None,\n",
      "                                              subsample=0.5, tree_method=None,\n",
      "                                              validate_parameters=None,\n",
      "                                              verbosity=None),\n",
      "                  n_estimators=10, random_state=80)\n",
      "19    BaggingRegressor(base_estimator=HuberRegressor(alpha=20, epsilon=1.7,\n",
      "                                               max_iter=20000),\n",
      "                 n_estimators=50, n_jobs=6, random_state=81, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:   10.5s remaining:   10.5s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:   10.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20    BaggingRegressor(base_estimator=Ridge(alpha=20000, max_iter=20000),\n",
      "                 n_estimators=50, n_jobs=6, random_state=82, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    6.5s remaining:    6.5s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21    BaggingRegressor(base_estimator=LinearSVR(C=10000, dual=False,\n",
      "                                          loss='squared_epsilon_insensitive',\n",
      "                                          random_state=10),\n",
      "                 n_estimators=50, n_jobs=6, random_state=83, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    6.2s remaining:    6.2s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    6.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22    BaggingRegressor(base_estimator=KNeighborsRegressor(algorithm='brute',\n",
      "                                                    n_neighbors=15,\n",
      "                                                    weights='distance'),\n",
      "                 n_estimators=8, n_jobs=6, random_state=85, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  6.9min remaining:  6.9min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  8.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23    BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=10,\n",
      "                                                      min_samples_split=200,\n",
      "                                                      random_state=40),\n",
      "                 n_estimators=50, n_jobs=6, random_state=86, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    6.7s remaining:    6.7s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    7.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24    BaggingRegressor(base_estimator=RandomForestRegressor(max_features=50,\n",
      "                                                      max_samples=0.95,\n",
      "                                                      n_jobs=4, random_state=40,\n",
      "                                                      verbose=4),\n",
      "                 n_jobs=6, random_state=87, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:   12.7s remaining:   12.7s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:   17.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25    BaggingRegressor(base_estimator=ExtraTreesRegressor(max_features=50, n_jobs=4,\n",
      "                                                    random_state=50,\n",
      "                                                    verbose=4),\n",
      "                 n_jobs=6, random_state=88, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:   14.0s remaining:   14.0s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:   20.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26    BaggingRegressor(base_estimator=GradientBoostingRegressor(max_depth=5,\n",
      "                                                          max_features=50,\n",
      "                                                          random_state=50,\n",
      "                                                          verbose=4),\n",
      "                 n_jobs=6, random_state=89, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    4.0s remaining:    4.0s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27    BaggingRegressor(base_estimator=XGBRegressor(alpha=100, base_score=None,\n",
      "                                             booster=None,\n",
      "                                             colsample_bylevel=None,\n",
      "                                             colsample_bynode=None,\n",
      "                                             colsample_bytree=0.3, gamma=None,\n",
      "                                             gpu_id=None,\n",
      "                                             importance_type='gain',\n",
      "                                             interaction_constraints=None,\n",
      "                                             lambda=10, learning_rate=0.1,\n",
      "                                             max_delta_step=None,\n",
      "                                             max_depth=None,\n",
      "                                             min_child_weight=None, missing=nan,\n",
      "                                             monotone_constraints=None,\n",
      "                                             n_estimators=100, n_jobs=None,\n",
      "                                             num_parallel_tree=None,\n",
      "                                             random_state=None, reg_alpha=None,\n",
      "                                             reg_lambda=None,\n",
      "                                             scale_pos_weight=None,\n",
      "                                             subsample=0.5, tree_method=None,\n",
      "                                             validate_parameters=None,\n",
      "                                             verbosity=None),\n",
      "                 n_estimators=6, n_jobs=6, random_state=90, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    7.2s remaining:    7.2s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28    VotingRegressor(estimators=[('ridge', Ridge(alpha=20000, max_iter=20000)),\n",
      "                            ('lsvr',\n",
      "                             LinearSVR(C=10000, dual=False,\n",
      "                                       loss='squared_epsilon_insensitive',\n",
      "                                       random_state=10)),\n",
      "                            ('svr', SVR(C=10000, max_iter=1000)),\n",
      "                            ('knr',\n",
      "                             KNeighborsRegressor(algorithm='brute',\n",
      "                                                 n_neighbors=15,\n",
      "                                                 weights='distance')),\n",
      "                            ('dtr',\n",
      "                             DecisionTreeRegressor(max_depth=10,\n",
      "                                                   min_samples_split=200,\n",
      "                                                   random_s...\n",
      "                                          interaction_constraints=None,\n",
      "                                          lambda=10, learning_rate=0.1,\n",
      "                                          max_delta_step=None, max_depth=None,\n",
      "                                          min_child_weight=None, missing=nan,\n",
      "                                          monotone_constraints=None,\n",
      "                                          n_estimators=100, n_jobs=None,\n",
      "                                          num_parallel_tree=None,\n",
      "                                          random_state=None, reg_alpha=None,\n",
      "                                          reg_lambda=None,\n",
      "                                          scale_pos_weight=None, subsample=0.5,\n",
      "                                          tree_method=None,\n",
      "                                          validate_parameters=None,\n",
      "                                          verbosity=None))],\n",
      "                n_jobs=6, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29    StackingRegressor(cv=2,\n",
      "                  estimators=[('ridge', Ridge(alpha=20000, max_iter=20000)),\n",
      "                              ('lsvr',\n",
      "                               LinearSVR(C=10000, dual=False,\n",
      "                                         loss='squared_epsilon_insensitive',\n",
      "                                         random_state=10)),\n",
      "                              ('svr', SVR(C=10000, max_iter=1000)),\n",
      "                              ('knr',\n",
      "                               KNeighborsRegressor(algorithm='brute',\n",
      "                                                   n_neighbors=15,\n",
      "                                                   weights='distance')),\n",
      "                              ('dtr',\n",
      "                               DecisionTreeRegressor(max_depth=10,\n",
      "                                                     min_samples_split=200,\n",
      "                                                     r...\n",
      "                                            interaction_constraints=None,\n",
      "                                            lambda=10, learning_rate=0.1,\n",
      "                                            max_delta_step=None, max_depth=None,\n",
      "                                            min_child_weight=None, missing=nan,\n",
      "                                            monotone_constraints=None,\n",
      "                                            n_estimators=100, n_jobs=None,\n",
      "                                            num_parallel_tree=None,\n",
      "                                            random_state=None, reg_alpha=None,\n",
      "                                            reg_lambda=None,\n",
      "                                            scale_pos_weight=None,\n",
      "                                            subsample=0.5, tree_method=None,\n",
      "                                            validate_parameters=None,\n",
      "                                            verbosity=None))],\n",
      "                  n_jobs=6, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "y_train_all_preds = []\n",
    "i = 0 \n",
    "for m in all_models_final:\n",
    "    print(i, '  ', m)\n",
    "    i += 1\n",
    "    y_train_all_preds.append(m.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_train_all_preds.joblib']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(y_train_all_preds, 'y_train_all_preds.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all_preds_array = np.zeros((len(y_clean), 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    y_train_all_preds_array[:,i] = y_train_all_preds[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arit_preds = np.mean(final_weights * y_train_all_preds_array, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    HuberRegressor(alpha=20, epsilon=1.7, max_iter=20000)\n",
      "1    Ridge(alpha=20000, max_iter=20000)\n",
      "2    LinearSVR(C=10000, dual=False, loss='squared_epsilon_insensitive',\n",
      "          random_state=10)\n",
      "3    SVR(C=10000)\n",
      "4    KNeighborsRegressor(algorithm='brute', n_neighbors=15, weights='distance')\n",
      "5    DecisionTreeRegressor(max_depth=10, min_samples_split=200, random_state=40)\n",
      "6    RandomForestRegressor(max_samples=0.95, n_jobs=4, random_state=40, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    ExtraTreesRegressor(n_jobs=4, random_state=50, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8    GradientBoostingRegressor(max_depth=5, random_state=50, verbose=4)\n",
      "9    XGBRegressor(alpha=100, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
      "             importance_type='gain', interaction_constraints='', lambda=10,\n",
      "             learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "             reg_alpha=100, reg_lambda=10, scale_pos_weight=1, subsample=0.5,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "10    AdaBoostRegressor(base_estimator=HuberRegressor(alpha=20, epsilon=1.7,\n",
      "                                                max_iter=20000),\n",
      "                  random_state=71)\n",
      "11    AdaBoostRegressor(base_estimator=Ridge(alpha=20000, max_iter=20000),\n",
      "                  random_state=72)\n",
      "12    AdaBoostRegressor(base_estimator=LinearSVR(C=10000, dual=False,\n",
      "                                           loss='squared_epsilon_insensitive',\n",
      "                                           random_state=10),\n",
      "                  random_state=73)\n",
      "13    AdaBoostRegressor(base_estimator=KNeighborsRegressor(algorithm='brute',\n",
      "                                                     n_neighbors=15,\n",
      "                                                     weights='distance'),\n",
      "                  n_estimators=10, random_state=75)\n",
      "14    AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=10,\n",
      "                                                       min_samples_split=200,\n",
      "                                                       random_state=40),\n",
      "                  random_state=76)\n",
      "15    AdaBoostRegressor(base_estimator=RandomForestRegressor(max_features=50,\n",
      "                                                       max_samples=0.95,\n",
      "                                                       n_jobs=4,\n",
      "                                                       random_state=40,\n",
      "                                                       verbose=4),\n",
      "                  n_estimators=10, random_state=77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16    AdaBoostRegressor(base_estimator=ExtraTreesRegressor(max_features=50, n_jobs=4,\n",
      "                                                     random_state=50,\n",
      "                                                     verbose=4),\n",
      "                  n_estimators=10, random_state=78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17    AdaBoostRegressor(base_estimator=GradientBoostingRegressor(max_depth=5,\n",
      "                                                           max_features=50,\n",
      "                                                           random_state=50,\n",
      "                                                           verbose=4),\n",
      "                  n_estimators=10, random_state=79)\n",
      "18    AdaBoostRegressor(base_estimator=XGBRegressor(alpha=100, base_score=None,\n",
      "                                              booster=None,\n",
      "                                              colsample_bylevel=None,\n",
      "                                              colsample_bynode=None,\n",
      "                                              colsample_bytree=0.3, gamma=None,\n",
      "                                              gpu_id=None,\n",
      "                                              importance_type='gain',\n",
      "                                              interaction_constraints=None,\n",
      "                                              lambda=10, learning_rate=0.1,\n",
      "                                              max_delta_step=None,\n",
      "                                              max_depth=None,\n",
      "                                              min_child_weight=None,\n",
      "                                              missing=nan,\n",
      "                                              monotone_constraints=None,\n",
      "                                              n_estimators=100, n_jobs=None,\n",
      "                                              num_parallel_tree=None,\n",
      "                                              random_state=None, reg_alpha=None,\n",
      "                                              reg_lambda=None,\n",
      "                                              scale_pos_weight=None,\n",
      "                                              subsample=0.5, tree_method=None,\n",
      "                                              validate_parameters=None,\n",
      "                                              verbosity=None),\n",
      "                  n_estimators=10, random_state=80)\n",
      "19    BaggingRegressor(base_estimator=HuberRegressor(alpha=20, epsilon=1.7,\n",
      "                                               max_iter=20000),\n",
      "                 n_estimators=50, n_jobs=6, random_state=81, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    4.9s remaining:    4.9s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20    BaggingRegressor(base_estimator=Ridge(alpha=20000, max_iter=20000),\n",
      "                 n_estimators=50, n_jobs=6, random_state=82, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    1.0s remaining:    1.0s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21    BaggingRegressor(base_estimator=LinearSVR(C=10000, dual=False,\n",
      "                                          loss='squared_epsilon_insensitive',\n",
      "                                          random_state=10),\n",
      "                 n_estimators=50, n_jobs=6, random_state=83, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    1.2s remaining:    1.2s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22    BaggingRegressor(base_estimator=KNeighborsRegressor(algorithm='brute',\n",
      "                                                    n_neighbors=15,\n",
      "                                                    weights='distance'),\n",
      "                 n_estimators=8, n_jobs=6, random_state=85, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:  2.1min remaining:  2.1min\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23    BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=10,\n",
      "                                                      min_samples_split=200,\n",
      "                                                      random_state=40),\n",
      "                 n_estimators=50, n_jobs=6, random_state=86, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    1.9s remaining:    1.9s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24    BaggingRegressor(base_estimator=RandomForestRegressor(max_features=50,\n",
      "                                                      max_samples=0.95,\n",
      "                                                      n_jobs=4, random_state=40,\n",
      "                                                      verbose=4),\n",
      "                 n_jobs=6, random_state=87, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    9.5s remaining:    9.5s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25    BaggingRegressor(base_estimator=ExtraTreesRegressor(max_features=50, n_jobs=4,\n",
      "                                                    random_state=50,\n",
      "                                                    verbose=4),\n",
      "                 n_jobs=6, random_state=88, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:   10.6s remaining:   10.6s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:   20.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26    BaggingRegressor(base_estimator=GradientBoostingRegressor(max_depth=5,\n",
      "                                                          max_features=50,\n",
      "                                                          random_state=50,\n",
      "                                                          verbose=4),\n",
      "                 n_jobs=6, random_state=89, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    0.6s remaining:    0.6s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27    BaggingRegressor(base_estimator=XGBRegressor(alpha=100, base_score=None,\n",
      "                                             booster=None,\n",
      "                                             colsample_bylevel=None,\n",
      "                                             colsample_bynode=None,\n",
      "                                             colsample_bytree=0.3, gamma=None,\n",
      "                                             gpu_id=None,\n",
      "                                             importance_type='gain',\n",
      "                                             interaction_constraints=None,\n",
      "                                             lambda=10, learning_rate=0.1,\n",
      "                                             max_delta_step=None,\n",
      "                                             max_depth=None,\n",
      "                                             min_child_weight=None, missing=nan,\n",
      "                                             monotone_constraints=None,\n",
      "                                             n_estimators=100, n_jobs=None,\n",
      "                                             num_parallel_tree=None,\n",
      "                                             random_state=None, reg_alpha=None,\n",
      "                                             reg_lambda=None,\n",
      "                                             scale_pos_weight=None,\n",
      "                                             subsample=0.5, tree_method=None,\n",
      "                                             validate_parameters=None,\n",
      "                                             verbosity=None),\n",
      "                 n_estimators=6, n_jobs=6, random_state=90, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   3 out of   6 | elapsed:    2.8s remaining:    2.8s\n",
      "[Parallel(n_jobs=6)]: Done   6 out of   6 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28    VotingRegressor(estimators=[('ridge', Ridge(alpha=20000, max_iter=20000)),\n",
      "                            ('lsvr',\n",
      "                             LinearSVR(C=10000, dual=False,\n",
      "                                       loss='squared_epsilon_insensitive',\n",
      "                                       random_state=10)),\n",
      "                            ('svr', SVR(C=10000, max_iter=1000)),\n",
      "                            ('knr',\n",
      "                             KNeighborsRegressor(algorithm='brute',\n",
      "                                                 n_neighbors=15,\n",
      "                                                 weights='distance')),\n",
      "                            ('dtr',\n",
      "                             DecisionTreeRegressor(max_depth=10,\n",
      "                                                   min_samples_split=200,\n",
      "                                                   random_s...\n",
      "                                          interaction_constraints=None,\n",
      "                                          lambda=10, learning_rate=0.1,\n",
      "                                          max_delta_step=None, max_depth=None,\n",
      "                                          min_child_weight=None, missing=nan,\n",
      "                                          monotone_constraints=None,\n",
      "                                          n_estimators=100, n_jobs=None,\n",
      "                                          num_parallel_tree=None,\n",
      "                                          random_state=None, reg_alpha=None,\n",
      "                                          reg_lambda=None,\n",
      "                                          scale_pos_weight=None, subsample=0.5,\n",
      "                                          tree_method=None,\n",
      "                                          validate_parameters=None,\n",
      "                                          verbosity=None))],\n",
      "                n_jobs=6, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29    StackingRegressor(cv=2,\n",
      "                  estimators=[('ridge', Ridge(alpha=20000, max_iter=20000)),\n",
      "                              ('lsvr',\n",
      "                               LinearSVR(C=10000, dual=False,\n",
      "                                         loss='squared_epsilon_insensitive',\n",
      "                                         random_state=10)),\n",
      "                              ('svr', SVR(C=10000, max_iter=1000)),\n",
      "                              ('knr',\n",
      "                               KNeighborsRegressor(algorithm='brute',\n",
      "                                                   n_neighbors=15,\n",
      "                                                   weights='distance')),\n",
      "                              ('dtr',\n",
      "                               DecisionTreeRegressor(max_depth=10,\n",
      "                                                     min_samples_split=200,\n",
      "                                                     r...\n",
      "                                            interaction_constraints=None,\n",
      "                                            lambda=10, learning_rate=0.1,\n",
      "                                            max_delta_step=None, max_depth=None,\n",
      "                                            min_child_weight=None, missing=nan,\n",
      "                                            monotone_constraints=None,\n",
      "                                            n_estimators=100, n_jobs=None,\n",
      "                                            num_parallel_tree=None,\n",
      "                                            random_state=None, reg_alpha=None,\n",
      "                                            reg_lambda=None,\n",
      "                                            scale_pos_weight=None,\n",
      "                                            subsample=0.5, tree_method=None,\n",
      "                                            validate_parameters=None,\n",
      "                                            verbosity=None))],\n",
      "                  n_jobs=6, verbose=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "y_test_all_preds = []\n",
    "i = 0 \n",
    "for m in all_models_final:\n",
    "    print(i, '  ', m)\n",
    "    i += 1\n",
    "    y_test_all_preds.append(m.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_test_all_preds.joblib']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(y_test_all_preds, 'y_test_all_preds.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_all_preds_array = np.zeros((len(X_test), 30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    y_test_all_preds_array[:,i] = y_test_all_preds[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "arit_preds_test = np.mean(final_weights * y_test_all_preds_array, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"arit_preds_test4.csv\", np.exp(arit_preds_test), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arit_preds_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arit_preds_test[arit_preds_test > 30000000] = 30000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1, Current best: 34.66306442962184, Global best: 34.66306442962184, Runtime: 7.27019 seconds\n",
      "> Epoch: 2, Current best: 67.01218353847783, Global best: 34.66306442962184, Runtime: 7.79269 seconds\n",
      "> Epoch: 3, Current best: 22.860564892386005, Global best: 22.860564892386005, Runtime: 8.23295 seconds\n",
      "> Epoch: 4, Current best: 5.4516343599631565, Global best: 5.4516343599631565, Runtime: 7.17402 seconds\n",
      "> Epoch: 5, Current best: 59.48127924222323, Global best: 5.4516343599631565, Runtime: 6.96276 seconds\n",
      "> Epoch: 6, Current best: 35.41507957369352, Global best: 5.4516343599631565, Runtime: 7.29399 seconds\n",
      "> Epoch: 7, Current best: 5.263556048185477, Global best: 5.263556048185477, Runtime: 7.09567 seconds\n",
      "> Epoch: 8, Current best: 22.743720640850693, Global best: 5.263556048185477, Runtime: 7.16002 seconds\n",
      "> Epoch: 9, Current best: 0.7198014100945629, Global best: 0.7198014100945629, Runtime: 7.24507 seconds\n",
      "> Epoch: 10, Current best: 5.1736500163683345, Global best: 0.7198014100945629, Runtime: 7.49762 seconds\n",
      "> Epoch: 11, Current best: 0.7352332334026812, Global best: 0.7198014100945629, Runtime: 7.15470 seconds\n",
      "> Epoch: 12, Current best: 26.996106180415925, Global best: 0.7198014100945629, Runtime: 8.61853 seconds\n",
      "> Epoch: 13, Current best: 16.443907881364925, Global best: 0.7198014100945629, Runtime: 7.45976 seconds\n",
      "> Epoch: 14, Current best: 15.225038221143299, Global best: 0.7198014100945629, Runtime: 7.37833 seconds\n",
      "> Epoch: 15, Current best: 28.999101812518386, Global best: 0.7198014100945629, Runtime: 7.39377 seconds\n",
      "> Epoch: 16, Current best: 54.134822499946296, Global best: 0.7198014100945629, Runtime: 7.29253 seconds\n",
      "> Epoch: 17, Current best: 70.37391895442646, Global best: 0.7198014100945629, Runtime: 7.43507 seconds\n",
      "> Epoch: 18, Current best: 11.321063159780605, Global best: 0.7198014100945629, Runtime: 8.34145 seconds\n",
      "> Epoch: 19, Current best: 6.5058633361044516, Global best: 0.7198014100945629, Runtime: 7.21206 seconds\n",
      "> Epoch: 20, Current best: 34.65416256318862, Global best: 0.7198014100945629, Runtime: 7.27481 seconds\n",
      "> Epoch: 21, Current best: 22.97896364444459, Global best: 0.7198014100945629, Runtime: 7.41172 seconds\n",
      "> Epoch: 22, Current best: 62.84186747910647, Global best: 0.7198014100945629, Runtime: 7.35949 seconds\n",
      "> Epoch: 23, Current best: 62.51780717352765, Global best: 0.7198014100945629, Runtime: 7.29055 seconds\n",
      "> Epoch: 24, Current best: 41.85362514409794, Global best: 0.7198014100945629, Runtime: 8.11459 seconds\n",
      "> Epoch: 25, Current best: 29.816197571297955, Global best: 0.7198014100945629, Runtime: 7.37571 seconds\n",
      "> Epoch: 26, Current best: 109.70395383886236, Global best: 0.7198014100945629, Runtime: 7.54332 seconds\n",
      "> Epoch: 27, Current best: 25.712873812395138, Global best: 0.7198014100945629, Runtime: 8.65822 seconds\n",
      "> Epoch: 28, Current best: 3.060057915822247, Global best: 0.7198014100945629, Runtime: 7.44364 seconds\n",
      "> Epoch: 29, Current best: 28.267419300771106, Global best: 0.7198014100945629, Runtime: 7.44459 seconds\n",
      "> Epoch: 30, Current best: 15.768352873337914, Global best: 0.7198014100945629, Runtime: 7.37553 seconds\n",
      "> Epoch: 31, Current best: 32.216583441700344, Global best: 0.7198014100945629, Runtime: 7.16088 seconds\n",
      "> Epoch: 32, Current best: 54.625229563550306, Global best: 0.7198014100945629, Runtime: 7.18476 seconds\n",
      "> Epoch: 33, Current best: 22.804322133512606, Global best: 0.7198014100945629, Runtime: 7.64977 seconds\n",
      "> Epoch: 34, Current best: 32.95064234751337, Global best: 0.7198014100945629, Runtime: 7.90254 seconds\n",
      "> Epoch: 35, Current best: 111.83336462530148, Global best: 0.7198014100945629, Runtime: 7.47179 seconds\n",
      "> Epoch: 36, Current best: 17.96533221349465, Global best: 0.7198014100945629, Runtime: 7.37161 seconds\n",
      "> Epoch: 37, Current best: 28.938884094044017, Global best: 0.7198014100945629, Runtime: 7.21452 seconds\n",
      "> Epoch: 38, Current best: 51.3859917734855, Global best: 0.7198014100945629, Runtime: 7.17258 seconds\n",
      "> Epoch: 39, Current best: 40.71793074528923, Global best: 0.7198014100945629, Runtime: 7.29306 seconds\n",
      "> Epoch: 40, Current best: 4.756336851983581, Global best: 0.7198014100945629, Runtime: 7.24671 seconds\n",
      "> Epoch: 41, Current best: 10.301671785288233, Global best: 0.7198014100945629, Runtime: 7.39135 seconds\n",
      "> Epoch: 42, Current best: 7.7738841659384965, Global best: 0.7198014100945629, Runtime: 7.60604 seconds\n",
      "> Epoch: 43, Current best: 13.346066287249634, Global best: 0.7198014100945629, Runtime: 7.95646 seconds\n",
      "> Epoch: 44, Current best: 3.388111288573941, Global best: 0.7198014100945629, Runtime: 9.30141 seconds\n",
      "> Epoch: 45, Current best: 29.497981360527394, Global best: 0.7198014100945629, Runtime: 7.26513 seconds\n",
      "> Epoch: 46, Current best: 23.25973809440525, Global best: 0.7198014100945629, Runtime: 7.41022 seconds\n",
      "> Epoch: 47, Current best: 7.466009912849714, Global best: 0.7198014100945629, Runtime: 9.83988 seconds\n",
      "> Epoch: 48, Current best: 14.692694188817834, Global best: 0.7198014100945629, Runtime: 7.37883 seconds\n",
      "> Epoch: 49, Current best: 28.99148492064928, Global best: 0.7198014100945629, Runtime: 7.29767 seconds\n",
      "> Epoch: 50, Current best: 8.748167400805045, Global best: 0.7198014100945629, Runtime: 7.50685 seconds\n",
      "> Epoch: 51, Current best: 7.719786263647687, Global best: 0.7198014100945629, Runtime: 7.29585 seconds\n",
      "> Epoch: 52, Current best: 17.196310968957434, Global best: 0.7198014100945629, Runtime: 7.34573 seconds\n",
      "> Epoch: 53, Current best: 18.790789776249195, Global best: 0.7198014100945629, Runtime: 7.35639 seconds\n",
      "> Epoch: 54, Current best: 8.92165180418513, Global best: 0.7198014100945629, Runtime: 7.41726 seconds\n",
      "> Epoch: 55, Current best: 80.38540670076505, Global best: 0.7198014100945629, Runtime: 7.82770 seconds\n",
      "> Epoch: 56, Current best: 6.986758695884144, Global best: 0.7198014100945629, Runtime: 7.44793 seconds\n",
      "> Epoch: 57, Current best: 14.755555837758475, Global best: 0.7198014100945629, Runtime: 7.64707 seconds\n",
      "> Epoch: 58, Current best: 2.7560485826494014, Global best: 0.7198014100945629, Runtime: 8.48579 seconds\n",
      "> Epoch: 59, Current best: 17.671190174143117, Global best: 0.7198014100945629, Runtime: 7.30285 seconds\n",
      "> Epoch: 60, Current best: 1.7704391336799699, Global best: 0.7198014100945629, Runtime: 8.54760 seconds\n",
      "> Epoch: 61, Current best: 3.5517928296386465, Global best: 0.7198014100945629, Runtime: 7.59708 seconds\n",
      "> Epoch: 62, Current best: 51.85033968879734, Global best: 0.7198014100945629, Runtime: 7.22742 seconds\n",
      "> Epoch: 63, Current best: 6.521908873473532, Global best: 0.7198014100945629, Runtime: 7.45671 seconds\n",
      "> Epoch: 64, Current best: 11.847032076699563, Global best: 0.7198014100945629, Runtime: 8.88334 seconds\n",
      "> Epoch: 65, Current best: 18.618734095228444, Global best: 0.7198014100945629, Runtime: 8.74417 seconds\n",
      "> Epoch: 66, Current best: 44.40668032259638, Global best: 0.7198014100945629, Runtime: 8.17144 seconds\n",
      "> Epoch: 67, Current best: 4.848651513962449, Global best: 0.7198014100945629, Runtime: 9.55267 seconds\n",
      "> Epoch: 68, Current best: 37.87286198288266, Global best: 0.7198014100945629, Runtime: 8.87615 seconds\n",
      "> Epoch: 69, Current best: 13.674712379581715, Global best: 0.7198014100945629, Runtime: 8.64017 seconds\n",
      "> Epoch: 70, Current best: 24.120604359056966, Global best: 0.7198014100945629, Runtime: 8.79474 seconds\n",
      "> Epoch: 71, Current best: 14.515840339718334, Global best: 0.7198014100945629, Runtime: 8.55779 seconds\n",
      "> Epoch: 72, Current best: 6.976686006082071, Global best: 0.7198014100945629, Runtime: 8.90220 seconds\n",
      "> Epoch: 73, Current best: 38.97711374430232, Global best: 0.7198014100945629, Runtime: 8.42512 seconds\n",
      "> Epoch: 74, Current best: 0.9577134695213179, Global best: 0.7198014100945629, Runtime: 7.57170 seconds\n",
      "> Epoch: 75, Current best: 14.730934836405202, Global best: 0.7198014100945629, Runtime: 8.67169 seconds\n",
      "> Epoch: 76, Current best: 76.64337681332776, Global best: 0.7198014100945629, Runtime: 8.25925 seconds\n",
      "> Epoch: 77, Current best: 24.441282853585488, Global best: 0.7198014100945629, Runtime: 7.99863 seconds\n",
      "> Epoch: 78, Current best: 2.5771062125800537, Global best: 0.7198014100945629, Runtime: 7.81812 seconds\n",
      "> Epoch: 79, Current best: 3.590421201774455, Global best: 0.7198014100945629, Runtime: 8.09804 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 80, Current best: 4.571038473010397, Global best: 0.7198014100945629, Runtime: 7.86559 seconds\n",
      "> Epoch: 81, Current best: 1.3723114826950782, Global best: 0.7198014100945629, Runtime: 7.42355 seconds\n",
      "> Epoch: 82, Current best: 14.581408058847122, Global best: 0.7198014100945629, Runtime: 7.22398 seconds\n",
      "> Epoch: 83, Current best: 12.223457543531218, Global best: 0.7198014100945629, Runtime: 7.32750 seconds\n",
      "> Epoch: 84, Current best: 15.609020625834287, Global best: 0.7198014100945629, Runtime: 7.33264 seconds\n",
      "> Epoch: 85, Current best: 14.252268701407163, Global best: 0.7198014100945629, Runtime: 7.37630 seconds\n",
      "> Epoch: 86, Current best: 11.478863120081495, Global best: 0.7198014100945629, Runtime: 7.37738 seconds\n",
      "> Epoch: 87, Current best: 133.9026349655735, Global best: 0.7198014100945629, Runtime: 7.48678 seconds\n",
      "> Epoch: 88, Current best: 6.644335830815878, Global best: 0.7198014100945629, Runtime: 8.44779 seconds\n",
      "> Epoch: 89, Current best: 2.0508244676728347, Global best: 0.7198014100945629, Runtime: 7.40012 seconds\n",
      "> Epoch: 90, Current best: 30.31154022178677, Global best: 0.7198014100945629, Runtime: 7.48994 seconds\n",
      "> Epoch: 91, Current best: 56.611028827430445, Global best: 0.7198014100945629, Runtime: 7.50363 seconds\n",
      "> Epoch: 92, Current best: 40.274968503824674, Global best: 0.7198014100945629, Runtime: 7.47880 seconds\n",
      "> Epoch: 93, Current best: 3.3930500520317706, Global best: 0.7198014100945629, Runtime: 7.92426 seconds\n",
      "> Epoch: 94, Current best: 3.182101163229368, Global best: 0.7198014100945629, Runtime: 7.53369 seconds\n",
      "> Epoch: 95, Current best: 45.721780015301384, Global best: 0.7198014100945629, Runtime: 7.68914 seconds\n",
      "> Epoch: 96, Current best: 20.509930673641836, Global best: 0.7198014100945629, Runtime: 7.47337 seconds\n",
      "> Epoch: 97, Current best: 4.896815669268775, Global best: 0.7198014100945629, Runtime: 7.97133 seconds\n",
      "> Epoch: 98, Current best: 5.790569566702706, Global best: 0.7198014100945629, Runtime: 9.04029 seconds\n",
      "> Epoch: 99, Current best: 13.722776087686556, Global best: 0.7198014100945629, Runtime: 8.74002 seconds\n",
      "> Epoch: 100, Current best: 41.25576379591164, Global best: 0.7198014100945629, Runtime: 8.05100 seconds\n",
      "> Epoch: 101, Current best: 3.691017099745046, Global best: 0.7198014100945629, Runtime: 9.06770 seconds\n",
      "> Epoch: 102, Current best: 11.004507531527922, Global best: 0.7198014100945629, Runtime: 8.63908 seconds\n",
      "> Epoch: 103, Current best: 1.8740277659780038, Global best: 0.7198014100945629, Runtime: 8.46130 seconds\n",
      "> Epoch: 104, Current best: 6.4059414193152735, Global best: 0.7198014100945629, Runtime: 9.51355 seconds\n",
      "> Epoch: 105, Current best: 35.80948884827223, Global best: 0.7198014100945629, Runtime: 7.59437 seconds\n",
      "> Epoch: 106, Current best: 5.453662199779334, Global best: 0.7198014100945629, Runtime: 7.48815 seconds\n",
      "> Epoch: 107, Current best: 15.17962667434513, Global best: 0.7198014100945629, Runtime: 7.33192 seconds\n",
      "> Epoch: 108, Current best: 32.914966152041565, Global best: 0.7198014100945629, Runtime: 7.49332 seconds\n",
      "> Epoch: 109, Current best: 59.11605665516576, Global best: 0.7198014100945629, Runtime: 7.15472 seconds\n",
      "> Epoch: 110, Current best: 13.55021074482469, Global best: 0.7198014100945629, Runtime: 7.41287 seconds\n",
      "> Epoch: 111, Current best: 3.855423797200338, Global best: 0.7198014100945629, Runtime: 7.07004 seconds\n",
      "> Epoch: 112, Current best: 59.13799618885785, Global best: 0.7198014100945629, Runtime: 7.07568 seconds\n",
      "> Epoch: 113, Current best: 485.7373320192508, Global best: 0.7198014100945629, Runtime: 7.17754 seconds\n",
      "> Epoch: 114, Current best: 9.891185041602812, Global best: 0.7198014100945629, Runtime: 7.06203 seconds\n",
      "> Epoch: 115, Current best: 64.39257360004476, Global best: 0.7198014100945629, Runtime: 6.92473 seconds\n",
      "> Epoch: 116, Current best: 2.9123432860046172, Global best: 0.7198014100945629, Runtime: 6.90062 seconds\n",
      "> Epoch: 117, Current best: 25.9129686567534, Global best: 0.7198014100945629, Runtime: 7.01579 seconds\n",
      "> Epoch: 118, Current best: 91.59012000426777, Global best: 0.7198014100945629, Runtime: 7.21721 seconds\n",
      "> Epoch: 119, Current best: 25.48888490328836, Global best: 0.7198014100945629, Runtime: 7.26499 seconds\n",
      "> Epoch: 120, Current best: 27.314794886768805, Global best: 0.7198014100945629, Runtime: 6.95697 seconds\n",
      "> Epoch: 121, Current best: 3.9912712423962886, Global best: 0.7198014100945629, Runtime: 7.59932 seconds\n",
      "> Epoch: 122, Current best: 53.46381758718177, Global best: 0.7198014100945629, Runtime: 8.95509 seconds\n",
      "> Epoch: 123, Current best: 17.84724004280946, Global best: 0.7198014100945629, Runtime: 8.69004 seconds\n",
      "> Epoch: 124, Current best: 16.06103642514753, Global best: 0.7198014100945629, Runtime: 8.20365 seconds\n",
      "> Epoch: 125, Current best: 25.76141717460334, Global best: 0.7198014100945629, Runtime: 8.47348 seconds\n",
      "> Epoch: 126, Current best: 24.51075941562, Global best: 0.7198014100945629, Runtime: 8.28898 seconds\n",
      "> Epoch: 127, Current best: 24.056625365554655, Global best: 0.7198014100945629, Runtime: 7.95170 seconds\n",
      "> Epoch: 128, Current best: 4.713671266947846, Global best: 0.7198014100945629, Runtime: 7.74738 seconds\n",
      "> Epoch: 129, Current best: 43.58551246106388, Global best: 0.7198014100945629, Runtime: 8.16223 seconds\n",
      "> Epoch: 130, Current best: 9.36333465705059, Global best: 0.7198014100945629, Runtime: 12.28748 seconds\n",
      "> Epoch: 131, Current best: 7.994253662711637, Global best: 0.7198014100945629, Runtime: 8.45112 seconds\n",
      "> Epoch: 132, Current best: 25.54622554357015, Global best: 0.7198014100945629, Runtime: 7.85975 seconds\n",
      "> Epoch: 133, Current best: 93.9765108131645, Global best: 0.7198014100945629, Runtime: 7.57616 seconds\n",
      "> Epoch: 134, Current best: 33.65802704861025, Global best: 0.7198014100945629, Runtime: 7.40357 seconds\n",
      "> Epoch: 135, Current best: 0.830095841602913, Global best: 0.7198014100945629, Runtime: 7.50790 seconds\n",
      "> Epoch: 136, Current best: 72.70038116553654, Global best: 0.7198014100945629, Runtime: 7.58273 seconds\n",
      "> Epoch: 137, Current best: 23.550632377852892, Global best: 0.7198014100945629, Runtime: 7.24290 seconds\n",
      "> Epoch: 138, Current best: 17.136287651158334, Global best: 0.7198014100945629, Runtime: 7.34479 seconds\n",
      "> Epoch: 139, Current best: 15.718912774108924, Global best: 0.7198014100945629, Runtime: 7.35426 seconds\n",
      "> Epoch: 140, Current best: 21.930344599086165, Global best: 0.7198014100945629, Runtime: 9.50128 seconds\n",
      "> Epoch: 141, Current best: 1.4187932034053723, Global best: 0.7198014100945629, Runtime: 8.30580 seconds\n",
      "> Epoch: 142, Current best: 31.678130077506875, Global best: 0.7198014100945629, Runtime: 8.39690 seconds\n",
      "> Epoch: 143, Current best: 64.0675002952595, Global best: 0.7198014100945629, Runtime: 8.43774 seconds\n",
      "> Epoch: 144, Current best: 4.0641119113179025, Global best: 0.7198014100945629, Runtime: 8.21360 seconds\n",
      "> Epoch: 145, Current best: 3.336012016665926, Global best: 0.7198014100945629, Runtime: 8.39147 seconds\n",
      "> Epoch: 146, Current best: 6.297509987928807, Global best: 0.7198014100945629, Runtime: 7.26405 seconds\n",
      "> Epoch: 147, Current best: 18.235232005657565, Global best: 0.7198014100945629, Runtime: 7.09512 seconds\n",
      "> Epoch: 148, Current best: 17.476781959472298, Global best: 0.7198014100945629, Runtime: 7.27804 seconds\n",
      "> Epoch: 149, Current best: 38.6606185455525, Global best: 0.7198014100945629, Runtime: 7.25971 seconds\n",
      "> Epoch: 150, Current best: 9.07844335125386, Global best: 0.7198014100945629, Runtime: 7.48634 seconds\n",
      "> Epoch: 151, Current best: 7.475846673781801, Global best: 0.7198014100945629, Runtime: 7.21421 seconds\n",
      "> Epoch: 152, Current best: 0.6594276452817621, Global best: 0.6594276452817621, Runtime: 7.19644 seconds\n",
      "> Epoch: 153, Current best: 5.866709634525397, Global best: 0.6594276452817621, Runtime: 7.04149 seconds\n",
      "> Epoch: 154, Current best: 50.80724014561946, Global best: 0.6594276452817621, Runtime: 6.93087 seconds\n",
      "> Epoch: 155, Current best: 15.55768233496696, Global best: 0.6594276452817621, Runtime: 7.06501 seconds\n",
      "> Epoch: 156, Current best: 3.659270801330774, Global best: 0.6594276452817621, Runtime: 7.26319 seconds\n",
      "> Epoch: 157, Current best: 45.5881709356139, Global best: 0.6594276452817621, Runtime: 7.28234 seconds\n",
      "> Epoch: 158, Current best: 11.388092033255935, Global best: 0.6594276452817621, Runtime: 7.24969 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 159, Current best: 16.594272220648786, Global best: 0.6594276452817621, Runtime: 7.01290 seconds\n",
      "> Epoch: 160, Current best: 16.342540962484122, Global best: 0.6594276452817621, Runtime: 7.01748 seconds\n",
      "> Epoch: 161, Current best: 54.05204831435958, Global best: 0.6594276452817621, Runtime: 7.03671 seconds\n",
      "> Epoch: 162, Current best: 6.269306683260376, Global best: 0.6594276452817621, Runtime: 7.06926 seconds\n",
      "> Epoch: 163, Current best: 3.0984593796160484, Global best: 0.6594276452817621, Runtime: 7.63038 seconds\n",
      "> Epoch: 164, Current best: 4.786662942541068, Global best: 0.6594276452817621, Runtime: 8.71700 seconds\n",
      "> Epoch: 165, Current best: 4.256776280788471, Global best: 0.6594276452817621, Runtime: 9.96725 seconds\n",
      "> Epoch: 166, Current best: 3.229384218860104, Global best: 0.6594276452817621, Runtime: 8.72716 seconds\n",
      "> Epoch: 167, Current best: 4.702270812266094, Global best: 0.6594276452817621, Runtime: 8.06325 seconds\n",
      "> Epoch: 168, Current best: 8.688827809105092, Global best: 0.6594276452817621, Runtime: 7.96016 seconds\n",
      "> Epoch: 169, Current best: 7.617209055218896, Global best: 0.6594276452817621, Runtime: 8.83762 seconds\n",
      "> Epoch: 170, Current best: 84.4036339857249, Global best: 0.6594276452817621, Runtime: 8.22742 seconds\n",
      "> Epoch: 171, Current best: 7.974476383637948, Global best: 0.6594276452817621, Runtime: 8.70780 seconds\n",
      "> Epoch: 172, Current best: 4.353791308069207, Global best: 0.6594276452817621, Runtime: 7.32315 seconds\n",
      "> Epoch: 173, Current best: 8.358751326262478, Global best: 0.6594276452817621, Runtime: 8.32704 seconds\n",
      "> Epoch: 174, Current best: 29.75037696293846, Global best: 0.6594276452817621, Runtime: 8.89670 seconds\n",
      "> Epoch: 175, Current best: 13.384265965336532, Global best: 0.6594276452817621, Runtime: 7.99719 seconds\n",
      "> Epoch: 176, Current best: 62.3094931305865, Global best: 0.6594276452817621, Runtime: 7.38916 seconds\n",
      "> Epoch: 177, Current best: 6.32574004523005, Global best: 0.6594276452817621, Runtime: 7.73311 seconds\n",
      "> Epoch: 178, Current best: 29.42543985574836, Global best: 0.6594276452817621, Runtime: 8.85743 seconds\n",
      "> Epoch: 179, Current best: 52.904195430697975, Global best: 0.6594276452817621, Runtime: 8.48001 seconds\n",
      "> Epoch: 180, Current best: 14.102855669172536, Global best: 0.6594276452817621, Runtime: 8.53509 seconds\n",
      "> Epoch: 181, Current best: 28.732616791297446, Global best: 0.6594276452817621, Runtime: 8.50176 seconds\n",
      "> Epoch: 182, Current best: 23.015426125731434, Global best: 0.6594276452817621, Runtime: 9.36765 seconds\n",
      "> Epoch: 183, Current best: 14.325419211196158, Global best: 0.6594276452817621, Runtime: 9.25310 seconds\n",
      "> Epoch: 184, Current best: 13.875813788505779, Global best: 0.6594276452817621, Runtime: 8.31049 seconds\n",
      "> Epoch: 185, Current best: 5.3531888439464765, Global best: 0.6594276452817621, Runtime: 8.18532 seconds\n",
      "> Epoch: 186, Current best: 0.5728469146888064, Global best: 0.5728469146888064, Runtime: 7.29600 seconds\n",
      "> Epoch: 187, Current best: 17.63233519349871, Global best: 0.5728469146888064, Runtime: 7.15239 seconds\n",
      "> Epoch: 188, Current best: 24.65475375912595, Global best: 0.5728469146888064, Runtime: 7.22005 seconds\n",
      "> Epoch: 189, Current best: 2.721560921725808, Global best: 0.5728469146888064, Runtime: 8.00100 seconds\n",
      "> Epoch: 190, Current best: 5.631072784917282, Global best: 0.5728469146888064, Runtime: 8.47441 seconds\n",
      "> Epoch: 191, Current best: 6.7944353722052275, Global best: 0.5728469146888064, Runtime: 8.11254 seconds\n",
      "> Epoch: 192, Current best: 20.17529765926982, Global best: 0.5728469146888064, Runtime: 8.09903 seconds\n",
      "> Epoch: 193, Current best: 19.981170936828327, Global best: 0.5728469146888064, Runtime: 8.44106 seconds\n",
      "> Epoch: 194, Current best: 9.430667203851083, Global best: 0.5728469146888064, Runtime: 7.21739 seconds\n",
      "> Epoch: 195, Current best: 7.9575617578064985, Global best: 0.5728469146888064, Runtime: 7.12179 seconds\n",
      "> Epoch: 196, Current best: 7.353922823989949, Global best: 0.5728469146888064, Runtime: 7.18960 seconds\n",
      "> Epoch: 197, Current best: 14.365856848916145, Global best: 0.5728469146888064, Runtime: 7.47508 seconds\n",
      "> Epoch: 198, Current best: 4.306613825635956, Global best: 0.5728469146888064, Runtime: 8.27604 seconds\n",
      "> Epoch: 199, Current best: 3.6716842637622396, Global best: 0.5728469146888064, Runtime: 7.51631 seconds\n",
      "> Epoch: 200, Current best: 26.06634294897679, Global best: 0.5728469146888064, Runtime: 7.43571 seconds\n",
      "> Epoch: 201, Current best: 5.001758051698195, Global best: 0.5728469146888064, Runtime: 7.45163 seconds\n",
      "> Epoch: 202, Current best: 2.3472875207854758, Global best: 0.5728469146888064, Runtime: 8.14920 seconds\n",
      "> Epoch: 203, Current best: 42.53042124566793, Global best: 0.5728469146888064, Runtime: 7.68590 seconds\n",
      "> Epoch: 204, Current best: 6.524222034083857, Global best: 0.5728469146888064, Runtime: 9.67397 seconds\n",
      "> Epoch: 205, Current best: 21.537336004938776, Global best: 0.5728469146888064, Runtime: 7.88477 seconds\n",
      "> Epoch: 206, Current best: 6.15134744090085, Global best: 0.5728469146888064, Runtime: 8.33204 seconds\n",
      "> Epoch: 207, Current best: 3.452249871798261, Global best: 0.5728469146888064, Runtime: 8.16703 seconds\n",
      "> Epoch: 208, Current best: 47.25207481925513, Global best: 0.5728469146888064, Runtime: 9.01342 seconds\n",
      "> Epoch: 209, Current best: 20.238978780962604, Global best: 0.5728469146888064, Runtime: 10.02944 seconds\n",
      "> Epoch: 210, Current best: 4.912516384409724, Global best: 0.5728469146888064, Runtime: 9.30922 seconds\n",
      "> Epoch: 211, Current best: 20.964868881123873, Global best: 0.5728469146888064, Runtime: 7.65799 seconds\n",
      "> Epoch: 212, Current best: 6.670393489000266, Global best: 0.5728469146888064, Runtime: 7.38890 seconds\n",
      "> Epoch: 213, Current best: 13.414412302488184, Global best: 0.5728469146888064, Runtime: 7.95095 seconds\n",
      "> Epoch: 214, Current best: 16.712187201129254, Global best: 0.5728469146888064, Runtime: 8.32922 seconds\n",
      "> Epoch: 215, Current best: 16.697887664645368, Global best: 0.5728469146888064, Runtime: 9.21174 seconds\n",
      "> Epoch: 216, Current best: 16.739872103689898, Global best: 0.5728469146888064, Runtime: 9.09927 seconds\n",
      "> Epoch: 217, Current best: 3.2668176407833722, Global best: 0.5728469146888064, Runtime: 8.00683 seconds\n",
      "> Epoch: 218, Current best: 13.370950914948565, Global best: 0.5728469146888064, Runtime: 8.94920 seconds\n",
      "> Epoch: 219, Current best: 21.399135773584042, Global best: 0.5728469146888064, Runtime: 7.65841 seconds\n",
      "> Epoch: 220, Current best: 28.163757768629, Global best: 0.5728469146888064, Runtime: 8.66657 seconds\n",
      "> Epoch: 221, Current best: 12.210483300269047, Global best: 0.5728469146888064, Runtime: 8.48021 seconds\n",
      "> Epoch: 222, Current best: 4.336497754692049, Global best: 0.5728469146888064, Runtime: 7.77175 seconds\n",
      "> Epoch: 223, Current best: 8.652654621520966, Global best: 0.5728469146888064, Runtime: 7.91013 seconds\n",
      "> Epoch: 224, Current best: 24.539468668393308, Global best: 0.5728469146888064, Runtime: 7.94041 seconds\n",
      "> Epoch: 225, Current best: 0.7169122513743044, Global best: 0.5728469146888064, Runtime: 7.52590 seconds\n",
      "> Epoch: 226, Current best: 7.682435649322956, Global best: 0.5728469146888064, Runtime: 7.35514 seconds\n",
      "> Epoch: 227, Current best: 9.918494951989606, Global best: 0.5728469146888064, Runtime: 7.21794 seconds\n",
      "> Epoch: 228, Current best: 26.24741406303685, Global best: 0.5728469146888064, Runtime: 7.18284 seconds\n",
      "> Epoch: 229, Current best: 30.78406765344757, Global best: 0.5728469146888064, Runtime: 7.39719 seconds\n",
      "> Epoch: 230, Current best: 5.554819079762784, Global best: 0.5728469146888064, Runtime: 7.18420 seconds\n",
      "> Epoch: 231, Current best: 12.505215852807192, Global best: 0.5728469146888064, Runtime: 7.58480 seconds\n",
      "> Epoch: 232, Current best: 5.391845833338446, Global best: 0.5728469146888064, Runtime: 7.44752 seconds\n",
      "> Epoch: 233, Current best: 59.271405773682474, Global best: 0.5728469146888064, Runtime: 7.11840 seconds\n",
      "> Epoch: 234, Current best: 5.7112374240586, Global best: 0.5728469146888064, Runtime: 7.25907 seconds\n",
      "> Epoch: 235, Current best: 8.750859219512126, Global best: 0.5728469146888064, Runtime: 7.18235 seconds\n",
      "> Epoch: 236, Current best: 41.76856849409788, Global best: 0.5728469146888064, Runtime: 7.77408 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 237, Current best: 10.777654977114771, Global best: 0.5728469146888064, Runtime: 7.49538 seconds\n",
      "> Epoch: 238, Current best: 62.03870942846837, Global best: 0.5728469146888064, Runtime: 7.59205 seconds\n",
      "> Epoch: 239, Current best: 2.1829387707944004, Global best: 0.5728469146888064, Runtime: 8.96727 seconds\n",
      "> Epoch: 240, Current best: 2.5828266718858486, Global best: 0.5728469146888064, Runtime: 8.39753 seconds\n",
      "> Epoch: 241, Current best: 13.352975941864878, Global best: 0.5728469146888064, Runtime: 8.47165 seconds\n",
      "> Epoch: 242, Current best: 7.253686876244653, Global best: 0.5728469146888064, Runtime: 8.69246 seconds\n",
      "> Epoch: 243, Current best: 10.800485055700994, Global best: 0.5728469146888064, Runtime: 8.18461 seconds\n",
      "> Epoch: 244, Current best: 5.575062714383423, Global best: 0.5728469146888064, Runtime: 9.94287 seconds\n",
      "> Epoch: 245, Current best: 11.123883441088898, Global best: 0.5728469146888064, Runtime: 9.87603 seconds\n",
      "> Epoch: 246, Current best: 6.25759746226243, Global best: 0.5728469146888064, Runtime: 7.82743 seconds\n",
      "> Epoch: 247, Current best: 2.1850680347462834, Global best: 0.5728469146888064, Runtime: 7.05349 seconds\n",
      "> Epoch: 248, Current best: 1.8744507054301696, Global best: 0.5728469146888064, Runtime: 6.92383 seconds\n",
      "> Epoch: 249, Current best: 5.318957320059127, Global best: 0.5728469146888064, Runtime: 7.26041 seconds\n",
      "> Epoch: 250, Current best: 4.026999483345432, Global best: 0.5728469146888064, Runtime: 8.17469 seconds\n",
      "> Epoch: 251, Current best: 1.0266510282528198, Global best: 0.5728469146888064, Runtime: 8.16029 seconds\n",
      "> Epoch: 252, Current best: 27.237515635831087, Global best: 0.5728469146888064, Runtime: 8.14208 seconds\n",
      "> Epoch: 253, Current best: 1.8184016949342983, Global best: 0.5728469146888064, Runtime: 8.79261 seconds\n",
      "> Epoch: 254, Current best: 3.4075067346702674, Global best: 0.5728469146888064, Runtime: 7.97612 seconds\n",
      "> Epoch: 255, Current best: 37.389910713115206, Global best: 0.5728469146888064, Runtime: 7.54980 seconds\n",
      "> Epoch: 256, Current best: 10.857750127873752, Global best: 0.5728469146888064, Runtime: 7.50389 seconds\n",
      "> Epoch: 257, Current best: 5.931815420795353, Global best: 0.5728469146888064, Runtime: 7.38773 seconds\n",
      "> Epoch: 258, Current best: 10.485433486001966, Global best: 0.5728469146888064, Runtime: 7.28751 seconds\n",
      "> Epoch: 259, Current best: 7.326097652510013, Global best: 0.5728469146888064, Runtime: 7.06976 seconds\n",
      "> Epoch: 260, Current best: 3.4871891361879928, Global best: 0.5728469146888064, Runtime: 7.06589 seconds\n",
      "> Epoch: 261, Current best: 43.52151318220194, Global best: 0.5728469146888064, Runtime: 8.20268 seconds\n",
      "> Epoch: 262, Current best: 1.050848997376341, Global best: 0.5728469146888064, Runtime: 7.28560 seconds\n",
      "> Epoch: 263, Current best: 22.749529583164943, Global best: 0.5728469146888064, Runtime: 7.17304 seconds\n",
      "> Epoch: 264, Current best: 10.117068826920704, Global best: 0.5728469146888064, Runtime: 7.16303 seconds\n",
      "> Epoch: 265, Current best: 15.167578794065285, Global best: 0.5728469146888064, Runtime: 7.35975 seconds\n",
      "> Epoch: 266, Current best: 10.07722103481461, Global best: 0.5728469146888064, Runtime: 7.16513 seconds\n",
      "> Epoch: 267, Current best: 2.7651735229033605, Global best: 0.5728469146888064, Runtime: 7.20807 seconds\n",
      "> Epoch: 268, Current best: 34.690355058170624, Global best: 0.5728469146888064, Runtime: 6.99260 seconds\n",
      "> Epoch: 269, Current best: 6.327009590981497, Global best: 0.5728469146888064, Runtime: 7.14644 seconds\n",
      "> Epoch: 270, Current best: 2.533371649717506, Global best: 0.5728469146888064, Runtime: 7.11718 seconds\n",
      "> Epoch: 271, Current best: 13.321600709474358, Global best: 0.5728469146888064, Runtime: 7.17700 seconds\n",
      "> Epoch: 272, Current best: 3.7648329128998945, Global best: 0.5728469146888064, Runtime: 8.56055 seconds\n",
      "> Epoch: 273, Current best: 16.612271362094052, Global best: 0.5728469146888064, Runtime: 7.49276 seconds\n",
      "> Epoch: 274, Current best: 1.7119546596775936, Global best: 0.5728469146888064, Runtime: 7.04759 seconds\n",
      "> Epoch: 275, Current best: 5.152343310356388, Global best: 0.5728469146888064, Runtime: 8.23306 seconds\n",
      "> Epoch: 276, Current best: 5.469708560228996, Global best: 0.5728469146888064, Runtime: 7.67895 seconds\n",
      "> Epoch: 277, Current best: 2.6457846203852187, Global best: 0.5728469146888064, Runtime: 7.89806 seconds\n",
      "> Epoch: 278, Current best: 1.02872347652681, Global best: 0.5728469146888064, Runtime: 7.71358 seconds\n",
      "> Epoch: 279, Current best: 2.0405962677732963, Global best: 0.5728469146888064, Runtime: 7.27893 seconds\n",
      "> Epoch: 280, Current best: 11.848559813550235, Global best: 0.5728469146888064, Runtime: 7.13935 seconds\n",
      "> Epoch: 281, Current best: 2.525956921158195, Global best: 0.5728469146888064, Runtime: 8.11910 seconds\n",
      "> Epoch: 282, Current best: 1.3630393262796874, Global best: 0.5728469146888064, Runtime: 7.51346 seconds\n",
      "> Epoch: 283, Current best: 10.120928038831648, Global best: 0.5728469146888064, Runtime: 7.58836 seconds\n",
      "> Epoch: 284, Current best: 54.255175029972165, Global best: 0.5728469146888064, Runtime: 7.88666 seconds\n",
      "> Epoch: 285, Current best: 6.014549997695954, Global best: 0.5728469146888064, Runtime: 8.12762 seconds\n",
      "> Epoch: 286, Current best: 2.1893855692005304, Global best: 0.5728469146888064, Runtime: 8.33891 seconds\n",
      "> Epoch: 287, Current best: 4.445370366903496, Global best: 0.5728469146888064, Runtime: 8.12879 seconds\n",
      "> Epoch: 288, Current best: 13.167132677992104, Global best: 0.5728469146888064, Runtime: 8.30768 seconds\n",
      "> Epoch: 289, Current best: 6.0212407053709125, Global best: 0.5728469146888064, Runtime: 7.60356 seconds\n",
      "> Epoch: 290, Current best: 0.9489306842317525, Global best: 0.5728469146888064, Runtime: 7.86919 seconds\n",
      "> Epoch: 291, Current best: 3.264856602284824, Global best: 0.5728469146888064, Runtime: 8.17759 seconds\n",
      "> Epoch: 292, Current best: 4.889205069386624, Global best: 0.5728469146888064, Runtime: 9.51560 seconds\n",
      "> Epoch: 293, Current best: 2.7307868151587553, Global best: 0.5728469146888064, Runtime: 7.81211 seconds\n",
      "> Epoch: 294, Current best: 4.290153378176174, Global best: 0.5728469146888064, Runtime: 8.40295 seconds\n",
      "> Epoch: 295, Current best: 3.715473718896207, Global best: 0.5728469146888064, Runtime: 7.75655 seconds\n",
      "> Epoch: 296, Current best: 1.8706361385132317, Global best: 0.5728469146888064, Runtime: 7.47724 seconds\n",
      "> Epoch: 297, Current best: 4.9281625275922725, Global best: 0.5728469146888064, Runtime: 7.39120 seconds\n",
      "> Epoch: 298, Current best: 0.5590606318577598, Global best: 0.5590606318577598, Runtime: 7.43158 seconds\n",
      "> Epoch: 299, Current best: 0.6917231949310733, Global best: 0.5590606318577598, Runtime: 7.60455 seconds\n",
      "> Epoch: 300, Current best: 1.2516904018689208, Global best: 0.5590606318577598, Runtime: 7.32729 seconds\n",
      "> Epoch: 301, Current best: 35.317702932193136, Global best: 0.5590606318577598, Runtime: 7.15448 seconds\n",
      "> Epoch: 302, Current best: 3.7609769729564624, Global best: 0.5590606318577598, Runtime: 7.13210 seconds\n",
      "> Epoch: 303, Current best: 0.6395800036942056, Global best: 0.5590606318577598, Runtime: 7.18976 seconds\n",
      "> Epoch: 304, Current best: 2.3396409316708047, Global best: 0.5590606318577598, Runtime: 7.53324 seconds\n",
      "> Epoch: 305, Current best: 1.6754032714101819, Global best: 0.5590606318577598, Runtime: 8.67987 seconds\n",
      "> Epoch: 306, Current best: 1.9624192126184739, Global best: 0.5590606318577598, Runtime: 7.18325 seconds\n",
      "> Epoch: 307, Current best: 3.035995696372723, Global best: 0.5590606318577598, Runtime: 7.89222 seconds\n",
      "> Epoch: 308, Current best: 8.92254742266298, Global best: 0.5590606318577598, Runtime: 8.15485 seconds\n",
      "> Epoch: 309, Current best: 4.8964591774058, Global best: 0.5590606318577598, Runtime: 8.45938 seconds\n",
      "> Epoch: 310, Current best: 9.171304218835415, Global best: 0.5590606318577598, Runtime: 8.38435 seconds\n",
      "> Epoch: 311, Current best: 1.832931053873265, Global best: 0.5590606318577598, Runtime: 7.62380 seconds\n",
      "> Epoch: 312, Current best: 1.4840129884406157, Global best: 0.5590606318577598, Runtime: 7.90066 seconds\n",
      "> Epoch: 313, Current best: 1.6346186633649775, Global best: 0.5590606318577598, Runtime: 8.06473 seconds\n",
      "> Epoch: 314, Current best: 0.38834901568960584, Global best: 0.38834901568960584, Runtime: 9.34662 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 315, Current best: 2.550908373690447, Global best: 0.38834901568960584, Runtime: 8.78929 seconds\n",
      "> Epoch: 316, Current best: 4.184114443889529, Global best: 0.38834901568960584, Runtime: 7.87814 seconds\n",
      "> Epoch: 317, Current best: 7.8890100053629855, Global best: 0.38834901568960584, Runtime: 7.01652 seconds\n",
      "> Epoch: 318, Current best: 0.6546711541664071, Global best: 0.38834901568960584, Runtime: 7.00855 seconds\n",
      "> Epoch: 319, Current best: 2.478588667234737, Global best: 0.38834901568960584, Runtime: 7.10334 seconds\n",
      "> Epoch: 320, Current best: 10.195411216089616, Global best: 0.38834901568960584, Runtime: 7.14694 seconds\n",
      "> Epoch: 321, Current best: 6.443252009699911, Global best: 0.38834901568960584, Runtime: 7.29707 seconds\n",
      "> Epoch: 322, Current best: 3.303631180181261, Global best: 0.38834901568960584, Runtime: 7.11833 seconds\n",
      "> Epoch: 323, Current best: 1.6581748220730674, Global best: 0.38834901568960584, Runtime: 7.40607 seconds\n",
      "> Epoch: 324, Current best: 2.8939590038600698, Global best: 0.38834901568960584, Runtime: 7.74884 seconds\n",
      "> Epoch: 325, Current best: 1.3551397558208609, Global best: 0.38834901568960584, Runtime: 7.04338 seconds\n",
      "> Epoch: 326, Current best: 3.489264130645498, Global best: 0.38834901568960584, Runtime: 7.01161 seconds\n",
      "> Epoch: 327, Current best: 6.5027468266104504, Global best: 0.38834901568960584, Runtime: 7.15706 seconds\n",
      "> Epoch: 328, Current best: 4.762703761628695, Global best: 0.38834901568960584, Runtime: 7.11923 seconds\n",
      "> Epoch: 329, Current best: 1.1082011639419216, Global best: 0.38834901568960584, Runtime: 7.08251 seconds\n",
      "> Epoch: 330, Current best: 5.995969550399272, Global best: 0.38834901568960584, Runtime: 6.95579 seconds\n",
      "> Epoch: 331, Current best: 0.43040792542227924, Global best: 0.38834901568960584, Runtime: 7.01812 seconds\n",
      "> Epoch: 332, Current best: 0.34516404833489367, Global best: 0.34516404833489367, Runtime: 7.71550 seconds\n",
      "> Epoch: 333, Current best: 1.5531185177191371, Global best: 0.34516404833489367, Runtime: 7.97457 seconds\n",
      "> Epoch: 334, Current best: 1.7311880830135218, Global best: 0.34516404833489367, Runtime: 9.56732 seconds\n",
      "> Epoch: 335, Current best: 1.7425565767023683, Global best: 0.34516404833489367, Runtime: 8.91361 seconds\n",
      "> Epoch: 336, Current best: 1.3457568417589347, Global best: 0.34516404833489367, Runtime: 8.72594 seconds\n",
      "> Epoch: 337, Current best: 3.0610161767648605, Global best: 0.34516404833489367, Runtime: 8.39942 seconds\n",
      "> Epoch: 338, Current best: 8.117154029196518, Global best: 0.34516404833489367, Runtime: 8.67497 seconds\n",
      "> Epoch: 339, Current best: 5.475241853034698, Global best: 0.34516404833489367, Runtime: 7.76830 seconds\n",
      "> Epoch: 340, Current best: 1.2817363428144857, Global best: 0.34516404833489367, Runtime: 7.72240 seconds\n",
      "> Epoch: 341, Current best: 0.9110691977488133, Global best: 0.34516404833489367, Runtime: 7.51586 seconds\n",
      "> Epoch: 342, Current best: 1.454089594145254, Global best: 0.34516404833489367, Runtime: 7.39916 seconds\n",
      "> Epoch: 343, Current best: 0.9150251971044474, Global best: 0.34516404833489367, Runtime: 7.26569 seconds\n",
      "> Epoch: 344, Current best: 0.6558887415312075, Global best: 0.34516404833489367, Runtime: 7.06844 seconds\n",
      "> Epoch: 345, Current best: 17.436998057702745, Global best: 0.34516404833489367, Runtime: 6.98065 seconds\n",
      "> Epoch: 346, Current best: 3.1601126571335674, Global best: 0.34516404833489367, Runtime: 7.03343 seconds\n",
      "> Epoch: 347, Current best: 1.1810813621018386, Global best: 0.34516404833489367, Runtime: 7.17993 seconds\n",
      "> Epoch: 348, Current best: 17.69572776405453, Global best: 0.34516404833489367, Runtime: 7.19112 seconds\n",
      "> Epoch: 349, Current best: 1.1073027714518673, Global best: 0.34516404833489367, Runtime: 6.91589 seconds\n",
      "> Epoch: 350, Current best: 1.8825155805607623, Global best: 0.34516404833489367, Runtime: 6.95883 seconds\n",
      "> Epoch: 351, Current best: 1.6055669239950996, Global best: 0.34516404833489367, Runtime: 6.97916 seconds\n",
      "> Epoch: 352, Current best: 3.4197746991059006, Global best: 0.34516404833489367, Runtime: 7.32355 seconds\n",
      "> Epoch: 353, Current best: 7.631544955071742, Global best: 0.34516404833489367, Runtime: 8.34409 seconds\n",
      "> Epoch: 354, Current best: 1.6746655556537633, Global best: 0.34516404833489367, Runtime: 7.98327 seconds\n",
      "> Epoch: 355, Current best: 2.954898593330215, Global best: 0.34516404833489367, Runtime: 7.58963 seconds\n",
      "> Epoch: 356, Current best: 3.0696699494816997, Global best: 0.34516404833489367, Runtime: 7.63687 seconds\n",
      "> Epoch: 357, Current best: 1.597191745023297, Global best: 0.34516404833489367, Runtime: 8.99890 seconds\n",
      "> Epoch: 358, Current best: 0.6088198077614301, Global best: 0.34516404833489367, Runtime: 7.19505 seconds\n",
      "> Epoch: 359, Current best: 3.495537974412853, Global best: 0.34516404833489367, Runtime: 7.04864 seconds\n",
      "> Epoch: 360, Current best: 1.5269952500621975, Global best: 0.34516404833489367, Runtime: 6.95610 seconds\n",
      "> Epoch: 361, Current best: 0.43342860591644256, Global best: 0.34516404833489367, Runtime: 7.11414 seconds\n",
      "> Epoch: 362, Current best: 1.7033739000895147, Global best: 0.34516404833489367, Runtime: 7.10523 seconds\n",
      "> Epoch: 363, Current best: 0.8752877544758348, Global best: 0.34516404833489367, Runtime: 7.17817 seconds\n",
      "> Epoch: 364, Current best: 4.806339414762987, Global best: 0.34516404833489367, Runtime: 7.75226 seconds\n",
      "> Epoch: 365, Current best: 1.2021005673838803, Global best: 0.34516404833489367, Runtime: 7.89279 seconds\n",
      "> Epoch: 366, Current best: 3.837581741226827, Global best: 0.34516404833489367, Runtime: 7.84303 seconds\n",
      "> Epoch: 367, Current best: 0.7477641705417384, Global best: 0.34516404833489367, Runtime: 8.39001 seconds\n",
      "> Epoch: 368, Current best: 0.5003123257811223, Global best: 0.34516404833489367, Runtime: 7.93535 seconds\n",
      "> Epoch: 369, Current best: 1.8547609351303962, Global best: 0.34516404833489367, Runtime: 7.65338 seconds\n",
      "> Epoch: 370, Current best: 0.7462599133524871, Global best: 0.34516404833489367, Runtime: 8.40340 seconds\n",
      "> Epoch: 371, Current best: 1.4076153017425268, Global best: 0.34516404833489367, Runtime: 8.44539 seconds\n",
      "> Epoch: 372, Current best: 8.689899699127363, Global best: 0.34516404833489367, Runtime: 8.06242 seconds\n",
      "> Epoch: 373, Current best: 3.6359636000224467, Global best: 0.34516404833489367, Runtime: 7.29274 seconds\n",
      "> Epoch: 374, Current best: 2.257511255232146, Global best: 0.34516404833489367, Runtime: 7.38445 seconds\n",
      "> Epoch: 375, Current best: 0.790433628122692, Global best: 0.34516404833489367, Runtime: 7.24556 seconds\n",
      "> Epoch: 376, Current best: 1.9009154040357075, Global best: 0.34516404833489367, Runtime: 7.21685 seconds\n",
      "> Epoch: 377, Current best: 1.7928178711680125, Global best: 0.34516404833489367, Runtime: 7.13217 seconds\n",
      "> Epoch: 378, Current best: 2.003404239013115, Global best: 0.34516404833489367, Runtime: 7.38306 seconds\n",
      "> Epoch: 379, Current best: 1.8037185831343914, Global best: 0.34516404833489367, Runtime: 8.48418 seconds\n",
      "> Epoch: 380, Current best: 1.691687265715249, Global best: 0.34516404833489367, Runtime: 7.99262 seconds\n",
      "> Epoch: 381, Current best: 0.8542825712633558, Global best: 0.34516404833489367, Runtime: 7.69720 seconds\n",
      "> Epoch: 382, Current best: 0.687252234793682, Global best: 0.34516404833489367, Runtime: 8.20358 seconds\n",
      "> Epoch: 383, Current best: 0.4795139697268411, Global best: 0.34516404833489367, Runtime: 8.40792 seconds\n",
      "> Epoch: 384, Current best: 0.8178968170576948, Global best: 0.34516404833489367, Runtime: 7.53791 seconds\n",
      "> Epoch: 385, Current best: 1.3160087173298542, Global best: 0.34516404833489367, Runtime: 7.58631 seconds\n",
      "> Epoch: 386, Current best: 0.7165717434817022, Global best: 0.34516404833489367, Runtime: 8.85678 seconds\n",
      "> Epoch: 387, Current best: 2.3241137703142796, Global best: 0.34516404833489367, Runtime: 8.95366 seconds\n",
      "> Epoch: 388, Current best: 0.5172462431169293, Global best: 0.34516404833489367, Runtime: 8.14889 seconds\n",
      "> Epoch: 389, Current best: 2.2336421684753387, Global best: 0.34516404833489367, Runtime: 8.06678 seconds\n",
      "> Epoch: 390, Current best: 1.0147226583068965, Global best: 0.34516404833489367, Runtime: 8.69309 seconds\n",
      "> Epoch: 391, Current best: 3.388061983638911, Global best: 0.34516404833489367, Runtime: 7.12447 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 392, Current best: 5.181491890848104, Global best: 0.34516404833489367, Runtime: 7.65854 seconds\n",
      "> Epoch: 393, Current best: 1.0095810612596452, Global best: 0.34516404833489367, Runtime: 7.24276 seconds\n",
      "> Epoch: 394, Current best: 1.1518233757986582, Global best: 0.34516404833489367, Runtime: 7.45406 seconds\n",
      "> Epoch: 395, Current best: 0.9246395233320794, Global best: 0.34516404833489367, Runtime: 7.85856 seconds\n",
      "> Epoch: 396, Current best: 1.0564051290018677, Global best: 0.34516404833489367, Runtime: 7.20906 seconds\n",
      "> Epoch: 397, Current best: 1.435468255591997, Global best: 0.34516404833489367, Runtime: 8.17668 seconds\n",
      "> Epoch: 398, Current best: 0.9681828191189563, Global best: 0.34516404833489367, Runtime: 8.77322 seconds\n",
      "> Epoch: 399, Current best: 0.6005173907222628, Global best: 0.34516404833489367, Runtime: 8.81084 seconds\n",
      "> Epoch: 400, Current best: 0.3443385462163274, Global best: 0.3443385462163274, Runtime: 7.75710 seconds\n",
      "> Epoch: 401, Current best: 0.8167529235348673, Global best: 0.3443385462163274, Runtime: 7.68229 seconds\n",
      "> Epoch: 402, Current best: 1.588153280275779, Global best: 0.3443385462163274, Runtime: 7.18794 seconds\n",
      "> Epoch: 403, Current best: 0.9039099306279347, Global best: 0.3443385462163274, Runtime: 8.19474 seconds\n",
      "> Epoch: 404, Current best: 0.34298655193770367, Global best: 0.34298655193770367, Runtime: 7.98731 seconds\n",
      "> Epoch: 405, Current best: 1.014145298245247, Global best: 0.34298655193770367, Runtime: 7.77018 seconds\n",
      "> Epoch: 406, Current best: 0.8352557819579332, Global best: 0.34298655193770367, Runtime: 7.12472 seconds\n",
      "> Epoch: 407, Current best: 0.7924297996937405, Global best: 0.34298655193770367, Runtime: 7.17104 seconds\n",
      "> Epoch: 408, Current best: 0.768514190277128, Global best: 0.34298655193770367, Runtime: 7.04586 seconds\n",
      "> Epoch: 409, Current best: 2.633784087367367, Global best: 0.34298655193770367, Runtime: 7.26771 seconds\n",
      "> Epoch: 410, Current best: 0.6050287635371461, Global best: 0.34298655193770367, Runtime: 7.10718 seconds\n",
      "> Epoch: 411, Current best: 0.44153997942178846, Global best: 0.34298655193770367, Runtime: 7.01655 seconds\n",
      "> Epoch: 412, Current best: 0.9688463953663651, Global best: 0.34298655193770367, Runtime: 7.55462 seconds\n",
      "> Epoch: 413, Current best: 0.7025382183206657, Global best: 0.34298655193770367, Runtime: 7.44065 seconds\n",
      "> Epoch: 414, Current best: 1.4200184986094089, Global best: 0.34298655193770367, Runtime: 7.94297 seconds\n",
      "> Epoch: 415, Current best: 2.640039714345, Global best: 0.34298655193770367, Runtime: 8.56667 seconds\n",
      "> Epoch: 416, Current best: 2.7777263983924616, Global best: 0.34298655193770367, Runtime: 8.03134 seconds\n",
      "> Epoch: 417, Current best: 1.9810419689601655, Global best: 0.34298655193770367, Runtime: 8.48935 seconds\n",
      "> Epoch: 418, Current best: 1.1022796391475465, Global best: 0.34298655193770367, Runtime: 8.10901 seconds\n",
      "> Epoch: 419, Current best: 1.7586866522639744, Global best: 0.34298655193770367, Runtime: 8.77952 seconds\n",
      "> Epoch: 420, Current best: 0.9429909734704767, Global best: 0.34298655193770367, Runtime: 8.16093 seconds\n",
      "> Epoch: 421, Current best: 1.699999690183813, Global best: 0.34298655193770367, Runtime: 7.36107 seconds\n",
      "> Epoch: 422, Current best: 0.4234116442150361, Global best: 0.34298655193770367, Runtime: 7.19808 seconds\n",
      "> Epoch: 423, Current best: 0.5125991524423716, Global best: 0.34298655193770367, Runtime: 7.27507 seconds\n",
      "> Epoch: 424, Current best: 1.3343059659637642, Global best: 0.34298655193770367, Runtime: 7.36236 seconds\n",
      "> Epoch: 425, Current best: 0.483885750212753, Global best: 0.34298655193770367, Runtime: 7.45823 seconds\n",
      "> Epoch: 426, Current best: 0.5033325602821015, Global best: 0.34298655193770367, Runtime: 7.27470 seconds\n",
      "> Epoch: 427, Current best: 0.7823387503136061, Global best: 0.34298655193770367, Runtime: 7.10223 seconds\n",
      "> Epoch: 428, Current best: 0.6491898216774502, Global best: 0.34298655193770367, Runtime: 7.26040 seconds\n",
      "> Epoch: 429, Current best: 0.7097356229059081, Global best: 0.34298655193770367, Runtime: 7.11615 seconds\n",
      "> Epoch: 430, Current best: 0.4936217411800597, Global best: 0.34298655193770367, Runtime: 8.22760 seconds\n",
      "> Epoch: 431, Current best: 0.7568842095350113, Global best: 0.34298655193770367, Runtime: 7.38839 seconds\n",
      "> Epoch: 432, Current best: 0.5485515768216942, Global best: 0.34298655193770367, Runtime: 7.38927 seconds\n",
      "> Epoch: 433, Current best: 1.2750867257089111, Global best: 0.34298655193770367, Runtime: 7.83052 seconds\n",
      "> Epoch: 434, Current best: 0.4595018279743577, Global best: 0.34298655193770367, Runtime: 7.30961 seconds\n",
      "> Epoch: 435, Current best: 1.2639498060514245, Global best: 0.34298655193770367, Runtime: 7.12028 seconds\n",
      "> Epoch: 436, Current best: 0.5272213163391584, Global best: 0.34298655193770367, Runtime: 7.02590 seconds\n",
      "> Epoch: 437, Current best: 0.48207340440767793, Global best: 0.34298655193770367, Runtime: 7.03646 seconds\n",
      "> Epoch: 438, Current best: 0.5812124499295812, Global best: 0.34298655193770367, Runtime: 7.11420 seconds\n",
      "> Epoch: 439, Current best: 0.7691144056533109, Global best: 0.34298655193770367, Runtime: 7.21687 seconds\n",
      "> Epoch: 440, Current best: 0.684537610806819, Global best: 0.34298655193770367, Runtime: 7.77708 seconds\n",
      "> Epoch: 441, Current best: 0.3766479000558957, Global best: 0.34298655193770367, Runtime: 7.52382 seconds\n",
      "> Epoch: 442, Current best: 0.8265584835387288, Global best: 0.34298655193770367, Runtime: 7.27773 seconds\n",
      "> Epoch: 443, Current best: 0.4121224188295049, Global best: 0.34298655193770367, Runtime: 7.45009 seconds\n",
      "> Epoch: 444, Current best: 0.9785464379005631, Global best: 0.34298655193770367, Runtime: 9.11142 seconds\n",
      "> Epoch: 445, Current best: 0.8259045976365411, Global best: 0.34298655193770367, Runtime: 7.84926 seconds\n",
      "> Epoch: 446, Current best: 0.8802781468594392, Global best: 0.34298655193770367, Runtime: 8.73496 seconds\n",
      "> Epoch: 447, Current best: 0.4899983408465074, Global best: 0.34298655193770367, Runtime: 8.29031 seconds\n",
      "> Epoch: 448, Current best: 0.5879850626759308, Global best: 0.34298655193770367, Runtime: 7.88478 seconds\n",
      "> Epoch: 449, Current best: 0.5033637227850088, Global best: 0.34298655193770367, Runtime: 8.02418 seconds\n",
      "> Epoch: 450, Current best: 0.6668169942700853, Global best: 0.34298655193770367, Runtime: 7.31751 seconds\n",
      "> Epoch: 451, Current best: 1.0723607020308061, Global best: 0.34298655193770367, Runtime: 7.24483 seconds\n",
      "> Epoch: 452, Current best: 0.8723120699592294, Global best: 0.34298655193770367, Runtime: 7.16901 seconds\n",
      "> Epoch: 453, Current best: 0.752283190091904, Global best: 0.34298655193770367, Runtime: 7.04194 seconds\n",
      "> Epoch: 454, Current best: 0.9621157592261115, Global best: 0.34298655193770367, Runtime: 7.09526 seconds\n",
      "> Epoch: 455, Current best: 0.5744158507477949, Global best: 0.34298655193770367, Runtime: 7.19485 seconds\n",
      "> Epoch: 456, Current best: 0.46576700301101304, Global best: 0.34298655193770367, Runtime: 7.38429 seconds\n",
      "> Epoch: 457, Current best: 0.41238101539526134, Global best: 0.34298655193770367, Runtime: 7.67332 seconds\n",
      "> Epoch: 458, Current best: 0.7404196832702915, Global best: 0.34298655193770367, Runtime: 7.68927 seconds\n",
      "> Epoch: 459, Current best: 0.7043008112959213, Global best: 0.34298655193770367, Runtime: 8.49015 seconds\n",
      "> Epoch: 460, Current best: 0.6070792975167641, Global best: 0.34298655193770367, Runtime: 7.72524 seconds\n",
      "> Epoch: 461, Current best: 1.0434347554974372, Global best: 0.34298655193770367, Runtime: 7.71027 seconds\n",
      "> Epoch: 462, Current best: 0.7366607769240167, Global best: 0.34298655193770367, Runtime: 8.33103 seconds\n",
      "> Epoch: 463, Current best: 0.4469177922641769, Global best: 0.34298655193770367, Runtime: 8.92390 seconds\n",
      "> Epoch: 464, Current best: 1.273593835529832, Global best: 0.34298655193770367, Runtime: 8.68605 seconds\n",
      "> Epoch: 465, Current best: 0.3520337664288408, Global best: 0.34298655193770367, Runtime: 7.48291 seconds\n",
      "> Epoch: 466, Current best: 0.6032037540462862, Global best: 0.34298655193770367, Runtime: 7.19407 seconds\n",
      "> Epoch: 467, Current best: 0.34964159446282095, Global best: 0.34298655193770367, Runtime: 7.30456 seconds\n",
      "> Epoch: 468, Current best: 0.4609623214090181, Global best: 0.34298655193770367, Runtime: 7.35243 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 469, Current best: 0.7478727378381238, Global best: 0.34298655193770367, Runtime: 7.26803 seconds\n",
      "> Epoch: 470, Current best: 0.616556748580197, Global best: 0.34298655193770367, Runtime: 7.31552 seconds\n",
      "> Epoch: 471, Current best: 0.5427662508092151, Global best: 0.34298655193770367, Runtime: 8.98256 seconds\n",
      "> Epoch: 472, Current best: 0.9904691514215475, Global best: 0.34298655193770367, Runtime: 10.11447 seconds\n",
      "> Epoch: 473, Current best: 0.3687512345747671, Global best: 0.34298655193770367, Runtime: 9.35405 seconds\n",
      "> Epoch: 474, Current best: 0.3729431574735239, Global best: 0.34298655193770367, Runtime: 8.20060 seconds\n",
      "> Epoch: 475, Current best: 0.6562110076085061, Global best: 0.34298655193770367, Runtime: 8.45341 seconds\n",
      "> Epoch: 476, Current best: 0.7455190676996111, Global best: 0.34298655193770367, Runtime: 8.90352 seconds\n",
      "> Epoch: 477, Current best: 1.5716164612573904, Global best: 0.34298655193770367, Runtime: 8.65475 seconds\n",
      "> Epoch: 478, Current best: 0.6066152050371373, Global best: 0.34298655193770367, Runtime: 7.98323 seconds\n",
      "> Epoch: 479, Current best: 0.4928066866257356, Global best: 0.34298655193770367, Runtime: 8.71705 seconds\n",
      "> Epoch: 480, Current best: 0.5676633102111935, Global best: 0.34298655193770367, Runtime: 8.41120 seconds\n",
      "> Epoch: 481, Current best: 0.961795912730709, Global best: 0.34298655193770367, Runtime: 8.33644 seconds\n",
      "> Epoch: 482, Current best: 0.6977289296370812, Global best: 0.34298655193770367, Runtime: 8.84643 seconds\n",
      "> Epoch: 483, Current best: 0.4148667392186186, Global best: 0.34298655193770367, Runtime: 8.73661 seconds\n",
      "> Epoch: 484, Current best: 0.5135369287178333, Global best: 0.34298655193770367, Runtime: 8.69827 seconds\n",
      "> Epoch: 485, Current best: 0.48772543665022644, Global best: 0.34298655193770367, Runtime: 9.02994 seconds\n",
      "> Epoch: 486, Current best: 0.4721488885926973, Global best: 0.34298655193770367, Runtime: 8.52968 seconds\n",
      "> Epoch: 487, Current best: 0.41106688347496345, Global best: 0.34298655193770367, Runtime: 8.58745 seconds\n",
      "> Epoch: 488, Current best: 0.6000010440938176, Global best: 0.34298655193770367, Runtime: 8.46240 seconds\n",
      "> Epoch: 489, Current best: 0.33756189123212593, Global best: 0.33756189123212593, Runtime: 8.24134 seconds\n",
      "> Epoch: 490, Current best: 0.40218792181398555, Global best: 0.33756189123212593, Runtime: 7.19993 seconds\n",
      "> Epoch: 491, Current best: 0.9576770355455164, Global best: 0.33756189123212593, Runtime: 7.23425 seconds\n",
      "> Epoch: 492, Current best: 0.9419404168314844, Global best: 0.33756189123212593, Runtime: 7.65226 seconds\n",
      "> Epoch: 493, Current best: 0.42575139304983367, Global best: 0.33756189123212593, Runtime: 8.06707 seconds\n",
      "> Epoch: 494, Current best: 0.44726263812051353, Global best: 0.33756189123212593, Runtime: 7.41229 seconds\n",
      "> Epoch: 495, Current best: 0.3885810720992183, Global best: 0.33756189123212593, Runtime: 7.17113 seconds\n",
      "> Epoch: 496, Current best: 0.6079591840517529, Global best: 0.33756189123212593, Runtime: 7.14582 seconds\n",
      "> Epoch: 497, Current best: 0.4228440752512291, Global best: 0.33756189123212593, Runtime: 7.19697 seconds\n",
      "> Epoch: 498, Current best: 0.4047525434372473, Global best: 0.33756189123212593, Runtime: 7.12666 seconds\n",
      "> Epoch: 499, Current best: 0.41037686760954234, Global best: 0.33756189123212593, Runtime: 7.11220 seconds\n",
      "> Epoch: 500, Current best: 0.37190104405191327, Global best: 0.33756189123212593, Runtime: 7.07632 seconds\n",
      "> Epoch: 501, Current best: 0.3574643435018298, Global best: 0.33756189123212593, Runtime: 7.47100 seconds\n",
      "> Epoch: 502, Current best: 1.8922586533689767, Global best: 0.33756189123212593, Runtime: 7.43013 seconds\n",
      "> Epoch: 503, Current best: 0.49639287498012274, Global best: 0.33756189123212593, Runtime: 7.16422 seconds\n",
      "> Epoch: 504, Current best: 0.511233208778712, Global best: 0.33756189123212593, Runtime: 7.16802 seconds\n",
      "> Epoch: 505, Current best: 0.36340670589068524, Global best: 0.33756189123212593, Runtime: 7.04795 seconds\n",
      "> Epoch: 506, Current best: 0.3833998768382346, Global best: 0.33756189123212593, Runtime: 7.03655 seconds\n",
      "> Epoch: 507, Current best: 0.7088385116418675, Global best: 0.33756189123212593, Runtime: 8.95010 seconds\n",
      "> Epoch: 508, Current best: 0.5122009856172821, Global best: 0.33756189123212593, Runtime: 8.77204 seconds\n",
      "> Epoch: 509, Current best: 0.5100435543467765, Global best: 0.33756189123212593, Runtime: 8.33619 seconds\n",
      "> Epoch: 510, Current best: 0.35792459653332065, Global best: 0.33756189123212593, Runtime: 8.98556 seconds\n",
      "> Epoch: 511, Current best: 0.39369402779285634, Global best: 0.33756189123212593, Runtime: 9.09024 seconds\n",
      "> Epoch: 512, Current best: 0.4055946857593966, Global best: 0.33756189123212593, Runtime: 7.54733 seconds\n",
      "> Epoch: 513, Current best: 0.4856308895966992, Global best: 0.33756189123212593, Runtime: 7.37193 seconds\n",
      "> Epoch: 514, Current best: 0.35804454690359555, Global best: 0.33756189123212593, Runtime: 7.59423 seconds\n",
      "> Epoch: 515, Current best: 0.9892279675320598, Global best: 0.33756189123212593, Runtime: 7.28661 seconds\n",
      "> Epoch: 516, Current best: 0.40725266504167396, Global best: 0.33756189123212593, Runtime: 7.25472 seconds\n",
      "> Epoch: 517, Current best: 0.477149438604697, Global best: 0.33756189123212593, Runtime: 8.53257 seconds\n",
      "> Epoch: 518, Current best: 0.35176359357893966, Global best: 0.33756189123212593, Runtime: 7.29283 seconds\n",
      "> Epoch: 519, Current best: 0.33023354114863224, Global best: 0.33023354114863224, Runtime: 7.18097 seconds\n",
      "> Epoch: 520, Current best: 0.5271193406148063, Global best: 0.33023354114863224, Runtime: 7.23095 seconds\n",
      "> Epoch: 521, Current best: 0.5044688520990058, Global best: 0.33023354114863224, Runtime: 7.27167 seconds\n",
      "> Epoch: 522, Current best: 0.40466878911955734, Global best: 0.33023354114863224, Runtime: 7.11325 seconds\n",
      "> Epoch: 523, Current best: 0.4384533569891927, Global best: 0.33023354114863224, Runtime: 7.11519 seconds\n",
      "> Epoch: 524, Current best: 0.5841797543033495, Global best: 0.33023354114863224, Runtime: 7.08453 seconds\n",
      "> Epoch: 525, Current best: 0.595184226646498, Global best: 0.33023354114863224, Runtime: 7.49930 seconds\n",
      "> Epoch: 526, Current best: 0.3516661815803082, Global best: 0.33023354114863224, Runtime: 7.13909 seconds\n",
      "> Epoch: 527, Current best: 0.3849183733532179, Global best: 0.33023354114863224, Runtime: 7.13413 seconds\n",
      "> Epoch: 528, Current best: 0.43198924031079905, Global best: 0.33023354114863224, Runtime: 7.08427 seconds\n",
      "> Epoch: 529, Current best: 0.4299117741451345, Global best: 0.33023354114863224, Runtime: 7.08539 seconds\n",
      "> Epoch: 530, Current best: 0.5910914924187104, Global best: 0.33023354114863224, Runtime: 8.55242 seconds\n",
      "> Epoch: 531, Current best: 0.47198096953024965, Global best: 0.33023354114863224, Runtime: 9.10253 seconds\n",
      "> Epoch: 532, Current best: 0.3462402626898502, Global best: 0.33023354114863224, Runtime: 9.54306 seconds\n",
      "> Epoch: 533, Current best: 0.387284078671992, Global best: 0.33023354114863224, Runtime: 8.33637 seconds\n",
      "> Epoch: 534, Current best: 0.3823224533059677, Global best: 0.33023354114863224, Runtime: 8.99314 seconds\n",
      "> Epoch: 535, Current best: 0.34303677652202696, Global best: 0.33023354114863224, Runtime: 8.42157 seconds\n",
      "> Epoch: 536, Current best: 0.36919710355623137, Global best: 0.33023354114863224, Runtime: 7.35901 seconds\n",
      "> Epoch: 537, Current best: 0.547175140609127, Global best: 0.33023354114863224, Runtime: 7.24019 seconds\n",
      "> Epoch: 538, Current best: 0.5196253415248573, Global best: 0.33023354114863224, Runtime: 7.13862 seconds\n",
      "> Epoch: 539, Current best: 0.568783607097987, Global best: 0.33023354114863224, Runtime: 7.06933 seconds\n",
      "> Epoch: 540, Current best: 0.36015881873857386, Global best: 0.33023354114863224, Runtime: 7.31711 seconds\n",
      "> Epoch: 541, Current best: 0.532177726670189, Global best: 0.33023354114863224, Runtime: 7.18930 seconds\n",
      "> Epoch: 542, Current best: 0.7061031758247204, Global best: 0.33023354114863224, Runtime: 7.16583 seconds\n",
      "> Epoch: 543, Current best: 0.3713900925996511, Global best: 0.33023354114863224, Runtime: 7.47313 seconds\n",
      "> Epoch: 544, Current best: 0.3208555924454699, Global best: 0.3208555924454699, Runtime: 7.19790 seconds\n",
      "> Epoch: 545, Current best: 0.48911038703410625, Global best: 0.3208555924454699, Runtime: 7.39060 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 546, Current best: 0.3342190021959088, Global best: 0.3208555924454699, Runtime: 7.25176 seconds\n",
      "> Epoch: 547, Current best: 0.4032349371094453, Global best: 0.3208555924454699, Runtime: 7.10622 seconds\n",
      "> Epoch: 548, Current best: 0.5170101615606105, Global best: 0.3208555924454699, Runtime: 7.10423 seconds\n",
      "> Epoch: 549, Current best: 0.41573865518414743, Global best: 0.3208555924454699, Runtime: 7.32150 seconds\n",
      "> Epoch: 550, Current best: 0.44199355309581045, Global best: 0.3208555924454699, Runtime: 7.39652 seconds\n",
      "> Epoch: 551, Current best: 0.31943342503278643, Global best: 0.31943342503278643, Runtime: 7.18249 seconds\n",
      "> Epoch: 552, Current best: 0.34732922163553415, Global best: 0.31943342503278643, Runtime: 7.13027 seconds\n",
      "> Epoch: 553, Current best: 0.4933081428753925, Global best: 0.31943342503278643, Runtime: 7.00256 seconds\n",
      "> Epoch: 554, Current best: 0.593505562759407, Global best: 0.31943342503278643, Runtime: 7.13349 seconds\n",
      "> Epoch: 555, Current best: 0.37961606457020247, Global best: 0.31943342503278643, Runtime: 7.21984 seconds\n",
      "> Epoch: 556, Current best: 0.35066841760243317, Global best: 0.31943342503278643, Runtime: 7.22582 seconds\n",
      "> Epoch: 557, Current best: 0.4388286910239036, Global best: 0.31943342503278643, Runtime: 7.47924 seconds\n",
      "> Epoch: 558, Current best: 0.3683644860101023, Global best: 0.31943342503278643, Runtime: 7.18624 seconds\n",
      "> Epoch: 559, Current best: 0.47610260048881653, Global best: 0.31943342503278643, Runtime: 7.10825 seconds\n",
      "> Epoch: 560, Current best: 0.40584507207616355, Global best: 0.31943342503278643, Runtime: 7.07478 seconds\n",
      "> Epoch: 561, Current best: 0.425794292494947, Global best: 0.31943342503278643, Runtime: 7.36381 seconds\n",
      "> Epoch: 562, Current best: 0.35823205479455933, Global best: 0.31943342503278643, Runtime: 7.08444 seconds\n",
      "> Epoch: 563, Current best: 0.3193571730518598, Global best: 0.3193571730518598, Runtime: 7.17134 seconds\n",
      "> Epoch: 564, Current best: 0.3163789587555005, Global best: 0.3163789587555005, Runtime: 7.33246 seconds\n",
      "> Epoch: 565, Current best: 0.4429955568453367, Global best: 0.3163789587555005, Runtime: 7.37034 seconds\n",
      "> Epoch: 566, Current best: 0.3459163505660162, Global best: 0.3163789587555005, Runtime: 7.19460 seconds\n",
      "> Epoch: 567, Current best: 0.37999745062961277, Global best: 0.3163789587555005, Runtime: 7.00655 seconds\n",
      "> Epoch: 568, Current best: 0.4505834428456727, Global best: 0.3163789587555005, Runtime: 7.10037 seconds\n",
      "> Epoch: 569, Current best: 0.4273324453120152, Global best: 0.3163789587555005, Runtime: 9.00500 seconds\n",
      "> Epoch: 570, Current best: 0.39889312702034463, Global best: 0.3163789587555005, Runtime: 7.71937 seconds\n",
      "> Epoch: 571, Current best: 0.3265774672464481, Global best: 0.3163789587555005, Runtime: 7.13811 seconds\n",
      "> Epoch: 572, Current best: 0.33423938397531583, Global best: 0.3163789587555005, Runtime: 7.16901 seconds\n",
      "> Epoch: 573, Current best: 0.368607806213196, Global best: 0.3163789587555005, Runtime: 7.37831 seconds\n",
      "> Epoch: 574, Current best: 0.36004354557880736, Global best: 0.3163789587555005, Runtime: 7.16598 seconds\n",
      "> Epoch: 575, Current best: 0.37938280800959145, Global best: 0.3163789587555005, Runtime: 7.14527 seconds\n",
      "> Epoch: 576, Current best: 0.39963743668275603, Global best: 0.3163789587555005, Runtime: 7.11618 seconds\n",
      "> Epoch: 577, Current best: 0.3253236706629276, Global best: 0.3163789587555005, Runtime: 7.14919 seconds\n",
      "> Epoch: 578, Current best: 0.3264700293125364, Global best: 0.3163789587555005, Runtime: 7.13865 seconds\n",
      "> Epoch: 579, Current best: 0.40100145897580036, Global best: 0.3163789587555005, Runtime: 7.05340 seconds\n",
      "> Epoch: 580, Current best: 0.32520230660371297, Global best: 0.3163789587555005, Runtime: 7.09330 seconds\n",
      "> Epoch: 581, Current best: 0.5035467400405786, Global best: 0.3163789587555005, Runtime: 7.09634 seconds\n",
      "> Epoch: 582, Current best: 0.44771795054173724, Global best: 0.3163789587555005, Runtime: 7.17797 seconds\n",
      "> Epoch: 583, Current best: 0.4734966227953111, Global best: 0.3163789587555005, Runtime: 7.07831 seconds\n",
      "> Epoch: 584, Current best: 0.31199793948650667, Global best: 0.31199793948650667, Runtime: 7.28363 seconds\n",
      "> Epoch: 585, Current best: 0.3103712233821041, Global best: 0.3103712233821041, Runtime: 7.50090 seconds\n",
      "> Epoch: 586, Current best: 0.31644371711429836, Global best: 0.3103712233821041, Runtime: 7.14717 seconds\n",
      "> Epoch: 587, Current best: 0.33460712635984874, Global best: 0.3103712233821041, Runtime: 7.15617 seconds\n",
      "> Epoch: 588, Current best: 0.3157760088580432, Global best: 0.3103712233821041, Runtime: 7.09720 seconds\n",
      "> Epoch: 589, Current best: 0.32517954864400334, Global best: 0.3103712233821041, Runtime: 7.10256 seconds\n",
      "> Epoch: 590, Current best: 0.5789159187961856, Global best: 0.3103712233821041, Runtime: 7.43313 seconds\n",
      "> Epoch: 591, Current best: 0.32852934157555436, Global best: 0.3103712233821041, Runtime: 7.27266 seconds\n",
      "> Epoch: 592, Current best: 0.32698404639852696, Global best: 0.3103712233821041, Runtime: 7.04144 seconds\n",
      "> Epoch: 593, Current best: 0.42488281030278824, Global best: 0.3103712233821041, Runtime: 7.05340 seconds\n",
      "> Epoch: 594, Current best: 0.3608702061615405, Global best: 0.3103712233821041, Runtime: 7.12843 seconds\n",
      "> Epoch: 595, Current best: 0.3843769366148808, Global best: 0.3103712233821041, Runtime: 7.93305 seconds\n",
      "> Epoch: 596, Current best: 0.3316450576838559, Global best: 0.3103712233821041, Runtime: 7.09725 seconds\n",
      "> Epoch: 597, Current best: 0.6999967109163927, Global best: 0.3103712233821041, Runtime: 7.10721 seconds\n",
      "> Epoch: 598, Current best: 0.43689914226550114, Global best: 0.3103712233821041, Runtime: 7.32449 seconds\n",
      "> Epoch: 599, Current best: 0.32005892997108976, Global best: 0.3103712233821041, Runtime: 7.10826 seconds\n",
      "> Epoch: 600, Current best: 0.3352479876784023, Global best: 0.3103712233821041, Runtime: 7.09621 seconds\n",
      "> Epoch: 601, Current best: 0.4222494480152806, Global best: 0.3103712233821041, Runtime: 7.16403 seconds\n",
      "> Epoch: 602, Current best: 0.315926004151465, Global best: 0.3103712233821041, Runtime: 7.31421 seconds\n",
      "> Epoch: 603, Current best: 0.30306032095782165, Global best: 0.30306032095782165, Runtime: 7.22134 seconds\n",
      "> Epoch: 604, Current best: 0.33127611085377007, Global best: 0.30306032095782165, Runtime: 7.12922 seconds\n",
      "> Epoch: 605, Current best: 0.30382626078606884, Global best: 0.30306032095782165, Runtime: 7.08233 seconds\n",
      "> Epoch: 606, Current best: 0.3142173531186505, Global best: 0.30306032095782165, Runtime: 7.14305 seconds\n",
      "> Epoch: 607, Current best: 0.3513299066210821, Global best: 0.30306032095782165, Runtime: 7.29684 seconds\n",
      "> Epoch: 608, Current best: 0.3027916549962856, Global best: 0.3027916549962856, Runtime: 7.21780 seconds\n",
      "> Epoch: 609, Current best: 0.2961732270511165, Global best: 0.2961732270511165, Runtime: 7.13412 seconds\n",
      "> Epoch: 610, Current best: 0.3609851443165293, Global best: 0.2961732270511165, Runtime: 7.22482 seconds\n",
      "> Epoch: 611, Current best: 0.45695656707149557, Global best: 0.2961732270511165, Runtime: 7.11369 seconds\n",
      "> Epoch: 612, Current best: 0.38255812668373973, Global best: 0.2961732270511165, Runtime: 7.12835 seconds\n",
      "> Epoch: 613, Current best: 0.3330202582132341, Global best: 0.2961732270511165, Runtime: 7.14492 seconds\n",
      "> Epoch: 614, Current best: 0.31337533802151424, Global best: 0.2961732270511165, Runtime: 7.59259 seconds\n",
      "> Epoch: 615, Current best: 0.31402525512037843, Global best: 0.2961732270511165, Runtime: 7.67631 seconds\n",
      "> Epoch: 616, Current best: 0.29797152328863574, Global best: 0.2961732270511165, Runtime: 7.21486 seconds\n",
      "> Epoch: 617, Current best: 0.3866847941219203, Global best: 0.2961732270511165, Runtime: 7.23180 seconds\n",
      "> Epoch: 618, Current best: 0.31839959007321234, Global best: 0.2961732270511165, Runtime: 7.26569 seconds\n",
      "> Epoch: 619, Current best: 0.37216017414920183, Global best: 0.2961732270511165, Runtime: 7.25273 seconds\n",
      "> Epoch: 620, Current best: 0.27839704386539393, Global best: 0.27839704386539393, Runtime: 7.23479 seconds\n",
      "> Epoch: 621, Current best: 0.27553309721870556, Global best: 0.27553309721870556, Runtime: 7.24276 seconds\n",
      "> Epoch: 622, Current best: 0.3346737658458933, Global best: 0.27553309721870556, Runtime: 7.23180 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 623, Current best: 0.3499861205555983, Global best: 0.27553309721870556, Runtime: 7.49392 seconds\n",
      "> Epoch: 624, Current best: 0.2744236538078562, Global best: 0.2744236538078562, Runtime: 7.25074 seconds\n",
      "> Epoch: 625, Current best: 0.2752775977615006, Global best: 0.2744236538078562, Runtime: 7.28562 seconds\n",
      "> Epoch: 626, Current best: 0.3214336261072605, Global best: 0.2744236538078562, Runtime: 7.24874 seconds\n",
      "> Epoch: 627, Current best: 0.26745790854315366, Global best: 0.26745790854315366, Runtime: 7.23824 seconds\n",
      "> Epoch: 628, Current best: 0.28891689748498, Global best: 0.26745790854315366, Runtime: 7.14929 seconds\n",
      "> Epoch: 629, Current best: 0.28848198852363055, Global best: 0.26745790854315366, Runtime: 7.17996 seconds\n",
      "> Epoch: 630, Current best: 0.25261878767495793, Global best: 0.25261878767495793, Runtime: 7.30072 seconds\n",
      "> Epoch: 631, Current best: 0.3126767913403579, Global best: 0.25261878767495793, Runtime: 7.42725 seconds\n",
      "> Epoch: 632, Current best: 0.25979560131965634, Global best: 0.25261878767495793, Runtime: 7.40524 seconds\n",
      "> Epoch: 633, Current best: 0.2712392913710042, Global best: 0.25261878767495793, Runtime: 7.21187 seconds\n",
      "> Epoch: 634, Current best: 0.2732580889537219, Global best: 0.25261878767495793, Runtime: 7.14808 seconds\n",
      "> Epoch: 635, Current best: 0.27669984574245676, Global best: 0.25261878767495793, Runtime: 7.95339 seconds\n",
      "> Epoch: 636, Current best: 0.25470597129360106, Global best: 0.25261878767495793, Runtime: 7.97232 seconds\n",
      "> Epoch: 637, Current best: 0.24346448276804616, Global best: 0.24346448276804616, Runtime: 7.56895 seconds\n",
      "> Epoch: 638, Current best: 0.24713756274583376, Global best: 0.24346448276804616, Runtime: 8.28132 seconds\n",
      "> Epoch: 639, Current best: 0.2553765305251236, Global best: 0.24346448276804616, Runtime: 7.71122 seconds\n",
      "> Epoch: 640, Current best: 0.26490660011331374, Global best: 0.24346448276804616, Runtime: 7.47753 seconds\n",
      "> Epoch: 641, Current best: 0.2484341738916116, Global best: 0.24346448276804616, Runtime: 7.32558 seconds\n",
      "> Epoch: 642, Current best: 0.2630212638634927, Global best: 0.24346448276804616, Runtime: 7.46601 seconds\n",
      "> Epoch: 643, Current best: 0.26193158018739426, Global best: 0.24346448276804616, Runtime: 7.35755 seconds\n",
      "> Epoch: 644, Current best: 0.2981688319799222, Global best: 0.24346448276804616, Runtime: 7.51485 seconds\n",
      "> Epoch: 645, Current best: 0.2564656823940326, Global best: 0.24346448276804616, Runtime: 7.59558 seconds\n",
      "> Epoch: 646, Current best: 0.2796380801698029, Global best: 0.24346448276804616, Runtime: 7.60071 seconds\n",
      "> Epoch: 647, Current best: 0.31956840660336966, Global best: 0.24346448276804616, Runtime: 7.80310 seconds\n",
      "> Epoch: 648, Current best: 0.2839484990646451, Global best: 0.24346448276804616, Runtime: 7.66435 seconds\n",
      "> Epoch: 649, Current best: 0.26637190013133066, Global best: 0.24346448276804616, Runtime: 7.41530 seconds\n",
      "> Epoch: 650, Current best: 0.2383041537287885, Global best: 0.2383041537287885, Runtime: 7.33247 seconds\n",
      "> Epoch: 651, Current best: 0.24279326059126202, Global best: 0.2383041537287885, Runtime: 7.37350 seconds\n",
      "> Epoch: 652, Current best: 0.23959109781093108, Global best: 0.2383041537287885, Runtime: 7.44923 seconds\n",
      "> Epoch: 653, Current best: 0.23575061276420653, Global best: 0.23575061276420653, Runtime: 7.43118 seconds\n",
      "> Epoch: 654, Current best: 0.2292058946862569, Global best: 0.2292058946862569, Runtime: 7.22206 seconds\n",
      "> Epoch: 655, Current best: 0.2553846029759992, Global best: 0.2292058946862569, Runtime: 7.62254 seconds\n",
      "> Epoch: 656, Current best: 0.26461650753065463, Global best: 0.2292058946862569, Runtime: 7.87063 seconds\n",
      "> Epoch: 657, Current best: 0.23359617054061405, Global best: 0.2292058946862569, Runtime: 7.47598 seconds\n",
      "> Epoch: 658, Current best: 0.2538601395719606, Global best: 0.2292058946862569, Runtime: 7.54674 seconds\n",
      "> Epoch: 659, Current best: 0.40432843783132677, Global best: 0.2292058946862569, Runtime: 7.61751 seconds\n",
      "> Epoch: 660, Current best: 0.25121321148061004, Global best: 0.2292058946862569, Runtime: 7.83677 seconds\n",
      "> Epoch: 661, Current best: 0.28752287933296666, Global best: 0.2292058946862569, Runtime: 8.87303 seconds\n",
      "> Epoch: 662, Current best: 0.32489883793968266, Global best: 0.2292058946862569, Runtime: 12.41745 seconds\n",
      "> Epoch: 663, Current best: 0.23861992671910942, Global best: 0.2292058946862569, Runtime: 13.04895 seconds\n",
      "> Epoch: 664, Current best: 0.2376848989251994, Global best: 0.2292058946862569, Runtime: 13.60248 seconds\n",
      "> Epoch: 665, Current best: 0.23674566186019474, Global best: 0.2292058946862569, Runtime: 12.76222 seconds\n",
      "> Epoch: 666, Current best: 0.3654412644104895, Global best: 0.2292058946862569, Runtime: 11.93155 seconds\n",
      "> Epoch: 667, Current best: 0.25623764821482664, Global best: 0.2292058946862569, Runtime: 12.52409 seconds\n",
      "> Epoch: 668, Current best: 0.22597882462149774, Global best: 0.22597882462149774, Runtime: 12.72999 seconds\n",
      "> Epoch: 669, Current best: 0.22477142763807095, Global best: 0.22477142763807095, Runtime: 12.10552 seconds\n",
      "> Epoch: 670, Current best: 0.22728681385707925, Global best: 0.22477142763807095, Runtime: 7.70239 seconds\n",
      "> Epoch: 671, Current best: 0.2164376523141913, Global best: 0.2164376523141913, Runtime: 7.24015 seconds\n",
      "> Epoch: 672, Current best: 0.20945142916395137, Global best: 0.20945142916395137, Runtime: 7.38195 seconds\n",
      "> Epoch: 673, Current best: 0.2626121722426976, Global best: 0.20945142916395137, Runtime: 7.46914 seconds\n",
      "> Epoch: 674, Current best: 0.21891708842848193, Global best: 0.20945142916395137, Runtime: 7.82984 seconds\n",
      "> Epoch: 675, Current best: 0.20887177172653143, Global best: 0.20887177172653143, Runtime: 7.38325 seconds\n",
      "> Epoch: 676, Current best: 0.2098743325869141, Global best: 0.20887177172653143, Runtime: 7.30261 seconds\n",
      "> Epoch: 677, Current best: 0.219720114952053, Global best: 0.20887177172653143, Runtime: 7.28861 seconds\n",
      "> Epoch: 678, Current best: 0.19192364170361925, Global best: 0.19192364170361925, Runtime: 7.38030 seconds\n",
      "> Epoch: 679, Current best: 0.20149302892691445, Global best: 0.19192364170361925, Runtime: 7.42914 seconds\n",
      "> Epoch: 680, Current best: 0.20195123993022393, Global best: 0.19192364170361925, Runtime: 7.40276 seconds\n",
      "> Epoch: 681, Current best: 0.18831714877002675, Global best: 0.18831714877002675, Runtime: 7.37848 seconds\n",
      "> Epoch: 682, Current best: 0.2287026203126552, Global best: 0.18831714877002675, Runtime: 7.51297 seconds\n",
      "> Epoch: 683, Current best: 0.18600422067908412, Global best: 0.18600422067908412, Runtime: 7.29870 seconds\n",
      "> Epoch: 684, Current best: 0.3086933927891944, Global best: 0.18600422067908412, Runtime: 7.32752 seconds\n",
      "> Epoch: 685, Current best: 0.18724772185108218, Global best: 0.18600422067908412, Runtime: 7.18794 seconds\n",
      "> Epoch: 686, Current best: 0.21939610377124857, Global best: 0.18600422067908412, Runtime: 7.25704 seconds\n",
      "> Epoch: 687, Current best: 0.1843727291007505, Global best: 0.1843727291007505, Runtime: 8.25946 seconds\n",
      "> Epoch: 688, Current best: 0.19314994693533563, Global best: 0.1843727291007505, Runtime: 8.37179 seconds\n",
      "> Epoch: 689, Current best: 0.19957563770788475, Global best: 0.1843727291007505, Runtime: 8.78457 seconds\n",
      "> Epoch: 690, Current best: 0.18495349735521946, Global best: 0.1843727291007505, Runtime: 9.26876 seconds\n",
      "> Epoch: 691, Current best: 0.18545718679489176, Global best: 0.1843727291007505, Runtime: 8.39094 seconds\n",
      "> Epoch: 692, Current best: 0.19596903889821995, Global best: 0.1843727291007505, Runtime: 7.80389 seconds\n",
      "> Epoch: 693, Current best: 0.18436153422331517, Global best: 0.18436153422331517, Runtime: 8.64930 seconds\n",
      "> Epoch: 694, Current best: 0.19044386926274218, Global best: 0.18436153422331517, Runtime: 9.45842 seconds\n",
      "> Epoch: 695, Current best: 0.18402540215245458, Global best: 0.18402540215245458, Runtime: 8.90631 seconds\n",
      "> Epoch: 696, Current best: 0.21803960947090745, Global best: 0.18402540215245458, Runtime: 8.57326 seconds\n",
      "> Epoch: 697, Current best: 0.20661645603767176, Global best: 0.18402540215245458, Runtime: 9.09149 seconds\n",
      "> Epoch: 698, Current best: 0.21153234809980465, Global best: 0.18402540215245458, Runtime: 9.04976 seconds\n",
      "> Epoch: 699, Current best: 0.23585315130095502, Global best: 0.18402540215245458, Runtime: 7.54077 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 700, Current best: 0.18075060341824092, Global best: 0.18075060341824092, Runtime: 7.22279 seconds\n",
      "> Epoch: 701, Current best: 0.1966617031845387, Global best: 0.18075060341824092, Runtime: 7.22685 seconds\n",
      "> Epoch: 702, Current best: 0.19755665759311047, Global best: 0.18075060341824092, Runtime: 7.39435 seconds\n",
      "> Epoch: 703, Current best: 0.19967564259301648, Global best: 0.18075060341824092, Runtime: 7.42061 seconds\n",
      "> Epoch: 704, Current best: 0.18262212016589982, Global best: 0.18075060341824092, Runtime: 7.71918 seconds\n",
      "> Epoch: 705, Current best: 0.19103407797229077, Global best: 0.18075060341824092, Runtime: 7.40621 seconds\n",
      "> Epoch: 706, Current best: 0.1943963034634545, Global best: 0.18075060341824092, Runtime: 7.73860 seconds\n",
      "> Epoch: 707, Current best: 0.19082408226639622, Global best: 0.18075060341824092, Runtime: 7.38553 seconds\n",
      "> Epoch: 708, Current best: 0.19056559490086153, Global best: 0.18075060341824092, Runtime: 7.37935 seconds\n",
      "> Epoch: 709, Current best: 0.19206184238911028, Global best: 0.18075060341824092, Runtime: 7.25069 seconds\n",
      "> Epoch: 710, Current best: 0.1917479016716469, Global best: 0.18075060341824092, Runtime: 7.31053 seconds\n",
      "> Epoch: 711, Current best: 0.23246766215670836, Global best: 0.18075060341824092, Runtime: 7.45693 seconds\n",
      "> Epoch: 712, Current best: 0.1862278424020002, Global best: 0.18075060341824092, Runtime: 8.04907 seconds\n",
      "> Epoch: 713, Current best: 0.17926643006029727, Global best: 0.17926643006029727, Runtime: 7.44712 seconds\n",
      "> Epoch: 714, Current best: 0.1934869518312581, Global best: 0.17926643006029727, Runtime: 7.98935 seconds\n",
      "> Epoch: 715, Current best: 0.18961634959029747, Global best: 0.17926643006029727, Runtime: 7.62948 seconds\n",
      "> Epoch: 716, Current best: 0.1859053610179083, Global best: 0.17926643006029727, Runtime: 7.97930 seconds\n",
      "> Epoch: 717, Current best: 0.21649035479494635, Global best: 0.17926643006029727, Runtime: 8.72979 seconds\n",
      "> Epoch: 718, Current best: 0.18537822063708165, Global best: 0.17926643006029727, Runtime: 10.85410 seconds\n",
      "> Epoch: 719, Current best: 0.18239208670770976, Global best: 0.17926643006029727, Runtime: 14.50247 seconds\n",
      "> Epoch: 720, Current best: 0.20809286129574942, Global best: 0.17926643006029727, Runtime: 12.86173 seconds\n",
      "> Epoch: 721, Current best: 0.18866099870092287, Global best: 0.17926643006029727, Runtime: 12.48422 seconds\n",
      "> Epoch: 722, Current best: 0.18264858526454122, Global best: 0.17926643006029727, Runtime: 13.02542 seconds\n",
      "> Epoch: 723, Current best: 0.20165717622706897, Global best: 0.17926643006029727, Runtime: 12.64311 seconds\n",
      "> Epoch: 724, Current best: 0.18561261584354172, Global best: 0.17926643006029727, Runtime: 12.83431 seconds\n",
      "> Epoch: 725, Current best: 0.19721923830893007, Global best: 0.17926643006029727, Runtime: 13.15545 seconds\n",
      "> Epoch: 726, Current best: 0.17896838870152842, Global best: 0.17896838870152842, Runtime: 12.69047 seconds\n",
      "> Epoch: 727, Current best: 0.18278299866273856, Global best: 0.17896838870152842, Runtime: 12.33572 seconds\n",
      "> Epoch: 728, Current best: 0.17871735986288145, Global best: 0.17871735986288145, Runtime: 10.28065 seconds\n",
      "> Epoch: 729, Current best: 0.1764994311139865, Global best: 0.1764994311139865, Runtime: 7.40322 seconds\n",
      "> Epoch: 730, Current best: 0.18091575003099097, Global best: 0.1764994311139865, Runtime: 7.71120 seconds\n",
      "> Epoch: 731, Current best: 0.18679907852518043, Global best: 0.1764994311139865, Runtime: 8.77466 seconds\n",
      "> Epoch: 732, Current best: 0.18831713784170265, Global best: 0.1764994311139865, Runtime: 9.37564 seconds\n",
      "> Epoch: 733, Current best: 0.17487599499645487, Global best: 0.17487599499645487, Runtime: 8.26140 seconds\n",
      "> Epoch: 734, Current best: 0.16942372214466625, Global best: 0.16942372214466625, Runtime: 8.70696 seconds\n",
      "> Epoch: 735, Current best: 0.12153009003655668, Global best: 0.12153009003655668, Runtime: 9.04877 seconds\n",
      "> Epoch: 736, Current best: 0.11223376149637879, Global best: 0.11223376149637879, Runtime: 9.46631 seconds\n",
      "> Epoch: 737, Current best: 0.11439221336795038, Global best: 0.11223376149637879, Runtime: 8.24446 seconds\n",
      "> Epoch: 738, Current best: 0.13208025826728703, Global best: 0.11223376149637879, Runtime: 8.84890 seconds\n",
      "> Epoch: 739, Current best: 0.148417927984237, Global best: 0.11223376149637879, Runtime: 9.50426 seconds\n",
      "> Epoch: 740, Current best: 0.11684149838392173, Global best: 0.11223376149637879, Runtime: 8.83543 seconds\n",
      "> Epoch: 741, Current best: 0.11150066776467912, Global best: 0.11150066776467912, Runtime: 9.30657 seconds\n",
      "> Epoch: 742, Current best: 0.12403057708523148, Global best: 0.11150066776467912, Runtime: 9.85911 seconds\n",
      "> Epoch: 743, Current best: 0.10454930795529702, Global best: 0.10454930795529702, Runtime: 8.18566 seconds\n",
      "> Epoch: 744, Current best: 0.10525948699430528, Global best: 0.10454930795529702, Runtime: 7.53142 seconds\n",
      "> Epoch: 745, Current best: 0.15762285685599015, Global best: 0.10454930795529702, Runtime: 8.92479 seconds\n",
      "> Epoch: 746, Current best: 0.12339953723032933, Global best: 0.10454930795529702, Runtime: 7.92554 seconds\n",
      "> Epoch: 747, Current best: 0.11364194935548892, Global best: 0.10454930795529702, Runtime: 7.41319 seconds\n",
      "> Epoch: 748, Current best: 0.0990020969370186, Global best: 0.0990020969370186, Runtime: 7.31063 seconds\n",
      "> Epoch: 749, Current best: 0.095303460999749, Global best: 0.095303460999749, Runtime: 7.36569 seconds\n",
      "> Epoch: 750, Current best: 0.09429245045812126, Global best: 0.09429245045812126, Runtime: 7.65216 seconds\n",
      "> Epoch: 751, Current best: 0.1049516862748107, Global best: 0.09429245045812126, Runtime: 7.20389 seconds\n",
      "> Epoch: 752, Current best: 0.0969176456030088, Global best: 0.09429245045812126, Runtime: 7.26885 seconds\n",
      "> Epoch: 753, Current best: 0.10153346097265746, Global best: 0.09429245045812126, Runtime: 7.18890 seconds\n",
      "> Epoch: 754, Current best: 0.1198663903564491, Global best: 0.09429245045812126, Runtime: 7.27677 seconds\n",
      "> Epoch: 755, Current best: 0.0892760958449285, Global best: 0.0892760958449285, Runtime: 7.23479 seconds\n",
      "> Epoch: 756, Current best: 0.09324296550810697, Global best: 0.0892760958449285, Runtime: 7.28063 seconds\n",
      "> Epoch: 757, Current best: 0.08663871795107433, Global best: 0.08663871795107433, Runtime: 7.56520 seconds\n",
      "> Epoch: 758, Current best: 0.10411108906177374, Global best: 0.08663871795107433, Runtime: 8.00058 seconds\n",
      "> Epoch: 759, Current best: 0.09496134684057111, Global best: 0.08663871795107433, Runtime: 7.29960 seconds\n",
      "> Epoch: 760, Current best: 0.08558268113904802, Global best: 0.08558268113904802, Runtime: 7.19193 seconds\n",
      "> Epoch: 761, Current best: 0.08151056237280035, Global best: 0.08151056237280035, Runtime: 8.17767 seconds\n",
      "> Epoch: 762, Current best: 0.08094542064490576, Global best: 0.08094542064490576, Runtime: 7.18800 seconds\n",
      "> Epoch: 763, Current best: 0.08118370196615193, Global best: 0.08094542064490576, Runtime: 7.99927 seconds\n",
      "> Epoch: 764, Current best: 0.09832992653049774, Global best: 0.08094542064490576, Runtime: 7.51086 seconds\n",
      "> Epoch: 765, Current best: 0.08710571560463254, Global best: 0.08094542064490576, Runtime: 7.31461 seconds\n",
      "> Epoch: 766, Current best: 0.09198212775997812, Global best: 0.08094542064490576, Runtime: 7.88525 seconds\n",
      "> Epoch: 767, Current best: 0.0856665412805904, Global best: 0.08094542064490576, Runtime: 8.84539 seconds\n",
      "> Epoch: 768, Current best: 0.08938843130046949, Global best: 0.08094542064490576, Runtime: 8.40794 seconds\n",
      "> Epoch: 769, Current best: 0.08818018755205101, Global best: 0.08094542064490576, Runtime: 8.43615 seconds\n",
      "> Epoch: 770, Current best: 0.0892607050497098, Global best: 0.08094542064490576, Runtime: 9.21625 seconds\n",
      "> Epoch: 771, Current best: 0.11857142520114536, Global best: 0.08094542064490576, Runtime: 9.03379 seconds\n",
      "> Epoch: 772, Current best: 0.08410292595590105, Global best: 0.08094542064490576, Runtime: 8.64108 seconds\n",
      "> Epoch: 773, Current best: 0.08254147608663445, Global best: 0.08094542064490576, Runtime: 9.85180 seconds\n",
      "> Epoch: 774, Current best: 0.08018056575501677, Global best: 0.08018056575501677, Runtime: 9.53712 seconds\n",
      "> Epoch: 775, Current best: 0.08171075387116448, Global best: 0.08018056575501677, Runtime: 7.83179 seconds\n",
      "> Epoch: 776, Current best: 0.0877054525683191, Global best: 0.08018056575501677, Runtime: 8.71594 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 777, Current best: 0.0967326743630314, Global best: 0.08018056575501677, Runtime: 7.43441 seconds\n",
      "> Epoch: 778, Current best: 0.08055489623138729, Global best: 0.08018056575501677, Runtime: 7.59559 seconds\n",
      "> Epoch: 779, Current best: 0.08583732691514082, Global best: 0.08018056575501677, Runtime: 8.02315 seconds\n",
      "> Epoch: 780, Current best: 0.09464415646217049, Global best: 0.08018056575501677, Runtime: 8.83391 seconds\n",
      "> Epoch: 781, Current best: 0.07892487774682624, Global best: 0.07892487774682624, Runtime: 9.15887 seconds\n",
      "> Epoch: 782, Current best: 0.08556412508387878, Global best: 0.07892487774682624, Runtime: 9.54787 seconds\n",
      "> Epoch: 783, Current best: 0.08893798852340833, Global best: 0.07892487774682624, Runtime: 7.37566 seconds\n",
      "> Epoch: 784, Current best: 0.08950433480630801, Global best: 0.07892487774682624, Runtime: 7.33557 seconds\n",
      "> Epoch: 785, Current best: 0.08701918603082556, Global best: 0.07892487774682624, Runtime: 7.24778 seconds\n",
      "> Epoch: 786, Current best: 0.090177456759808, Global best: 0.07892487774682624, Runtime: 7.69621 seconds\n",
      "> Epoch: 787, Current best: 0.07819559069482142, Global best: 0.07819559069482142, Runtime: 7.73534 seconds\n",
      "> Epoch: 788, Current best: 0.08848932573205438, Global best: 0.07819559069482142, Runtime: 7.45530 seconds\n",
      "> Epoch: 789, Current best: 0.08755665635210264, Global best: 0.07819559069482142, Runtime: 7.24746 seconds\n",
      "> Epoch: 790, Current best: 0.08513247085685309, Global best: 0.07819559069482142, Runtime: 7.48103 seconds\n",
      "> Epoch: 791, Current best: 0.08534913112565438, Global best: 0.07819559069482142, Runtime: 7.52090 seconds\n",
      "> Epoch: 792, Current best: 0.08182356080321869, Global best: 0.07819559069482142, Runtime: 7.56365 seconds\n",
      "> Epoch: 793, Current best: 0.08253539649248152, Global best: 0.07819559069482142, Runtime: 7.53997 seconds\n",
      "> Epoch: 794, Current best: 0.09252083917156927, Global best: 0.07819559069482142, Runtime: 7.30855 seconds\n",
      "> Epoch: 795, Current best: 0.07783454199107932, Global best: 0.07783454199107932, Runtime: 7.74707 seconds\n",
      "> Epoch: 796, Current best: 0.09999270933619375, Global best: 0.07783454199107932, Runtime: 7.46701 seconds\n",
      "> Epoch: 797, Current best: 0.08142918143882782, Global best: 0.07783454199107932, Runtime: 7.29590 seconds\n",
      "> Epoch: 798, Current best: 0.07995548512957275, Global best: 0.07783454199107932, Runtime: 7.35680 seconds\n",
      "> Epoch: 799, Current best: 0.0941226593979351, Global best: 0.07783454199107932, Runtime: 7.22030 seconds\n",
      "> Epoch: 800, Current best: 0.08397065302015956, Global best: 0.07783454199107932, Runtime: 7.23680 seconds\n",
      "> Epoch: 801, Current best: 0.0832242824783405, Global best: 0.07783454199107932, Runtime: 7.24384 seconds\n",
      "> Epoch: 802, Current best: 0.09318416228785288, Global best: 0.07783454199107932, Runtime: 7.32848 seconds\n",
      "> Epoch: 803, Current best: 0.07855147445487007, Global best: 0.07783454199107932, Runtime: 7.53989 seconds\n",
      "> Epoch: 804, Current best: 0.08123805801307189, Global best: 0.07783454199107932, Runtime: 7.28861 seconds\n",
      "> Epoch: 805, Current best: 0.07773290582640294, Global best: 0.07773290582640294, Runtime: 7.45904 seconds\n",
      "> Epoch: 806, Current best: 0.0771511790432118, Global best: 0.0771511790432118, Runtime: 7.47698 seconds\n",
      "> Epoch: 807, Current best: 0.07414781541405212, Global best: 0.07414781541405212, Runtime: 8.97829 seconds\n",
      "> Epoch: 808, Current best: 0.07695563080453252, Global best: 0.07414781541405212, Runtime: 9.36527 seconds\n",
      "> Epoch: 809, Current best: 0.0803765238009849, Global best: 0.07414781541405212, Runtime: 8.93309 seconds\n",
      "> Epoch: 810, Current best: 0.07429341085276683, Global best: 0.07414781541405212, Runtime: 8.88914 seconds\n",
      "> Epoch: 811, Current best: 0.0826008270160901, Global best: 0.07414781541405212, Runtime: 8.87131 seconds\n",
      "> Epoch: 812, Current best: 0.07378465385782328, Global best: 0.07378465385782328, Runtime: 9.04128 seconds\n",
      "> Epoch: 813, Current best: 0.0771868340041108, Global best: 0.07378465385782328, Runtime: 8.16639 seconds\n",
      "> Epoch: 814, Current best: 0.07659521018905661, Global best: 0.07378465385782328, Runtime: 7.50089 seconds\n",
      "> Epoch: 815, Current best: 0.07700445723222749, Global best: 0.07378465385782328, Runtime: 7.23774 seconds\n",
      "> Epoch: 816, Current best: 0.07622367337799117, Global best: 0.07378465385782328, Runtime: 7.81684 seconds\n",
      "> Epoch: 817, Current best: 0.08073147032046077, Global best: 0.07378465385782328, Runtime: 10.96068 seconds\n",
      "> Epoch: 818, Current best: 0.0760300023460072, Global best: 0.07378465385782328, Runtime: 9.04672 seconds\n",
      "> Epoch: 819, Current best: 0.08221400048504408, Global best: 0.07378465385782328, Runtime: 8.77906 seconds\n",
      "> Epoch: 820, Current best: 0.07524566324861978, Global best: 0.07378465385782328, Runtime: 7.68756 seconds\n",
      "> Epoch: 821, Current best: 0.08112498386537613, Global best: 0.07378465385782328, Runtime: 7.91233 seconds\n",
      "> Epoch: 822, Current best: 0.07699587817997916, Global best: 0.07378465385782328, Runtime: 7.65788 seconds\n",
      "> Epoch: 823, Current best: 0.07492596507860315, Global best: 0.07378465385782328, Runtime: 8.96698 seconds\n",
      "> Epoch: 824, Current best: 0.08265229116148598, Global best: 0.07378465385782328, Runtime: 12.93793 seconds\n",
      "> Epoch: 825, Current best: 0.07529325189582192, Global best: 0.07378465385782328, Runtime: 14.38187 seconds\n",
      "> Epoch: 826, Current best: 0.08413969659962343, Global best: 0.07378465385782328, Runtime: 12.99403 seconds\n",
      "> Epoch: 827, Current best: 0.07723855663476312, Global best: 0.07378465385782328, Runtime: 12.61180 seconds\n",
      "> Epoch: 828, Current best: 0.07415632012546496, Global best: 0.07378465385782328, Runtime: 12.29186 seconds\n",
      "> Epoch: 829, Current best: 0.07409016273101057, Global best: 0.07378465385782328, Runtime: 15.02605 seconds\n",
      "> Epoch: 830, Current best: 0.07756286030431268, Global best: 0.07378465385782328, Runtime: 13.54313 seconds\n",
      "> Epoch: 831, Current best: 0.07386207290855917, Global best: 0.07378465385782328, Runtime: 8.44354 seconds\n",
      "> Epoch: 832, Current best: 0.07696421867437894, Global best: 0.07378465385782328, Runtime: 8.16778 seconds\n",
      "> Epoch: 833, Current best: 0.07490389769680884, Global best: 0.07378465385782328, Runtime: 8.08175 seconds\n",
      "> Epoch: 834, Current best: 0.07680344115538602, Global best: 0.07378465385782328, Runtime: 18.34801 seconds\n",
      "> Epoch: 835, Current best: 0.07336822655073919, Global best: 0.07336822655073919, Runtime: 10.96409 seconds\n",
      "> Epoch: 836, Current best: 0.0744442111701268, Global best: 0.07336822655073919, Runtime: 10.26214 seconds\n",
      "> Epoch: 837, Current best: 0.07537036423104822, Global best: 0.07336822655073919, Runtime: 13.84880 seconds\n",
      "> Epoch: 838, Current best: 0.07454908831764083, Global best: 0.07336822655073919, Runtime: 11.03503 seconds\n",
      "> Epoch: 839, Current best: 0.07720265119206177, Global best: 0.07336822655073919, Runtime: 12.14104 seconds\n",
      "> Epoch: 840, Current best: 0.07431346283056153, Global best: 0.07336822655073919, Runtime: 9.24155 seconds\n",
      "> Epoch: 841, Current best: 0.07435275482326577, Global best: 0.07336822655073919, Runtime: 13.85515 seconds\n",
      "> Epoch: 842, Current best: 0.07497523807710994, Global best: 0.07336822655073919, Runtime: 14.01549 seconds\n",
      "> Epoch: 843, Current best: 0.07620281244636305, Global best: 0.07336822655073919, Runtime: 10.82164 seconds\n",
      "> Epoch: 844, Current best: 0.08648317048147193, Global best: 0.07336822655073919, Runtime: 11.14183 seconds\n",
      "> Epoch: 845, Current best: 0.07763087845074607, Global best: 0.07336822655073919, Runtime: 12.02161 seconds\n",
      "> Epoch: 846, Current best: 0.07406808301176815, Global best: 0.07336822655073919, Runtime: 9.43473 seconds\n",
      "> Epoch: 847, Current best: 0.07386704060752751, Global best: 0.07336822655073919, Runtime: 9.04444 seconds\n",
      "> Epoch: 848, Current best: 0.07272190708237199, Global best: 0.07272190708237199, Runtime: 9.29485 seconds\n",
      "> Epoch: 849, Current best: 0.07267676141563295, Global best: 0.07267676141563295, Runtime: 13.21464 seconds\n",
      "> Epoch: 850, Current best: 0.07485486840770918, Global best: 0.07267676141563295, Runtime: 9.42826 seconds\n",
      "> Epoch: 851, Current best: 0.07627734338944046, Global best: 0.07267676141563295, Runtime: 9.09259 seconds\n",
      "> Epoch: 852, Current best: 0.07706558836819179, Global best: 0.07267676141563295, Runtime: 8.20336 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 853, Current best: 0.07314729402514088, Global best: 0.07267676141563295, Runtime: 10.76843 seconds\n",
      "> Epoch: 854, Current best: 0.0763116238851235, Global best: 0.07267676141563295, Runtime: 8.78403 seconds\n",
      "> Epoch: 855, Current best: 0.07590215686440625, Global best: 0.07267676141563295, Runtime: 7.79491 seconds\n",
      "> Epoch: 856, Current best: 0.08357925223090602, Global best: 0.07267676141563295, Runtime: 7.74612 seconds\n",
      "> Epoch: 857, Current best: 0.08926970280983139, Global best: 0.07267676141563295, Runtime: 8.30915 seconds\n",
      "> Epoch: 858, Current best: 0.07382989133869514, Global best: 0.07267676141563295, Runtime: 8.70438 seconds\n",
      "> Epoch: 859, Current best: 0.07233007882471507, Global best: 0.07233007882471507, Runtime: 7.88874 seconds\n",
      "> Epoch: 860, Current best: 0.07580294247604256, Global best: 0.07233007882471507, Runtime: 9.27695 seconds\n",
      "> Epoch: 861, Current best: 0.071831822997156, Global best: 0.071831822997156, Runtime: 10.94315 seconds\n",
      "> Epoch: 862, Current best: 0.07494149683602733, Global best: 0.071831822997156, Runtime: 8.27583 seconds\n",
      "> Epoch: 863, Current best: 0.07220098736289803, Global best: 0.071831822997156, Runtime: 8.46372 seconds\n",
      "> Epoch: 864, Current best: 0.0723222808440586, Global best: 0.071831822997156, Runtime: 10.96271 seconds\n",
      "> Epoch: 865, Current best: 0.0733291200578483, Global best: 0.071831822997156, Runtime: 9.93782 seconds\n",
      "> Epoch: 866, Current best: 0.07281407339708322, Global best: 0.071831822997156, Runtime: 9.36675 seconds\n",
      "> Epoch: 867, Current best: 0.07281811400032527, Global best: 0.071831822997156, Runtime: 10.68929 seconds\n",
      "> Epoch: 868, Current best: 0.07336640655687844, Global best: 0.071831822997156, Runtime: 8.58956 seconds\n",
      "> Epoch: 869, Current best: 0.07206700160037172, Global best: 0.071831822997156, Runtime: 8.09305 seconds\n",
      "> Epoch: 870, Current best: 0.07452171235418459, Global best: 0.071831822997156, Runtime: 8.98300 seconds\n",
      "> Epoch: 871, Current best: 0.0772603815227, Global best: 0.071831822997156, Runtime: 9.95120 seconds\n",
      "> Epoch: 872, Current best: 0.07295827830260344, Global best: 0.071831822997156, Runtime: 11.33536 seconds\n",
      "> Epoch: 873, Current best: 0.07797419821225021, Global best: 0.071831822997156, Runtime: 8.72331 seconds\n",
      "> Epoch: 874, Current best: 0.0759159157643936, Global best: 0.071831822997156, Runtime: 8.19157 seconds\n",
      "> Epoch: 875, Current best: 0.07641417201439403, Global best: 0.071831822997156, Runtime: 8.24758 seconds\n",
      "> Epoch: 876, Current best: 0.07382076729251762, Global best: 0.071831822997156, Runtime: 8.86727 seconds\n",
      "> Epoch: 877, Current best: 0.07927743307249763, Global best: 0.071831822997156, Runtime: 8.85644 seconds\n",
      "> Epoch: 878, Current best: 0.07328965044772293, Global best: 0.071831822997156, Runtime: 8.65803 seconds\n",
      "> Epoch: 879, Current best: 0.0792409707597367, Global best: 0.071831822997156, Runtime: 9.86747 seconds\n",
      "> Epoch: 880, Current best: 0.08308463826017445, Global best: 0.071831822997156, Runtime: 9.31560 seconds\n",
      "> Epoch: 881, Current best: 0.07201522771329248, Global best: 0.071831822997156, Runtime: 8.52772 seconds\n",
      "> Epoch: 882, Current best: 0.07162662790860959, Global best: 0.07162662790860959, Runtime: 10.14044 seconds\n",
      "> Epoch: 883, Current best: 0.07531488979496764, Global best: 0.07162662790860959, Runtime: 10.79224 seconds\n",
      "> Epoch: 884, Current best: 0.07728119940625092, Global best: 0.07162662790860959, Runtime: 11.74669 seconds\n",
      "> Epoch: 885, Current best: 0.07388942021432605, Global best: 0.07162662790860959, Runtime: 11.33315 seconds\n",
      "> Epoch: 886, Current best: 0.07146798059785611, Global best: 0.07146798059785611, Runtime: 11.64182 seconds\n",
      "> Epoch: 887, Current best: 0.07832165729349164, Global best: 0.07146798059785611, Runtime: 10.46762 seconds\n",
      "> Epoch: 888, Current best: 0.07134024830748398, Global best: 0.07134024830748398, Runtime: 11.01368 seconds\n",
      "> Epoch: 889, Current best: 0.07215809137965233, Global best: 0.07134024830748398, Runtime: 10.97717 seconds\n",
      "> Epoch: 890, Current best: 0.07663486239065463, Global best: 0.07134024830748398, Runtime: 9.71944 seconds\n",
      "> Epoch: 891, Current best: 0.07255167126658604, Global best: 0.07134024830748398, Runtime: 10.00423 seconds\n",
      "> Epoch: 892, Current best: 0.0774897196848933, Global best: 0.07134024830748398, Runtime: 9.48200 seconds\n",
      "> Epoch: 893, Current best: 0.07953637592650785, Global best: 0.07134024830748398, Runtime: 12.01315 seconds\n",
      "> Epoch: 894, Current best: 0.0725349701770587, Global best: 0.07134024830748398, Runtime: 13.56205 seconds\n",
      "> Epoch: 895, Current best: 0.08035850486057394, Global best: 0.07134024830748398, Runtime: 8.92707 seconds\n",
      "> Epoch: 896, Current best: 0.0687930353474313, Global best: 0.0687930353474313, Runtime: 8.18074 seconds\n",
      "> Epoch: 897, Current best: 0.06571548038227382, Global best: 0.06571548038227382, Runtime: 9.08652 seconds\n",
      "> Epoch: 898, Current best: 0.06553959547862928, Global best: 0.06553959547862928, Runtime: 7.70551 seconds\n",
      "> Epoch: 899, Current best: 0.06412498232780497, Global best: 0.06412498232780497, Runtime: 9.00687 seconds\n",
      "> Epoch: 900, Current best: 0.06546795058389668, Global best: 0.06412498232780497, Runtime: 14.12237 seconds\n",
      "> Epoch: 901, Current best: 0.06622879530908599, Global best: 0.06412498232780497, Runtime: 12.74259 seconds\n",
      "> Epoch: 902, Current best: 0.06748483886781215, Global best: 0.06412498232780497, Runtime: 11.14303 seconds\n",
      "> Epoch: 903, Current best: 0.06577286926037007, Global best: 0.06412498232780497, Runtime: 12.26891 seconds\n",
      "> Epoch: 904, Current best: 0.06431109934192994, Global best: 0.06412498232780497, Runtime: 10.38538 seconds\n",
      "> Epoch: 905, Current best: 0.06621608640649894, Global best: 0.06412498232780497, Runtime: 11.55159 seconds\n",
      "> Epoch: 906, Current best: 0.06344158908956977, Global best: 0.06344158908956977, Runtime: 9.66735 seconds\n",
      "> Epoch: 907, Current best: 0.06286948169354101, Global best: 0.06286948169354101, Runtime: 11.29303 seconds\n",
      "> Epoch: 908, Current best: 0.06659840567091348, Global best: 0.06286948169354101, Runtime: 16.70416 seconds\n",
      "> Epoch: 909, Current best: 0.06258189617477218, Global best: 0.06258189617477218, Runtime: 14.08170 seconds\n",
      "> Epoch: 910, Current best: 0.06286440098030847, Global best: 0.06258189617477218, Runtime: 13.56742 seconds\n",
      "> Epoch: 911, Current best: 0.06319739311053585, Global best: 0.06258189617477218, Runtime: 15.55108 seconds\n",
      "> Epoch: 912, Current best: 0.06286784032108929, Global best: 0.06258189617477218, Runtime: 16.80306 seconds\n",
      "> Epoch: 913, Current best: 0.05837733173083173, Global best: 0.05837733173083173, Runtime: 18.33322 seconds\n",
      "> Epoch: 914, Current best: 0.059669290879466375, Global best: 0.05837733173083173, Runtime: 17.48627 seconds\n",
      "> Epoch: 915, Current best: 0.058407850386842666, Global best: 0.05837733173083173, Runtime: 16.05609 seconds\n",
      "> Epoch: 916, Current best: 0.05580800118039449, Global best: 0.05580800118039449, Runtime: 14.76858 seconds\n",
      "> Epoch: 917, Current best: 0.05762785404307109, Global best: 0.05580800118039449, Runtime: 20.18097 seconds\n",
      "> Epoch: 918, Current best: 0.056177850827718784, Global best: 0.05580800118039449, Runtime: 18.85728 seconds\n",
      "> Epoch: 919, Current best: 0.05718678065596126, Global best: 0.05580800118039449, Runtime: 16.56734 seconds\n",
      "> Epoch: 920, Current best: 0.05682646907286838, Global best: 0.05580800118039449, Runtime: 17.48656 seconds\n",
      "> Epoch: 921, Current best: 0.06148282266885678, Global best: 0.05580800118039449, Runtime: 16.72916 seconds\n",
      "> Epoch: 922, Current best: 0.06326658981023392, Global best: 0.05580800118039449, Runtime: 11.29270 seconds\n",
      "> Epoch: 923, Current best: 0.05610760669595006, Global best: 0.05580800118039449, Runtime: 14.91764 seconds\n",
      "> Epoch: 924, Current best: 0.05181435720832009, Global best: 0.05181435720832009, Runtime: 18.49758 seconds\n",
      "> Epoch: 925, Current best: 0.05169036821989682, Global best: 0.05169036821989682, Runtime: 12.58029 seconds\n",
      "> Epoch: 926, Current best: 0.057290938682532426, Global best: 0.05169036821989682, Runtime: 19.22833 seconds\n",
      "> Epoch: 927, Current best: 0.052960758376905316, Global best: 0.05169036821989682, Runtime: 25.63838 seconds\n",
      "> Epoch: 928, Current best: 0.05385279314853811, Global best: 0.05169036821989682, Runtime: 18.47546 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 929, Current best: 0.05261801357914443, Global best: 0.05169036821989682, Runtime: 14.26257 seconds\n",
      "> Epoch: 930, Current best: 0.05108584949955635, Global best: 0.05108584949955635, Runtime: 12.95765 seconds\n",
      "> Epoch: 931, Current best: 0.05127024460019719, Global best: 0.05108584949955635, Runtime: 9.42090 seconds\n",
      "> Epoch: 932, Current best: 0.053655103047227135, Global best: 0.05108584949955635, Runtime: 15.82463 seconds\n",
      "> Epoch: 933, Current best: 0.05405257148656291, Global best: 0.05108584949955635, Runtime: 12.07849 seconds\n",
      "> Epoch: 934, Current best: 0.05337991364037006, Global best: 0.05108584949955635, Runtime: 11.29503 seconds\n",
      "> Epoch: 935, Current best: 0.05220052838206805, Global best: 0.05108584949955635, Runtime: 10.63173 seconds\n",
      "> Epoch: 936, Current best: 0.056042621217186124, Global best: 0.05108584949955635, Runtime: 10.85547 seconds\n",
      "> Epoch: 937, Current best: 0.05073842430483168, Global best: 0.05073842430483168, Runtime: 10.58112 seconds\n",
      "> Epoch: 938, Current best: 0.053898710216639736, Global best: 0.05073842430483168, Runtime: 8.66066 seconds\n",
      "> Epoch: 939, Current best: 0.053892807767514976, Global best: 0.05073842430483168, Runtime: 9.26324 seconds\n",
      "> Epoch: 940, Current best: 0.053057600359980306, Global best: 0.05073842430483168, Runtime: 9.13522 seconds\n",
      "> Epoch: 941, Current best: 0.05173632490146369, Global best: 0.05073842430483168, Runtime: 9.03369 seconds\n",
      "> Epoch: 942, Current best: 0.051137358687498455, Global best: 0.05073842430483168, Runtime: 9.76484 seconds\n",
      "> Epoch: 943, Current best: 0.0517281474693525, Global best: 0.05073842430483168, Runtime: 11.13984 seconds\n",
      "> Epoch: 944, Current best: 0.05374220108510767, Global best: 0.05073842430483168, Runtime: 11.45819 seconds\n",
      "> Epoch: 945, Current best: 0.051748932653185586, Global best: 0.05073842430483168, Runtime: 10.95841 seconds\n",
      "> Epoch: 946, Current best: 0.051379457626302937, Global best: 0.05073842430483168, Runtime: 9.52115 seconds\n",
      "> Epoch: 947, Current best: 0.05175841319230028, Global best: 0.05073842430483168, Runtime: 7.83484 seconds\n",
      "> Epoch: 948, Current best: 0.055609403296590566, Global best: 0.05073842430483168, Runtime: 7.73312 seconds\n",
      "> Epoch: 949, Current best: 0.05096472307620107, Global best: 0.05073842430483168, Runtime: 7.95252 seconds\n",
      "> Epoch: 950, Current best: 0.05710142917279531, Global best: 0.05073842430483168, Runtime: 7.91675 seconds\n",
      "> Epoch: 951, Current best: 0.05226729041535041, Global best: 0.05073842430483168, Runtime: 7.52622 seconds\n",
      "> Epoch: 952, Current best: 0.05265359118684547, Global best: 0.05073842430483168, Runtime: 8.15923 seconds\n",
      "> Epoch: 953, Current best: 0.052210360326418716, Global best: 0.05073842430483168, Runtime: 8.01550 seconds\n",
      "> Epoch: 954, Current best: 0.05166734561111024, Global best: 0.05073842430483168, Runtime: 7.70477 seconds\n",
      "> Epoch: 955, Current best: 0.05119909999820414, Global best: 0.05073842430483168, Runtime: 7.78353 seconds\n",
      "> Epoch: 956, Current best: 0.04549999657345549, Global best: 0.04549999657345549, Runtime: 7.83773 seconds\n",
      "> Epoch: 957, Current best: 0.04467461336514423, Global best: 0.04467461336514423, Runtime: 7.88966 seconds\n",
      "> Epoch: 958, Current best: 0.044824876970160764, Global best: 0.04467461336514423, Runtime: 7.73518 seconds\n",
      "> Epoch: 959, Current best: 0.04489521523809515, Global best: 0.04467461336514423, Runtime: 7.67963 seconds\n",
      "> Epoch: 960, Current best: 0.04390878748698576, Global best: 0.04390878748698576, Runtime: 7.73113 seconds\n",
      "> Epoch: 961, Current best: 0.044023328909970245, Global best: 0.04390878748698576, Runtime: 8.80944 seconds\n",
      "> Epoch: 962, Current best: 0.046150122295166705, Global best: 0.04390878748698576, Runtime: 8.00106 seconds\n",
      "> Epoch: 963, Current best: 0.04918404982081614, Global best: 0.04390878748698576, Runtime: 7.76681 seconds\n",
      "> Epoch: 964, Current best: 0.0452436348367006, Global best: 0.04390878748698576, Runtime: 7.82681 seconds\n",
      "> Epoch: 965, Current best: 0.04962156217568656, Global best: 0.04390878748698576, Runtime: 8.32320 seconds\n",
      "> Epoch: 966, Current best: 0.04532733652183007, Global best: 0.04390878748698576, Runtime: 7.64542 seconds\n",
      "> Epoch: 967, Current best: 0.04454817809388355, Global best: 0.04390878748698576, Runtime: 7.63663 seconds\n",
      "> Epoch: 968, Current best: 0.04405886707864608, Global best: 0.04390878748698576, Runtime: 7.58163 seconds\n",
      "> Epoch: 969, Current best: 0.043199782626745245, Global best: 0.043199782626745245, Runtime: 10.04169 seconds\n",
      "> Epoch: 970, Current best: 0.04604598991536522, Global best: 0.043199782626745245, Runtime: 7.53813 seconds\n",
      "> Epoch: 971, Current best: 0.04309423277758253, Global best: 0.04309423277758253, Runtime: 7.74335 seconds\n",
      "> Epoch: 972, Current best: 0.04434976777858847, Global best: 0.04309423277758253, Runtime: 7.75723 seconds\n",
      "> Epoch: 973, Current best: 0.043563218637826105, Global best: 0.04309423277758253, Runtime: 7.51694 seconds\n",
      "> Epoch: 974, Current best: 0.04350270665930421, Global best: 0.04309423277758253, Runtime: 7.48002 seconds\n",
      "> Epoch: 975, Current best: 0.0468703844558556, Global best: 0.04309423277758253, Runtime: 8.11922 seconds\n",
      "> Epoch: 976, Current best: 0.04535221385800033, Global best: 0.04309423277758253, Runtime: 7.79458 seconds\n",
      "> Epoch: 977, Current best: 0.043361316464893086, Global best: 0.04309423277758253, Runtime: 9.55986 seconds\n",
      "> Epoch: 978, Current best: 0.043743246620208294, Global best: 0.04309423277758253, Runtime: 10.31672 seconds\n",
      "> Epoch: 979, Current best: 0.04293714584207054, Global best: 0.04293714584207054, Runtime: 8.89129 seconds\n",
      "> Epoch: 980, Current best: 0.04490856719080247, Global best: 0.04293714584207054, Runtime: 7.96278 seconds\n",
      "> Epoch: 981, Current best: 0.04878014702406029, Global best: 0.04293714584207054, Runtime: 9.73817 seconds\n",
      "> Epoch: 982, Current best: 0.0447812886619093, Global best: 0.04293714584207054, Runtime: 9.80658 seconds\n",
      "> Epoch: 983, Current best: 0.043132339291448316, Global best: 0.04293714584207054, Runtime: 10.25668 seconds\n",
      "> Epoch: 984, Current best: 0.04390254391728873, Global best: 0.04293714584207054, Runtime: 9.32280 seconds\n",
      "> Epoch: 985, Current best: 0.04428250266261159, Global best: 0.04293714584207054, Runtime: 9.66743 seconds\n",
      "> Epoch: 986, Current best: 0.04196061615230954, Global best: 0.04196061615230954, Runtime: 7.93544 seconds\n",
      "> Epoch: 987, Current best: 0.04484981615076943, Global best: 0.04196061615230954, Runtime: 8.04811 seconds\n",
      "> Epoch: 988, Current best: 0.04268433663824753, Global best: 0.04196061615230954, Runtime: 8.71310 seconds\n",
      "> Epoch: 989, Current best: 0.04245629263412144, Global best: 0.04196061615230954, Runtime: 11.73837 seconds\n",
      "> Epoch: 990, Current best: 0.042462662395040755, Global best: 0.04196061615230954, Runtime: 12.52105 seconds\n",
      "> Epoch: 991, Current best: 0.04494987110452792, Global best: 0.04196061615230954, Runtime: 8.91183 seconds\n",
      "> Epoch: 992, Current best: 0.04537225178837067, Global best: 0.04196061615230954, Runtime: 10.88683 seconds\n",
      "> Epoch: 993, Current best: 0.042496209547244736, Global best: 0.04196061615230954, Runtime: 11.08292 seconds\n",
      "> Epoch: 994, Current best: 0.04339538028904745, Global best: 0.04196061615230954, Runtime: 11.29895 seconds\n",
      "> Epoch: 995, Current best: 0.043087062472420015, Global best: 0.04196061615230954, Runtime: 11.74731 seconds\n",
      "> Epoch: 996, Current best: 0.04214359341228799, Global best: 0.04196061615230954, Runtime: 9.81615 seconds\n",
      "> Epoch: 997, Current best: 0.04233137075687314, Global best: 0.04196061615230954, Runtime: 11.17134 seconds\n",
      "> Epoch: 998, Current best: 0.04344722422008792, Global best: 0.04196061615230954, Runtime: 10.51032 seconds\n",
      "> Epoch: 999, Current best: 0.0443036404065656, Global best: 0.04196061615230954, Runtime: 11.02624 seconds\n",
      "> Epoch: 1000, Current best: 0.042302576439553596, Global best: 0.04196061615230954, Runtime: 9.12962 seconds\n",
      "> Epoch: 1001, Current best: 0.04372236135881161, Global best: 0.04196061615230954, Runtime: 8.85653 seconds\n",
      "> Epoch: 1002, Current best: 0.04279174471071952, Global best: 0.04196061615230954, Runtime: 11.60252 seconds\n",
      "> Epoch: 1003, Current best: 0.042274967093956395, Global best: 0.04196061615230954, Runtime: 10.74026 seconds\n",
      "> Epoch: 1004, Current best: 0.04382627039756584, Global best: 0.04196061615230954, Runtime: 11.21248 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1005, Current best: 0.04225426487406599, Global best: 0.04196061615230954, Runtime: 13.12429 seconds\n",
      "> Epoch: 1006, Current best: 0.04143319869109045, Global best: 0.04143319869109045, Runtime: 13.42769 seconds\n",
      "> Epoch: 1007, Current best: 0.04256550087274881, Global best: 0.04143319869109045, Runtime: 12.29766 seconds\n",
      "> Epoch: 1008, Current best: 0.04573861648756438, Global best: 0.04143319869109045, Runtime: 17.16926 seconds\n",
      "> Epoch: 1009, Current best: 0.04342922939238308, Global best: 0.04143319869109045, Runtime: 14.56903 seconds\n",
      "> Epoch: 1010, Current best: 0.041066506267826396, Global best: 0.041066506267826396, Runtime: 14.18354 seconds\n",
      "> Epoch: 1011, Current best: 0.042317846991854535, Global best: 0.041066506267826396, Runtime: 14.29172 seconds\n",
      "> Epoch: 1012, Current best: 0.040834964376274895, Global best: 0.040834964376274895, Runtime: 10.89231 seconds\n",
      "> Epoch: 1013, Current best: 0.03985644338334532, Global best: 0.03985644338334532, Runtime: 9.33305 seconds\n",
      "> Epoch: 1014, Current best: 0.04007082169373442, Global best: 0.03985644338334532, Runtime: 9.15325 seconds\n",
      "> Epoch: 1015, Current best: 0.03974951548133365, Global best: 0.03974951548133365, Runtime: 9.19394 seconds\n",
      "> Epoch: 1016, Current best: 0.04173519480803371, Global best: 0.03974951548133365, Runtime: 10.51869 seconds\n",
      "> Epoch: 1017, Current best: 0.0394941880369676, Global best: 0.0394941880369676, Runtime: 9.17429 seconds\n",
      "> Epoch: 1018, Current best: 0.03740317754561199, Global best: 0.03740317754561199, Runtime: 9.69363 seconds\n",
      "> Epoch: 1019, Current best: 0.03803975270568013, Global best: 0.03740317754561199, Runtime: 9.29402 seconds\n",
      "> Epoch: 1020, Current best: 0.0373053637989385, Global best: 0.0373053637989385, Runtime: 10.59187 seconds\n",
      "> Epoch: 1021, Current best: 0.03711147138875709, Global best: 0.03711147138875709, Runtime: 10.89526 seconds\n",
      "> Epoch: 1022, Current best: 0.03766351389202269, Global best: 0.03711147138875709, Runtime: 10.16402 seconds\n",
      "> Epoch: 1023, Current best: 0.03932285660533735, Global best: 0.03711147138875709, Runtime: 9.57096 seconds\n",
      "> Epoch: 1024, Current best: 0.037258637723994174, Global best: 0.03711147138875709, Runtime: 10.53272 seconds\n",
      "> Epoch: 1025, Current best: 0.03758103264598479, Global best: 0.03711147138875709, Runtime: 8.64013 seconds\n",
      "> Epoch: 1026, Current best: 0.03826165798792189, Global best: 0.03711147138875709, Runtime: 8.45379 seconds\n",
      "> Epoch: 1027, Current best: 0.038656835386537775, Global best: 0.03711147138875709, Runtime: 8.41683 seconds\n",
      "> Epoch: 1028, Current best: 0.03788647951340322, Global best: 0.03711147138875709, Runtime: 8.37227 seconds\n",
      "> Epoch: 1029, Current best: 0.03927896877774889, Global best: 0.03711147138875709, Runtime: 8.36098 seconds\n",
      "> Epoch: 1030, Current best: 0.0375904993687812, Global best: 0.03711147138875709, Runtime: 8.54140 seconds\n",
      "> Epoch: 1031, Current best: 0.03873685643852424, Global best: 0.03711147138875709, Runtime: 8.65971 seconds\n",
      "> Epoch: 1032, Current best: 0.03719435806199998, Global best: 0.03711147138875709, Runtime: 8.43375 seconds\n",
      "> Epoch: 1033, Current best: 0.037135317890720214, Global best: 0.03711147138875709, Runtime: 8.59353 seconds\n",
      "> Epoch: 1034, Current best: 0.03824859020471361, Global best: 0.03711147138875709, Runtime: 8.82747 seconds\n",
      "> Epoch: 1035, Current best: 0.03758053128441829, Global best: 0.03711147138875709, Runtime: 8.76761 seconds\n",
      "> Epoch: 1036, Current best: 0.038220591015115386, Global best: 0.03711147138875709, Runtime: 8.67999 seconds\n",
      "> Epoch: 1037, Current best: 0.037799928420587756, Global best: 0.03711147138875709, Runtime: 8.29424 seconds\n",
      "> Epoch: 1038, Current best: 0.03641576866466039, Global best: 0.03641576866466039, Runtime: 8.64072 seconds\n",
      "> Epoch: 1039, Current best: 0.03657678709025869, Global best: 0.03641576866466039, Runtime: 8.41684 seconds\n",
      "> Epoch: 1040, Current best: 0.03655431918912796, Global best: 0.03641576866466039, Runtime: 8.25239 seconds\n",
      "> Epoch: 1041, Current best: 0.03749913951527682, Global best: 0.03641576866466039, Runtime: 8.98416 seconds\n",
      "> Epoch: 1042, Current best: 0.03659670214541245, Global best: 0.03641576866466039, Runtime: 8.47859 seconds\n",
      "> Epoch: 1043, Current best: 0.03584673697611149, Global best: 0.03584673697611149, Runtime: 8.30752 seconds\n",
      "> Epoch: 1044, Current best: 0.036364933884253804, Global best: 0.03584673697611149, Runtime: 8.40087 seconds\n",
      "> Epoch: 1045, Current best: 0.03684645353801035, Global best: 0.03584673697611149, Runtime: 8.80167 seconds\n",
      "> Epoch: 1046, Current best: 0.037478568882328506, Global best: 0.03584673697611149, Runtime: 8.38997 seconds\n",
      "> Epoch: 1047, Current best: 0.03682277101826289, Global best: 0.03584673697611149, Runtime: 8.57028 seconds\n",
      "> Epoch: 1048, Current best: 0.03466460768035743, Global best: 0.03466460768035743, Runtime: 8.65903 seconds\n",
      "> Epoch: 1049, Current best: 0.034841376532937195, Global best: 0.03466460768035743, Runtime: 8.36115 seconds\n",
      "> Epoch: 1050, Current best: 0.0388357582731449, Global best: 0.03466460768035743, Runtime: 8.41701 seconds\n",
      "> Epoch: 1051, Current best: 0.03538732272600677, Global best: 0.03466460768035743, Runtime: 8.42183 seconds\n",
      "> Epoch: 1052, Current best: 0.039374568557837726, Global best: 0.03466460768035743, Runtime: 8.93425 seconds\n",
      "> Epoch: 1053, Current best: 0.036749616470669065, Global best: 0.03466460768035743, Runtime: 8.52049 seconds\n",
      "> Epoch: 1054, Current best: 0.037772188719238375, Global best: 0.03466460768035743, Runtime: 8.40487 seconds\n",
      "> Epoch: 1055, Current best: 0.03658831525259047, Global best: 0.03466460768035743, Runtime: 8.39697 seconds\n",
      "> Epoch: 1056, Current best: 0.034860146907001906, Global best: 0.03466460768035743, Runtime: 8.54541 seconds\n",
      "> Epoch: 1057, Current best: 0.033111404208738274, Global best: 0.033111404208738274, Runtime: 8.52746 seconds\n",
      "> Epoch: 1058, Current best: 0.034101780537231646, Global best: 0.033111404208738274, Runtime: 8.41388 seconds\n",
      "> Epoch: 1059, Current best: 0.03391113080399583, Global best: 0.033111404208738274, Runtime: 9.37862 seconds\n",
      "> Epoch: 1060, Current best: 0.03231084663012996, Global best: 0.03231084663012996, Runtime: 8.42577 seconds\n",
      "> Epoch: 1061, Current best: 0.032656484445244695, Global best: 0.03231084663012996, Runtime: 8.61218 seconds\n",
      "> Epoch: 1062, Current best: 0.032826762140391645, Global best: 0.03231084663012996, Runtime: 8.59823 seconds\n",
      "> Epoch: 1063, Current best: 0.03218758312233814, Global best: 0.03218758312233814, Runtime: 8.93011 seconds\n",
      "> Epoch: 1064, Current best: 0.033284752331006015, Global best: 0.03218758312233814, Runtime: 8.32215 seconds\n",
      "> Epoch: 1065, Current best: 0.03228694163364241, Global best: 0.03218758312233814, Runtime: 8.37895 seconds\n",
      "> Epoch: 1066, Current best: 0.03325595940844868, Global best: 0.03218758312233814, Runtime: 8.69391 seconds\n",
      "> Epoch: 1067, Current best: 0.03265706881724539, Global best: 0.03218758312233814, Runtime: 8.79083 seconds\n",
      "> Epoch: 1068, Current best: 0.032812019273461335, Global best: 0.03218758312233814, Runtime: 9.00890 seconds\n",
      "> Epoch: 1069, Current best: 0.032085644385706415, Global best: 0.032085644385706415, Runtime: 8.42162 seconds\n",
      "> Epoch: 1070, Current best: 0.032430339880062824, Global best: 0.032085644385706415, Runtime: 8.73980 seconds\n",
      "> Epoch: 1071, Current best: 0.03224029656598741, Global best: 0.032085644385706415, Runtime: 8.82942 seconds\n",
      "> Epoch: 1072, Current best: 0.03204951930637105, Global best: 0.03204951930637105, Runtime: 10.39093 seconds\n",
      "> Epoch: 1073, Current best: 0.03564682842029823, Global best: 0.03204951930637105, Runtime: 12.18729 seconds\n",
      "> Epoch: 1074, Current best: 0.027635610552005845, Global best: 0.027635610552005845, Runtime: 12.21339 seconds\n",
      "> Epoch: 1075, Current best: 0.028112567048161747, Global best: 0.027635610552005845, Runtime: 9.26403 seconds\n",
      "> Epoch: 1076, Current best: 0.029755631953721336, Global best: 0.027635610552005845, Runtime: 9.15257 seconds\n",
      "> Epoch: 1077, Current best: 0.025601649398280833, Global best: 0.025601649398280833, Runtime: 10.89155 seconds\n",
      "> Epoch: 1078, Current best: 0.025177772251757803, Global best: 0.025177772251757803, Runtime: 10.48089 seconds\n",
      "> Epoch: 1079, Current best: 0.026736954846660107, Global best: 0.025177772251757803, Runtime: 9.54226 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1080, Current best: 0.027159672343951644, Global best: 0.025177772251757803, Runtime: 8.99897 seconds\n",
      "> Epoch: 1081, Current best: 0.02513428599430572, Global best: 0.02513428599430572, Runtime: 8.77981 seconds\n",
      "> Epoch: 1082, Current best: 0.024995200068180495, Global best: 0.024995200068180495, Runtime: 8.57232 seconds\n",
      "> Epoch: 1083, Current best: 0.027071203281050067, Global best: 0.024995200068180495, Runtime: 8.64925 seconds\n",
      "> Epoch: 1084, Current best: 0.02403085290539268, Global best: 0.02403085290539268, Runtime: 8.50074 seconds\n",
      "> Epoch: 1085, Current best: 0.024629344483806163, Global best: 0.02403085290539268, Runtime: 8.73776 seconds\n",
      "> Epoch: 1086, Current best: 0.024032633912361705, Global best: 0.02403085290539268, Runtime: 8.36998 seconds\n",
      "> Epoch: 1087, Current best: 0.02824325245390675, Global best: 0.02403085290539268, Runtime: 8.46473 seconds\n",
      "> Epoch: 1088, Current best: 0.02495579014259449, Global best: 0.02403085290539268, Runtime: 8.47585 seconds\n",
      "> Epoch: 1089, Current best: 0.024326761194212437, Global best: 0.02403085290539268, Runtime: 8.39591 seconds\n",
      "> Epoch: 1090, Current best: 0.02423529414286681, Global best: 0.02403085290539268, Runtime: 8.44287 seconds\n",
      "> Epoch: 1091, Current best: 0.02527535974992045, Global best: 0.02403085290539268, Runtime: 10.53969 seconds\n",
      "> Epoch: 1092, Current best: 0.02447470833533998, Global best: 0.02403085290539268, Runtime: 13.04337 seconds\n",
      "> Epoch: 1093, Current best: 0.024121426815929277, Global best: 0.02403085290539268, Runtime: 9.61595 seconds\n",
      "> Epoch: 1094, Current best: 0.024079840491621797, Global best: 0.02403085290539268, Runtime: 8.85739 seconds\n",
      "> Epoch: 1095, Current best: 0.025016914165896433, Global best: 0.02403085290539268, Runtime: 8.55015 seconds\n",
      "> Epoch: 1096, Current best: 0.02710323710285394, Global best: 0.02403085290539268, Runtime: 8.60405 seconds\n",
      "> Epoch: 1097, Current best: 0.024528571054573234, Global best: 0.02403085290539268, Runtime: 9.55809 seconds\n",
      "> Epoch: 1098, Current best: 0.024879628931701755, Global best: 0.02403085290539268, Runtime: 10.51487 seconds\n",
      "> Epoch: 1099, Current best: 0.02488926376099552, Global best: 0.02403085290539268, Runtime: 12.98554 seconds\n",
      "> Epoch: 1100, Current best: 0.02465729699609132, Global best: 0.02403085290539268, Runtime: 12.33777 seconds\n",
      "> Epoch: 1101, Current best: 0.024015337792390984, Global best: 0.024015337792390984, Runtime: 9.28163 seconds\n",
      "> Epoch: 1102, Current best: 0.026147563440473493, Global best: 0.024015337792390984, Runtime: 9.55001 seconds\n",
      "> Epoch: 1103, Current best: 0.024481407497996925, Global best: 0.024015337792390984, Runtime: 10.75552 seconds\n",
      "> Epoch: 1104, Current best: 0.024191902431079253, Global best: 0.024015337792390984, Runtime: 9.82611 seconds\n",
      "> Epoch: 1105, Current best: 0.025137530727097412, Global best: 0.024015337792390984, Runtime: 8.70891 seconds\n",
      "> Epoch: 1106, Current best: 0.024183403823092114, Global best: 0.024015337792390984, Runtime: 9.03873 seconds\n",
      "> Epoch: 1107, Current best: 0.025289345663801803, Global best: 0.024015337792390984, Runtime: 8.89619 seconds\n",
      "> Epoch: 1108, Current best: 0.02550824293651645, Global best: 0.024015337792390984, Runtime: 8.86133 seconds\n",
      "> Epoch: 1109, Current best: 0.025399397867208198, Global best: 0.024015337792390984, Runtime: 13.16745 seconds\n",
      "> Epoch: 1110, Current best: 0.026115583357431634, Global best: 0.024015337792390984, Runtime: 13.58661 seconds\n",
      "> Epoch: 1111, Current best: 0.024271269273842568, Global best: 0.024015337792390984, Runtime: 12.17741 seconds\n",
      "> Epoch: 1112, Current best: 0.02421744377415656, Global best: 0.024015337792390984, Runtime: 8.77168 seconds\n",
      "> Epoch: 1113, Current best: 0.024712021445971837, Global best: 0.024015337792390984, Runtime: 8.69487 seconds\n",
      "> Epoch: 1114, Current best: 0.020042969895760026, Global best: 0.020042969895760026, Runtime: 9.85601 seconds\n",
      "> Epoch: 1115, Current best: 0.021660855052447397, Global best: 0.020042969895760026, Runtime: 11.75567 seconds\n",
      "> Epoch: 1116, Current best: 0.01885315502332751, Global best: 0.01885315502332751, Runtime: 10.02880 seconds\n",
      "> Epoch: 1117, Current best: 0.019924522651393897, Global best: 0.01885315502332751, Runtime: 9.08166 seconds\n",
      "> Epoch: 1118, Current best: 0.01979747834265355, Global best: 0.01885315502332751, Runtime: 8.71818 seconds\n",
      "> Epoch: 1119, Current best: 0.02123355202676003, Global best: 0.01885315502332751, Runtime: 8.51913 seconds\n",
      "> Epoch: 1120, Current best: 0.020597480900207306, Global best: 0.01885315502332751, Runtime: 8.58926 seconds\n",
      "> Epoch: 1121, Current best: 0.019675468111218546, Global best: 0.01885315502332751, Runtime: 8.47663 seconds\n",
      "> Epoch: 1122, Current best: 0.018846299155702188, Global best: 0.018846299155702188, Runtime: 8.82945 seconds\n",
      "> Epoch: 1123, Current best: 0.01918629126811047, Global best: 0.018846299155702188, Runtime: 8.49472 seconds\n",
      "> Epoch: 1124, Current best: 0.01863698640515545, Global best: 0.01863698640515545, Runtime: 8.57032 seconds\n",
      "> Epoch: 1125, Current best: 0.01910735985531934, Global best: 0.01863698640515545, Runtime: 8.63509 seconds\n",
      "> Epoch: 1126, Current best: 0.019054514998214644, Global best: 0.01863698640515545, Runtime: 8.48166 seconds\n",
      "> Epoch: 1127, Current best: 0.018659616162423176, Global best: 0.01863698640515545, Runtime: 8.69197 seconds\n",
      "> Epoch: 1128, Current best: 0.018803096572487087, Global best: 0.01863698640515545, Runtime: 8.77657 seconds\n",
      "> Epoch: 1129, Current best: 0.019334607026438612, Global best: 0.01863698640515545, Runtime: 9.13644 seconds\n",
      "> Epoch: 1130, Current best: 0.019204207854075433, Global best: 0.01863698640515545, Runtime: 8.47763 seconds\n",
      "> Epoch: 1131, Current best: 0.019159594451798507, Global best: 0.01863698640515545, Runtime: 8.48859 seconds\n",
      "> Epoch: 1132, Current best: 0.0187466827192375, Global best: 0.01863698640515545, Runtime: 8.68294 seconds\n",
      "> Epoch: 1133, Current best: 0.020621409424931274, Global best: 0.01863698640515545, Runtime: 8.57630 seconds\n",
      "> Epoch: 1134, Current best: 0.02012399190496969, Global best: 0.01863698640515545, Runtime: 8.53344 seconds\n",
      "> Epoch: 1135, Current best: 0.01912330029158464, Global best: 0.01863698640515545, Runtime: 8.73286 seconds\n",
      "> Epoch: 1136, Current best: 0.019387919015862547, Global best: 0.01863698640515545, Runtime: 8.99689 seconds\n",
      "> Epoch: 1137, Current best: 0.018893315171534423, Global best: 0.01863698640515545, Runtime: 8.59922 seconds\n",
      "> Epoch: 1138, Current best: 0.0192312254806688, Global best: 0.01863698640515545, Runtime: 8.77065 seconds\n",
      "> Epoch: 1139, Current best: 0.01887561509773589, Global best: 0.01863698640515545, Runtime: 9.39456 seconds\n",
      "> Epoch: 1140, Current best: 0.01928397745269075, Global best: 0.01863698640515545, Runtime: 8.61318 seconds\n",
      "> Epoch: 1141, Current best: 0.018916376380486925, Global best: 0.01863698640515545, Runtime: 8.63710 seconds\n",
      "> Epoch: 1142, Current best: 0.01877548883141712, Global best: 0.01863698640515545, Runtime: 8.79258 seconds\n",
      "> Epoch: 1143, Current best: 0.019125402920640085, Global best: 0.01863698640515545, Runtime: 8.75171 seconds\n",
      "> Epoch: 1144, Current best: 0.01838630381401625, Global best: 0.01838630381401625, Runtime: 8.85935 seconds\n",
      "> Epoch: 1145, Current best: 0.017318888068787633, Global best: 0.017318888068787633, Runtime: 9.06666 seconds\n",
      "> Epoch: 1146, Current best: 0.017680768479899602, Global best: 0.017318888068787633, Runtime: 8.80862 seconds\n",
      "> Epoch: 1147, Current best: 0.018140541676089524, Global best: 0.017318888068787633, Runtime: 8.47276 seconds\n",
      "> Epoch: 1148, Current best: 0.017265922380498806, Global best: 0.017265922380498806, Runtime: 8.69394 seconds\n",
      "> Epoch: 1149, Current best: 0.016559998989656988, Global best: 0.016559998989656988, Runtime: 8.99786 seconds\n",
      "> Epoch: 1150, Current best: 0.01715892305440698, Global best: 0.016559998989656988, Runtime: 8.94111 seconds\n",
      "> Epoch: 1151, Current best: 0.01617822832618047, Global best: 0.01617822832618047, Runtime: 8.87628 seconds\n",
      "> Epoch: 1152, Current best: 0.015730875449799984, Global best: 0.015730875449799984, Runtime: 8.59138 seconds\n",
      "> Epoch: 1153, Current best: 0.015604302265983808, Global best: 0.015604302265983808, Runtime: 8.80550 seconds\n",
      "> Epoch: 1154, Current best: 0.01578211514597355, Global best: 0.015604302265983808, Runtime: 9.52018 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1155, Current best: 0.015573659611698782, Global best: 0.015573659611698782, Runtime: 9.19932 seconds\n",
      "> Epoch: 1156, Current best: 0.015736424323757006, Global best: 0.015573659611698782, Runtime: 9.77525 seconds\n",
      "> Epoch: 1157, Current best: 0.016818783376925495, Global best: 0.015573659611698782, Runtime: 9.15355 seconds\n",
      "> Epoch: 1158, Current best: 0.015770880380821105, Global best: 0.015573659611698782, Runtime: 8.67095 seconds\n",
      "> Epoch: 1159, Current best: 0.016330164333309617, Global best: 0.015573659611698782, Runtime: 8.97222 seconds\n",
      "> Epoch: 1160, Current best: 0.016449975443792394, Global best: 0.015573659611698782, Runtime: 12.14043 seconds\n",
      "> Epoch: 1161, Current best: 0.015686861422729775, Global best: 0.015573659611698782, Runtime: 11.91084 seconds\n",
      "> Epoch: 1162, Current best: 0.01632045544764279, Global best: 0.015573659611698782, Runtime: 9.93978 seconds\n",
      "> Epoch: 1163, Current best: 0.016295562374007814, Global best: 0.015573659611698782, Runtime: 8.77015 seconds\n",
      "> Epoch: 1164, Current best: 0.01600726044114196, Global best: 0.015573659611698782, Runtime: 8.50953 seconds\n",
      "> Epoch: 1165, Current best: 0.016211485941775843, Global best: 0.015573659611698782, Runtime: 11.04106 seconds\n",
      "> Epoch: 1166, Current best: 0.01875719662916351, Global best: 0.015573659611698782, Runtime: 14.45765 seconds\n",
      "> Epoch: 1167, Current best: 0.01646434094687949, Global best: 0.015573659611698782, Runtime: 10.10344 seconds\n",
      "> Epoch: 1168, Current best: 0.015902613797362908, Global best: 0.015573659611698782, Runtime: 14.31534 seconds\n",
      "> Epoch: 1169, Current best: 0.016342686865817708, Global best: 0.015573659611698782, Runtime: 10.85639 seconds\n",
      "> Epoch: 1170, Current best: 0.01578884549920742, Global best: 0.015573659611698782, Runtime: 14.61298 seconds\n",
      "> Epoch: 1171, Current best: 0.015716270735684938, Global best: 0.015573659611698782, Runtime: 9.92947 seconds\n",
      "> Epoch: 1172, Current best: 0.01619330265446639, Global best: 0.015573659611698782, Runtime: 15.26996 seconds\n",
      "> Epoch: 1173, Current best: 0.01584671394781887, Global best: 0.015573659611698782, Runtime: 11.95650 seconds\n",
      "> Epoch: 1174, Current best: 0.015279218988929347, Global best: 0.015279218988929347, Runtime: 11.74683 seconds\n",
      "> Epoch: 1175, Current best: 0.015064899566646531, Global best: 0.015064899566646531, Runtime: 18.53237 seconds\n",
      "> Epoch: 1176, Current best: 0.014808174499351318, Global best: 0.014808174499351318, Runtime: 18.34063 seconds\n",
      "> Epoch: 1177, Current best: 0.016149866337918893, Global best: 0.014808174499351318, Runtime: 17.87051 seconds\n",
      "> Epoch: 1178, Current best: 0.01679117274117126, Global best: 0.014808174499351318, Runtime: 15.08187 seconds\n",
      "> Epoch: 1179, Current best: 0.015048926202723406, Global best: 0.014808174499351318, Runtime: 17.72768 seconds\n",
      "> Epoch: 1180, Current best: 0.014915580267468016, Global best: 0.014808174499351318, Runtime: 19.10706 seconds\n",
      "> Epoch: 1181, Current best: 0.015263379962414466, Global best: 0.014808174499351318, Runtime: 18.80208 seconds\n",
      "> Epoch: 1182, Current best: 0.015238286001136863, Global best: 0.014808174499351318, Runtime: 16.91738 seconds\n",
      "> Epoch: 1183, Current best: 0.014465125492162001, Global best: 0.014465125492162001, Runtime: 16.98118 seconds\n",
      "> Epoch: 1184, Current best: 0.014431278273744921, Global best: 0.014431278273744921, Runtime: 15.92501 seconds\n",
      "> Epoch: 1185, Current best: 0.015515666861878948, Global best: 0.014431278273744921, Runtime: 17.72246 seconds\n",
      "> Epoch: 1186, Current best: 0.015231811406064567, Global best: 0.014431278273744921, Runtime: 17.50420 seconds\n",
      "> Epoch: 1187, Current best: 0.014191776905370605, Global best: 0.014191776905370605, Runtime: 15.58657 seconds\n",
      "> Epoch: 1188, Current best: 0.01458371682358814, Global best: 0.014191776905370605, Runtime: 20.85520 seconds\n",
      "> Epoch: 1189, Current best: 0.014465617000367267, Global best: 0.014191776905370605, Runtime: 17.82133 seconds\n",
      "> Epoch: 1190, Current best: 0.01383312077290469, Global best: 0.01383312077290469, Runtime: 12.84939 seconds\n",
      "> Epoch: 1191, Current best: 0.015005396109750213, Global best: 0.01383312077290469, Runtime: 14.85876 seconds\n",
      "> Epoch: 1192, Current best: 0.01569923979064395, Global best: 0.01383312077290469, Runtime: 12.22540 seconds\n",
      "> Epoch: 1193, Current best: 0.01383036786809471, Global best: 0.01383036786809471, Runtime: 11.30929 seconds\n",
      "> Epoch: 1194, Current best: 0.013315015529628237, Global best: 0.013315015529628237, Runtime: 14.80079 seconds\n",
      "> Epoch: 1195, Current best: 0.015287249762050783, Global best: 0.013315015529628237, Runtime: 15.78518 seconds\n",
      "> Epoch: 1196, Current best: 0.014712689305057832, Global best: 0.013315015529628237, Runtime: 12.08369 seconds\n",
      "> Epoch: 1197, Current best: 0.013256379560104302, Global best: 0.013256379560104302, Runtime: 10.75827 seconds\n",
      "> Epoch: 1198, Current best: 0.0138089440743762, Global best: 0.013256379560104302, Runtime: 8.85440 seconds\n",
      "> Epoch: 1199, Current best: 0.013581159955274294, Global best: 0.013256379560104302, Runtime: 11.70172 seconds\n",
      "> Epoch: 1200, Current best: 0.014020823359932022, Global best: 0.013256379560104302, Runtime: 11.34748 seconds\n",
      "> Epoch: 1201, Current best: 0.014230612682951458, Global best: 0.013256379560104302, Runtime: 9.25110 seconds\n",
      "> Epoch: 1202, Current best: 0.013379851271009375, Global best: 0.013256379560104302, Runtime: 12.28520 seconds\n",
      "> Epoch: 1203, Current best: 0.013734562367524473, Global best: 0.013256379560104302, Runtime: 10.89959 seconds\n",
      "> Epoch: 1204, Current best: 0.013214832657808115, Global best: 0.013214832657808115, Runtime: 12.75137 seconds\n",
      "> Epoch: 1205, Current best: 0.015268115930367683, Global best: 0.013214832657808115, Runtime: 12.86745 seconds\n",
      "> Epoch: 1206, Current best: 0.013212077268360106, Global best: 0.013212077268360106, Runtime: 12.59505 seconds\n",
      "> Epoch: 1207, Current best: 0.01327110243059244, Global best: 0.013212077268360106, Runtime: 11.54635 seconds\n",
      "> Epoch: 1208, Current best: 0.013513210990192296, Global best: 0.013212077268360106, Runtime: 9.85826 seconds\n",
      "> Epoch: 1209, Current best: 0.013493892253149746, Global best: 0.013212077268360106, Runtime: 9.46732 seconds\n",
      "> Epoch: 1210, Current best: 0.012926499666216298, Global best: 0.012926499666216298, Runtime: 12.17135 seconds\n",
      "> Epoch: 1211, Current best: 0.012848365071479635, Global best: 0.012848365071479635, Runtime: 10.98362 seconds\n",
      "> Epoch: 1212, Current best: 0.013443821254536262, Global best: 0.012848365071479635, Runtime: 11.26473 seconds\n",
      "> Epoch: 1213, Current best: 0.013046432236355819, Global best: 0.012848365071479635, Runtime: 12.13036 seconds\n",
      "> Epoch: 1214, Current best: 0.01385872674769091, Global best: 0.012848365071479635, Runtime: 9.25802 seconds\n",
      "> Epoch: 1215, Current best: 0.013669273820442664, Global best: 0.012848365071479635, Runtime: 10.53648 seconds\n",
      "> Epoch: 1216, Current best: 0.013260521925166887, Global best: 0.012848365071479635, Runtime: 10.92280 seconds\n",
      "> Epoch: 1217, Current best: 0.012445423007674428, Global best: 0.012445423007674428, Runtime: 11.03245 seconds\n",
      "> Epoch: 1218, Current best: 0.012687377667344943, Global best: 0.012445423007674428, Runtime: 9.43590 seconds\n",
      "> Epoch: 1219, Current best: 0.012565577213290042, Global best: 0.012445423007674428, Runtime: 8.56746 seconds\n",
      "> Epoch: 1220, Current best: 0.013053447152512835, Global best: 0.012445423007674428, Runtime: 8.61520 seconds\n",
      "> Epoch: 1221, Current best: 0.013275368674749167, Global best: 0.012445423007674428, Runtime: 8.80276 seconds\n",
      "> Epoch: 1222, Current best: 0.012746083171568582, Global best: 0.012445423007674428, Runtime: 8.91816 seconds\n",
      "> Epoch: 1223, Current best: 0.012644429516183692, Global best: 0.012445423007674428, Runtime: 8.86764 seconds\n",
      "> Epoch: 1224, Current best: 0.012828811855791577, Global best: 0.012445423007674428, Runtime: 8.54410 seconds\n",
      "> Epoch: 1225, Current best: 0.012444878429080068, Global best: 0.012444878429080068, Runtime: 8.52698 seconds\n",
      "> Epoch: 1226, Current best: 0.01293470473965608, Global best: 0.012444878429080068, Runtime: 8.66203 seconds\n",
      "> Epoch: 1227, Current best: 0.013565602037551694, Global best: 0.012444878429080068, Runtime: 12.50557 seconds\n",
      "> Epoch: 1228, Current best: 0.012764543176032877, Global best: 0.012444878429080068, Runtime: 10.19697 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1229, Current best: 0.012715214911293505, Global best: 0.012444878429080068, Runtime: 9.30406 seconds\n",
      "> Epoch: 1230, Current best: 0.012839756008715012, Global best: 0.012444878429080068, Runtime: 8.54592 seconds\n",
      "> Epoch: 1231, Current best: 0.012568115925220943, Global best: 0.012444878429080068, Runtime: 8.55938 seconds\n",
      "> Epoch: 1232, Current best: 0.012494470169384169, Global best: 0.012444878429080068, Runtime: 8.63903 seconds\n",
      "> Epoch: 1233, Current best: 0.012568926383248523, Global best: 0.012444878429080068, Runtime: 8.55138 seconds\n",
      "> Epoch: 1234, Current best: 0.012795652928770397, Global best: 0.012444878429080068, Runtime: 8.74374 seconds\n",
      "> Epoch: 1235, Current best: 0.013100951437483374, Global best: 0.012444878429080068, Runtime: 8.82750 seconds\n",
      "> Epoch: 1236, Current best: 0.012430157899919237, Global best: 0.012430157899919237, Runtime: 8.75269 seconds\n",
      "> Epoch: 1237, Current best: 0.013058966715766987, Global best: 0.012430157899919237, Runtime: 9.78079 seconds\n",
      "> Epoch: 1238, Current best: 0.012383035351180195, Global best: 0.012383035351180195, Runtime: 10.96056 seconds\n",
      "> Epoch: 1239, Current best: 0.01264536645189993, Global best: 0.012383035351180195, Runtime: 8.64304 seconds\n",
      "> Epoch: 1240, Current best: 0.012760371958526166, Global best: 0.012383035351180195, Runtime: 8.48003 seconds\n",
      "> Epoch: 1241, Current best: 0.013254784435637646, Global best: 0.012383035351180195, Runtime: 8.59723 seconds\n",
      "> Epoch: 1242, Current best: 0.01219499316317341, Global best: 0.01219499316317341, Runtime: 8.83741 seconds\n",
      "> Epoch: 1243, Current best: 0.01257932732062278, Global best: 0.01219499316317341, Runtime: 8.62314 seconds\n",
      "> Epoch: 1244, Current best: 0.01178116472204125, Global best: 0.01178116472204125, Runtime: 8.28129 seconds\n",
      "> Epoch: 1245, Current best: 0.01173381342882871, Global best: 0.01173381342882871, Runtime: 8.70303 seconds\n",
      "> Epoch: 1246, Current best: 0.012071436833663627, Global best: 0.01173381342882871, Runtime: 8.41783 seconds\n",
      "> Epoch: 1247, Current best: 0.011850449035210516, Global best: 0.01173381342882871, Runtime: 8.49059 seconds\n",
      "> Epoch: 1248, Current best: 0.011451427318315244, Global best: 0.011451427318315244, Runtime: 8.80354 seconds\n",
      "> Epoch: 1249, Current best: 0.011716135325398176, Global best: 0.011451427318315244, Runtime: 8.63809 seconds\n",
      "> Epoch: 1250, Current best: 0.01149418537648099, Global best: 0.011451427318315244, Runtime: 8.46667 seconds\n",
      "> Epoch: 1251, Current best: 0.011469941343120987, Global best: 0.011451427318315244, Runtime: 8.61517 seconds\n",
      "> Epoch: 1252, Current best: 0.01164849818558571, Global best: 0.011451427318315244, Runtime: 8.53046 seconds\n",
      "> Epoch: 1253, Current best: 0.011969193375318975, Global best: 0.011451427318315244, Runtime: 8.45371 seconds\n",
      "> Epoch: 1254, Current best: 0.011843754973048261, Global best: 0.011451427318315244, Runtime: 8.48616 seconds\n",
      "> Epoch: 1255, Current best: 0.01200059752881659, Global best: 0.011451427318315244, Runtime: 8.76467 seconds\n",
      "> Epoch: 1256, Current best: 0.011746470583919158, Global best: 0.011451427318315244, Runtime: 8.84540 seconds\n",
      "> Epoch: 1257, Current best: 0.01156188990028713, Global best: 0.011451427318315244, Runtime: 8.56036 seconds\n",
      "> Epoch: 1258, Current best: 0.011745195903941245, Global best: 0.011451427318315244, Runtime: 8.67895 seconds\n",
      "> Epoch: 1259, Current best: 0.011732966488955013, Global best: 0.011451427318315244, Runtime: 8.46766 seconds\n",
      "> Epoch: 1260, Current best: 0.01126698233619778, Global best: 0.01126698233619778, Runtime: 8.49856 seconds\n",
      "> Epoch: 1261, Current best: 0.011208809273569641, Global best: 0.011208809273569641, Runtime: 8.63211 seconds\n",
      "> Epoch: 1262, Current best: 0.010594206681763339, Global best: 0.010594206681763339, Runtime: 8.67995 seconds\n",
      "> Epoch: 1263, Current best: 0.01062058898377194, Global best: 0.010594206681763339, Runtime: 8.82447 seconds\n",
      "> Epoch: 1264, Current best: 0.01056936434155651, Global best: 0.01056936434155651, Runtime: 8.71882 seconds\n",
      "> Epoch: 1265, Current best: 0.010541562496147393, Global best: 0.010541562496147393, Runtime: 8.57231 seconds\n",
      "> Epoch: 1266, Current best: 0.010836031921552502, Global best: 0.010541562496147393, Runtime: 8.70602 seconds\n",
      "> Epoch: 1267, Current best: 0.01055263004346845, Global best: 0.010541562496147393, Runtime: 9.58211 seconds\n",
      "> Epoch: 1268, Current best: 0.01094109217333229, Global best: 0.010541562496147393, Runtime: 8.98992 seconds\n",
      "> Epoch: 1269, Current best: 0.010855521380565723, Global best: 0.010541562496147393, Runtime: 9.24239 seconds\n",
      "> Epoch: 1270, Current best: 0.010812656829741687, Global best: 0.010541562496147393, Runtime: 9.84108 seconds\n",
      "> Epoch: 1271, Current best: 0.010654940364649442, Global best: 0.010541562496147393, Runtime: 8.90531 seconds\n",
      "> Epoch: 1272, Current best: 0.010800769466245164, Global best: 0.010541562496147393, Runtime: 8.62314 seconds\n",
      "> Epoch: 1273, Current best: 0.01047280932758233, Global best: 0.01047280932758233, Runtime: 8.95707 seconds\n",
      "> Epoch: 1274, Current best: 0.01049859000788949, Global best: 0.01047280932758233, Runtime: 10.53678 seconds\n",
      "> Epoch: 1275, Current best: 0.010457994575209751, Global best: 0.010457994575209751, Runtime: 8.74474 seconds\n",
      "> Epoch: 1276, Current best: 0.010472453960061825, Global best: 0.010457994575209751, Runtime: 8.83392 seconds\n",
      "> Epoch: 1277, Current best: 0.010571544579632575, Global best: 0.010457994575209751, Runtime: 8.61486 seconds\n",
      "> Epoch: 1278, Current best: 0.010027225746890195, Global best: 0.010027225746890195, Runtime: 8.75868 seconds\n",
      "> Epoch: 1279, Current best: 0.007905214272128595, Global best: 0.007905214272128595, Runtime: 8.72620 seconds\n",
      "> Epoch: 1280, Current best: 0.007077487271480394, Global best: 0.007077487271480394, Runtime: 8.94158 seconds\n",
      "> Epoch: 1281, Current best: 0.006855845125862869, Global best: 0.006855845125862869, Runtime: 8.73078 seconds\n",
      "> Epoch: 1282, Current best: 0.006800865291300665, Global best: 0.006800865291300665, Runtime: 8.59524 seconds\n",
      "> Epoch: 1283, Current best: 0.006690430635368311, Global best: 0.006690430635368311, Runtime: 9.92877 seconds\n",
      "> Epoch: 1284, Current best: 0.00669692935072377, Global best: 0.006690430635368311, Runtime: 8.56631 seconds\n",
      "> Epoch: 1285, Current best: 0.00683014093062172, Global best: 0.006690430635368311, Runtime: 8.48661 seconds\n",
      "> Epoch: 1286, Current best: 0.006817750874081307, Global best: 0.006690430635368311, Runtime: 8.82550 seconds\n",
      "> Epoch: 1287, Current best: 0.007126683383243764, Global best: 0.006690430635368311, Runtime: 8.72580 seconds\n",
      "> Epoch: 1288, Current best: 0.0067487963162680955, Global best: 0.006690430635368311, Runtime: 8.44773 seconds\n",
      "> Epoch: 1289, Current best: 0.00662652351311604, Global best: 0.00662652351311604, Runtime: 9.47230 seconds\n",
      "> Epoch: 1290, Current best: 0.006608249748706319, Global best: 0.006608249748706319, Runtime: 9.07264 seconds\n",
      "> Epoch: 1291, Current best: 0.007203192829585351, Global best: 0.006608249748706319, Runtime: 8.51052 seconds\n",
      "> Epoch: 1292, Current best: 0.006597740516727544, Global best: 0.006597740516727544, Runtime: 8.64706 seconds\n",
      "> Epoch: 1293, Current best: 0.006446474011963216, Global best: 0.006446474011963216, Runtime: 8.66972 seconds\n",
      "> Epoch: 1294, Current best: 0.006474215104148441, Global best: 0.006446474011963216, Runtime: 8.65902 seconds\n",
      "> Epoch: 1295, Current best: 0.006424415917510193, Global best: 0.006424415917510193, Runtime: 8.59125 seconds\n",
      "> Epoch: 1296, Current best: 0.006741953285110414, Global best: 0.006424415917510193, Runtime: 8.83643 seconds\n",
      "> Epoch: 1297, Current best: 0.006309861084389905, Global best: 0.006309861084389905, Runtime: 8.68794 seconds\n",
      "> Epoch: 1298, Current best: 0.006290810586287941, Global best: 0.006290810586287941, Runtime: 8.48361 seconds\n",
      "> Epoch: 1299, Current best: 0.006319520101907591, Global best: 0.006290810586287941, Runtime: 8.90619 seconds\n",
      "> Epoch: 1300, Current best: 0.006361901387627781, Global best: 0.006290810586287941, Runtime: 8.77563 seconds\n",
      "> Epoch: 1301, Current best: 0.006936752648178348, Global best: 0.006290810586287941, Runtime: 8.97998 seconds\n",
      "> Epoch: 1302, Current best: 0.006571991879428194, Global best: 0.006290810586287941, Runtime: 9.91180 seconds\n",
      "> Epoch: 1303, Current best: 0.006373970739865839, Global best: 0.006290810586287941, Runtime: 12.02878 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1304, Current best: 0.007488321118346641, Global best: 0.006290810586287941, Runtime: 9.61084 seconds\n",
      "> Epoch: 1305, Current best: 0.006168733390840413, Global best: 0.006168733390840413, Runtime: 9.29196 seconds\n",
      "> Epoch: 1306, Current best: 0.006254930495730535, Global best: 0.006168733390840413, Runtime: 8.74773 seconds\n",
      "> Epoch: 1307, Current best: 0.006453988828929085, Global best: 0.006168733390840413, Runtime: 10.69621 seconds\n",
      "> Epoch: 1308, Current best: 0.006152030964937041, Global best: 0.006152030964937041, Runtime: 15.11442 seconds\n",
      "> Epoch: 1309, Current best: 0.006262213111772181, Global best: 0.006152030964937041, Runtime: 9.01783 seconds\n",
      "> Epoch: 1310, Current best: 0.006327879144291385, Global best: 0.006152030964937041, Runtime: 9.54312 seconds\n",
      "> Epoch: 1311, Current best: 0.006338547058201544, Global best: 0.006152030964937041, Runtime: 10.70218 seconds\n",
      "> Epoch: 1312, Current best: 0.006568613798903448, Global best: 0.006152030964937041, Runtime: 8.72381 seconds\n",
      "> Epoch: 1313, Current best: 0.006120058493186299, Global best: 0.006120058493186299, Runtime: 9.34672 seconds\n",
      "> Epoch: 1314, Current best: 0.006078458721615078, Global best: 0.006078458721615078, Runtime: 11.37633 seconds\n",
      "> Epoch: 1315, Current best: 0.006429837887979618, Global best: 0.006078458721615078, Runtime: 12.85671 seconds\n",
      "> Epoch: 1316, Current best: 0.0059576041978766136, Global best: 0.0059576041978766136, Runtime: 8.82746 seconds\n",
      "> Epoch: 1317, Current best: 0.005968515270353749, Global best: 0.0059576041978766136, Runtime: 9.12933 seconds\n",
      "> Epoch: 1318, Current best: 0.0060913460865203144, Global best: 0.0059576041978766136, Runtime: 8.85638 seconds\n",
      "> Epoch: 1319, Current best: 0.006215314135022061, Global best: 0.0059576041978766136, Runtime: 8.77467 seconds\n",
      "> Epoch: 1320, Current best: 0.005986376049142287, Global best: 0.0059576041978766136, Runtime: 8.66600 seconds\n",
      "> Epoch: 1321, Current best: 0.006023305601491676, Global best: 0.0059576041978766136, Runtime: 10.00704 seconds\n",
      "> Epoch: 1322, Current best: 0.0059403629562248235, Global best: 0.0059403629562248235, Runtime: 11.47126 seconds\n",
      "> Epoch: 1323, Current best: 0.006001709606237924, Global best: 0.0059403629562248235, Runtime: 10.73652 seconds\n",
      "> Epoch: 1324, Current best: 0.0059322897091851215, Global best: 0.0059322897091851215, Runtime: 11.30053 seconds\n",
      "> Epoch: 1325, Current best: 0.006104891834654004, Global best: 0.0059322897091851215, Runtime: 10.49162 seconds\n",
      "> Epoch: 1326, Current best: 0.005941580983808585, Global best: 0.0059322897091851215, Runtime: 10.54073 seconds\n",
      "> Epoch: 1327, Current best: 0.005864474255749435, Global best: 0.005864474255749435, Runtime: 9.67064 seconds\n",
      "> Epoch: 1328, Current best: 0.0062099351551266205, Global best: 0.005864474255749435, Runtime: 9.07563 seconds\n",
      "> Epoch: 1329, Current best: 0.0061378774799030434, Global best: 0.005864474255749435, Runtime: 9.29806 seconds\n",
      "> Epoch: 1330, Current best: 0.005994194669573048, Global best: 0.005864474255749435, Runtime: 9.17036 seconds\n",
      "> Epoch: 1331, Current best: 0.006053094005833153, Global best: 0.005864474255749435, Runtime: 9.00521 seconds\n",
      "> Epoch: 1332, Current best: 0.006481969547588971, Global best: 0.005864474255749435, Runtime: 8.74773 seconds\n",
      "> Epoch: 1333, Current best: 0.006263600602833887, Global best: 0.005864474255749435, Runtime: 9.01203 seconds\n",
      "> Epoch: 1334, Current best: 0.005904412600446593, Global best: 0.005864474255749435, Runtime: 9.12943 seconds\n",
      "> Epoch: 1335, Current best: 0.006118447406036283, Global best: 0.005864474255749435, Runtime: 8.60435 seconds\n",
      "> Epoch: 1336, Current best: 0.006200527767149201, Global best: 0.005864474255749435, Runtime: 8.89145 seconds\n",
      "> Epoch: 1337, Current best: 0.005903624185566722, Global best: 0.005864474255749435, Runtime: 8.65902 seconds\n",
      "> Epoch: 1338, Current best: 0.005922916330843918, Global best: 0.005864474255749435, Runtime: 8.87485 seconds\n",
      "> Epoch: 1339, Current best: 0.005937472843154089, Global best: 0.005864474255749435, Runtime: 8.79425 seconds\n",
      "> Epoch: 1340, Current best: 0.00583662946084373, Global best: 0.00583662946084373, Runtime: 9.01683 seconds\n",
      "> Epoch: 1341, Current best: 0.006379295302869399, Global best: 0.00583662946084373, Runtime: 8.82347 seconds\n",
      "> Epoch: 1342, Current best: 0.005646494604290624, Global best: 0.005646494604290624, Runtime: 8.64308 seconds\n",
      "> Epoch: 1343, Current best: 0.005727994839388348, Global best: 0.005646494604290624, Runtime: 11.41989 seconds\n",
      "> Epoch: 1344, Current best: 0.005887973217142188, Global best: 0.005646494604290624, Runtime: 10.89159 seconds\n",
      "> Epoch: 1345, Current best: 0.0058324016803461935, Global best: 0.005646494604290624, Runtime: 9.00334 seconds\n",
      "> Epoch: 1346, Current best: 0.006167761669376254, Global best: 0.005646494604290624, Runtime: 9.02779 seconds\n",
      "> Epoch: 1347, Current best: 0.00602932496273731, Global best: 0.005646494604290624, Runtime: 8.90520 seconds\n",
      "> Epoch: 1348, Current best: 0.0059239340739579115, Global best: 0.005646494604290624, Runtime: 8.58437 seconds\n",
      "> Epoch: 1349, Current best: 0.005745830118179689, Global best: 0.005646494604290624, Runtime: 8.82951 seconds\n",
      "> Epoch: 1350, Current best: 0.005652406760475939, Global best: 0.005646494604290624, Runtime: 9.54802 seconds\n",
      "> Epoch: 1351, Current best: 0.005814701164390938, Global best: 0.005646494604290624, Runtime: 9.32833 seconds\n",
      "> Epoch: 1352, Current best: 0.005749886238926641, Global best: 0.005646494604290624, Runtime: 9.43549 seconds\n",
      "> Epoch: 1353, Current best: 0.005722930874677267, Global best: 0.005646494604290624, Runtime: 9.38858 seconds\n",
      "> Epoch: 1354, Current best: 0.005626248956831093, Global best: 0.005626248956831093, Runtime: 8.70891 seconds\n",
      "> Epoch: 1355, Current best: 0.006054934698057649, Global best: 0.005626248956831093, Runtime: 9.25217 seconds\n",
      "> Epoch: 1356, Current best: 0.005681481399289655, Global best: 0.005626248956831093, Runtime: 9.08710 seconds\n",
      "> Epoch: 1357, Current best: 0.005668754511875001, Global best: 0.005626248956831093, Runtime: 8.98030 seconds\n",
      "> Epoch: 1358, Current best: 0.005755869928125449, Global best: 0.005626248956831093, Runtime: 8.89723 seconds\n",
      "> Epoch: 1359, Current best: 0.005735300233548823, Global best: 0.005626248956831093, Runtime: 9.02202 seconds\n",
      "> Epoch: 1360, Current best: 0.005719142353262377, Global best: 0.005626248956831093, Runtime: 8.94702 seconds\n",
      "> Epoch: 1361, Current best: 0.005612772075918681, Global best: 0.005612772075918681, Runtime: 8.95604 seconds\n",
      "> Epoch: 1362, Current best: 0.005622682163948842, Global best: 0.005612772075918681, Runtime: 8.75670 seconds\n",
      "> Epoch: 1363, Current best: 0.005680564509559878, Global best: 0.005612772075918681, Runtime: 9.00492 seconds\n",
      "> Epoch: 1364, Current best: 0.005722321103593016, Global best: 0.005612772075918681, Runtime: 8.67298 seconds\n",
      "> Epoch: 1365, Current best: 0.005703830031320539, Global best: 0.005612772075918681, Runtime: 8.84488 seconds\n",
      "> Epoch: 1366, Current best: 0.005623823834910526, Global best: 0.005612772075918681, Runtime: 9.36307 seconds\n",
      "> Epoch: 1367, Current best: 0.005598823581611375, Global best: 0.005598823581611375, Runtime: 8.93914 seconds\n",
      "> Epoch: 1368, Current best: 0.005643341673342257, Global best: 0.005598823581611375, Runtime: 9.00407 seconds\n",
      "> Epoch: 1369, Current best: 0.005813621988646506, Global best: 0.005598823581611375, Runtime: 8.85837 seconds\n",
      "> Epoch: 1370, Current best: 0.005612861125642049, Global best: 0.005598823581611375, Runtime: 8.83543 seconds\n",
      "> Epoch: 1371, Current best: 0.005732981283650441, Global best: 0.005598823581611375, Runtime: 9.03590 seconds\n",
      "> Epoch: 1372, Current best: 0.005646722300345128, Global best: 0.005598823581611375, Runtime: 9.23633 seconds\n",
      "> Epoch: 1373, Current best: 0.005729192287976242, Global best: 0.005598823581611375, Runtime: 9.85526 seconds\n",
      "> Epoch: 1374, Current best: 0.0057594317781711, Global best: 0.005598823581611375, Runtime: 10.96039 seconds\n",
      "> Epoch: 1375, Current best: 0.005599143022850742, Global best: 0.005598823581611375, Runtime: 9.58194 seconds\n",
      "> Epoch: 1376, Current best: 0.005986027905946754, Global best: 0.005598823581611375, Runtime: 11.12083 seconds\n",
      "> Epoch: 1377, Current best: 0.005811691611236988, Global best: 0.005598823581611375, Runtime: 10.72230 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1378, Current best: 0.005622061799469015, Global best: 0.005598823581611375, Runtime: 11.99662 seconds\n",
      "> Epoch: 1379, Current best: 0.005603004196701298, Global best: 0.005598823581611375, Runtime: 11.50379 seconds\n",
      "> Epoch: 1380, Current best: 0.005901921653444242, Global best: 0.005598823581611375, Runtime: 10.69831 seconds\n",
      "> Epoch: 1381, Current best: 0.005806929866639628, Global best: 0.005598823581611375, Runtime: 10.98589 seconds\n",
      "> Epoch: 1382, Current best: 0.005598759417316483, Global best: 0.005598759417316483, Runtime: 9.89338 seconds\n",
      "> Epoch: 1383, Current best: 0.005903024928138195, Global best: 0.005598759417316483, Runtime: 9.77641 seconds\n",
      "> Epoch: 1384, Current best: 0.005709454516677626, Global best: 0.005598759417316483, Runtime: 11.06628 seconds\n",
      "> Epoch: 1385, Current best: 0.005689029586385361, Global best: 0.005598759417316483, Runtime: 11.76279 seconds\n",
      "> Epoch: 1386, Current best: 0.005614854442723394, Global best: 0.005598759417316483, Runtime: 11.05344 seconds\n",
      "> Epoch: 1387, Current best: 0.005656835309650202, Global best: 0.005598759417316483, Runtime: 12.02492 seconds\n",
      "> Epoch: 1388, Current best: 0.00565472644734568, Global best: 0.005598759417316483, Runtime: 12.00998 seconds\n",
      "> Epoch: 1389, Current best: 0.005738628042898888, Global best: 0.005598759417316483, Runtime: 11.68682 seconds\n",
      "> Epoch: 1390, Current best: 0.0058936842868775215, Global best: 0.005598759417316483, Runtime: 10.41233 seconds\n",
      "> Epoch: 1391, Current best: 0.005685734607058441, Global best: 0.005598759417316483, Runtime: 10.69835 seconds\n",
      "> Epoch: 1392, Current best: 0.005630110307277414, Global best: 0.005598759417316483, Runtime: 11.50773 seconds\n",
      "> Epoch: 1393, Current best: 0.00564410909144088, Global best: 0.005598759417316483, Runtime: 11.61620 seconds\n",
      "> Epoch: 1394, Current best: 0.005660550459878107, Global best: 0.005598759417316483, Runtime: 12.93319 seconds\n",
      "> Epoch: 1395, Current best: 0.005628421216331786, Global best: 0.005598759417316483, Runtime: 11.30158 seconds\n",
      "> Epoch: 1396, Current best: 0.005613403423973073, Global best: 0.005598759417316483, Runtime: 11.00816 seconds\n",
      "> Epoch: 1397, Current best: 0.005603562674759733, Global best: 0.005598759417316483, Runtime: 11.90635 seconds\n",
      "> Epoch: 1398, Current best: 0.005649604857424182, Global best: 0.005598759417316483, Runtime: 9.49754 seconds\n",
      "> Epoch: 1399, Current best: 0.005600467299222292, Global best: 0.005598759417316483, Runtime: 9.26966 seconds\n",
      "> Epoch: 1400, Current best: 0.005624293779690307, Global best: 0.005598759417316483, Runtime: 9.87117 seconds\n",
      "> Epoch: 1401, Current best: 0.005636374473707263, Global best: 0.005598759417316483, Runtime: 9.41748 seconds\n",
      "> Epoch: 1402, Current best: 0.005557538366773734, Global best: 0.005557538366773734, Runtime: 9.40774 seconds\n",
      "> Epoch: 1403, Current best: 0.005477181944851502, Global best: 0.005477181944851502, Runtime: 9.57356 seconds\n",
      "> Epoch: 1404, Current best: 0.005524589019755744, Global best: 0.005477181944851502, Runtime: 9.61180 seconds\n",
      "> Epoch: 1405, Current best: 0.0056309831946634355, Global best: 0.005477181944851502, Runtime: 9.53608 seconds\n",
      "> Epoch: 1406, Current best: 0.0055209868847777895, Global best: 0.005477181944851502, Runtime: 9.88045 seconds\n",
      "> Epoch: 1407, Current best: 0.005682421719496179, Global best: 0.005477181944851502, Runtime: 9.99115 seconds\n",
      "> Epoch: 1408, Current best: 0.0059111361844674404, Global best: 0.005477181944851502, Runtime: 11.35901 seconds\n",
      "> Epoch: 1409, Current best: 0.005617391961602209, Global best: 0.005477181944851502, Runtime: 10.56772 seconds\n",
      "> Epoch: 1410, Current best: 0.005601018142070108, Global best: 0.005477181944851502, Runtime: 10.31579 seconds\n",
      "> Epoch: 1411, Current best: 0.005540151149426579, Global best: 0.005477181944851502, Runtime: 9.28703 seconds\n",
      "> Epoch: 1412, Current best: 0.005526284222117897, Global best: 0.005477181944851502, Runtime: 9.23521 seconds\n",
      "> Epoch: 1413, Current best: 0.005553641988901557, Global best: 0.005477181944851502, Runtime: 9.14081 seconds\n",
      "> Epoch: 1414, Current best: 0.005661579390204942, Global best: 0.005477181944851502, Runtime: 8.93421 seconds\n",
      "> Epoch: 1415, Current best: 0.005565470438076, Global best: 0.005477181944851502, Runtime: 8.89234 seconds\n",
      "> Epoch: 1416, Current best: 0.005515614632479731, Global best: 0.005477181944851502, Runtime: 8.90422 seconds\n",
      "> Epoch: 1417, Current best: 0.005585501133451356, Global best: 0.005477181944851502, Runtime: 8.88523 seconds\n",
      "> Epoch: 1418, Current best: 0.005577869582344626, Global best: 0.005477181944851502, Runtime: 8.69590 seconds\n",
      "> Epoch: 1419, Current best: 0.0054858138696711875, Global best: 0.005477181944851502, Runtime: 9.35098 seconds\n",
      "> Epoch: 1420, Current best: 0.005527900981520051, Global best: 0.005477181944851502, Runtime: 8.81988 seconds\n",
      "> Epoch: 1421, Current best: 0.005483336708330986, Global best: 0.005477181944851502, Runtime: 9.02971 seconds\n",
      "> Epoch: 1422, Current best: 0.005486014007732641, Global best: 0.005477181944851502, Runtime: 8.90121 seconds\n",
      "> Epoch: 1423, Current best: 0.005464519165333947, Global best: 0.005464519165333947, Runtime: 9.09557 seconds\n",
      "> Epoch: 1424, Current best: 0.00525624684502466, Global best: 0.00525624684502466, Runtime: 9.07363 seconds\n",
      "> Epoch: 1425, Current best: 0.005289840863922985, Global best: 0.00525624684502466, Runtime: 9.22839 seconds\n",
      "> Epoch: 1426, Current best: 0.00523383581550089, Global best: 0.00523383581550089, Runtime: 9.16969 seconds\n",
      "> Epoch: 1427, Current best: 0.005278021256567221, Global best: 0.00523383581550089, Runtime: 8.75222 seconds\n",
      "> Epoch: 1428, Current best: 0.005355854107803037, Global best: 0.00523383581550089, Runtime: 8.88925 seconds\n",
      "> Epoch: 1429, Current best: 0.005267180867749768, Global best: 0.00523383581550089, Runtime: 9.01699 seconds\n",
      "> Epoch: 1430, Current best: 0.0052058088161701915, Global best: 0.0052058088161701915, Runtime: 8.92114 seconds\n",
      "> Epoch: 1431, Current best: 0.005259115749067149, Global best: 0.0052058088161701915, Runtime: 9.00885 seconds\n",
      "> Epoch: 1432, Current best: 0.0051111073739958665, Global best: 0.0051111073739958665, Runtime: 9.46233 seconds\n",
      "> Epoch: 1433, Current best: 0.005092794897749671, Global best: 0.005092794897749671, Runtime: 8.81151 seconds\n",
      "> Epoch: 1434, Current best: 0.005260066965215613, Global best: 0.005092794897749671, Runtime: 9.09756 seconds\n",
      "> Epoch: 1435, Current best: 0.005134678857600488, Global best: 0.005092794897749671, Runtime: 9.11251 seconds\n",
      "> Epoch: 1436, Current best: 0.005171531812649633, Global best: 0.005092794897749671, Runtime: 9.33078 seconds\n",
      "> Epoch: 1437, Current best: 0.005103721342478213, Global best: 0.005092794897749671, Runtime: 9.94472 seconds\n",
      "> Epoch: 1438, Current best: 0.005120092602468772, Global best: 0.005092794897749671, Runtime: 9.43144 seconds\n",
      "> Epoch: 1439, Current best: 0.005088649519791404, Global best: 0.005088649519791404, Runtime: 9.30885 seconds\n",
      "> Epoch: 1440, Current best: 0.005132809171949537, Global best: 0.005088649519791404, Runtime: 9.54006 seconds\n",
      "> Epoch: 1441, Current best: 0.005128820144976181, Global best: 0.005088649519791404, Runtime: 9.05570 seconds\n",
      "> Epoch: 1442, Current best: 0.005105818910022077, Global best: 0.005088649519791404, Runtime: 8.87442 seconds\n",
      "> Epoch: 1443, Current best: 0.005119676652312277, Global best: 0.005088649519791404, Runtime: 9.91684 seconds\n",
      "> Epoch: 1444, Current best: 0.005122062797844533, Global best: 0.005088649519791404, Runtime: 10.38065 seconds\n",
      "> Epoch: 1445, Current best: 0.005062573139417712, Global best: 0.005062573139417712, Runtime: 10.97824 seconds\n",
      "> Epoch: 1446, Current best: 0.005044446355701016, Global best: 0.005044446355701016, Runtime: 8.98394 seconds\n",
      "> Epoch: 1447, Current best: 0.005116270833091744, Global best: 0.005044446355701016, Runtime: 10.42810 seconds\n",
      "> Epoch: 1448, Current best: 0.005018219098515513, Global best: 0.005018219098515513, Runtime: 9.77130 seconds\n",
      "> Epoch: 1449, Current best: 0.005161619594222839, Global best: 0.005018219098515513, Runtime: 9.84033 seconds\n",
      "> Epoch: 1450, Current best: 0.005146017298410334, Global best: 0.005018219098515513, Runtime: 9.31604 seconds\n",
      "> Epoch: 1451, Current best: 0.004991032609522652, Global best: 0.004991032609522652, Runtime: 9.68160 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1452, Current best: 0.0051680893527676856, Global best: 0.004991032609522652, Runtime: 9.70683 seconds\n",
      "> Epoch: 1453, Current best: 0.004935506593091028, Global best: 0.004935506593091028, Runtime: 9.73056 seconds\n",
      "> Epoch: 1454, Current best: 0.004895872817163608, Global best: 0.004895872817163608, Runtime: 9.86725 seconds\n",
      "> Epoch: 1455, Current best: 0.00489144931391181, Global best: 0.00489144931391181, Runtime: 9.95224 seconds\n",
      "> Epoch: 1456, Current best: 0.004869483638351058, Global best: 0.004869483638351058, Runtime: 9.91094 seconds\n",
      "> Epoch: 1457, Current best: 0.004701131405267677, Global best: 0.004701131405267677, Runtime: 9.90384 seconds\n",
      "> Epoch: 1458, Current best: 0.004711475240485711, Global best: 0.004701131405267677, Runtime: 9.59688 seconds\n",
      "> Epoch: 1459, Current best: 0.0047186862822956675, Global best: 0.004701131405267677, Runtime: 11.99668 seconds\n",
      "> Epoch: 1460, Current best: 0.004694836842095513, Global best: 0.004694836842095513, Runtime: 9.85807 seconds\n",
      "> Epoch: 1461, Current best: 0.004700684254151263, Global best: 0.004694836842095513, Runtime: 9.74126 seconds\n",
      "> Epoch: 1462, Current best: 0.004729629154263099, Global best: 0.004694836842095513, Runtime: 9.84724 seconds\n",
      "> Epoch: 1463, Current best: 0.00473732099741368, Global best: 0.004694836842095513, Runtime: 10.06832 seconds\n",
      "> Epoch: 1464, Current best: 0.004716719998412727, Global best: 0.004694836842095513, Runtime: 9.16433 seconds\n",
      "> Epoch: 1465, Current best: 0.00470367368805733, Global best: 0.004694836842095513, Runtime: 9.17036 seconds\n",
      "> Epoch: 1466, Current best: 0.004727394079128661, Global best: 0.004694836842095513, Runtime: 9.31397 seconds\n",
      "> Epoch: 1467, Current best: 0.0047431856036175505, Global best: 0.004694836842095513, Runtime: 13.42192 seconds\n",
      "> Epoch: 1468, Current best: 0.004778505300708148, Global best: 0.004694836842095513, Runtime: 10.71717 seconds\n",
      "> Epoch: 1469, Current best: 0.004711306394549941, Global best: 0.004694836842095513, Runtime: 9.86994 seconds\n",
      "> Epoch: 1470, Current best: 0.004803981195278174, Global best: 0.004694836842095513, Runtime: 10.45800 seconds\n",
      "> Epoch: 1471, Current best: 0.004733094802677736, Global best: 0.004694836842095513, Runtime: 10.78273 seconds\n",
      "> Epoch: 1472, Current best: 0.004744593356322416, Global best: 0.004694836842095513, Runtime: 10.16797 seconds\n",
      "> Epoch: 1473, Current best: 0.004764628762023043, Global best: 0.004694836842095513, Runtime: 9.70294 seconds\n",
      "> Epoch: 1474, Current best: 0.004679149341945751, Global best: 0.004679149341945751, Runtime: 9.60546 seconds\n",
      "> Epoch: 1475, Current best: 0.004705745822314101, Global best: 0.004679149341945751, Runtime: 10.26873 seconds\n",
      "> Epoch: 1476, Current best: 0.00471050684182283, Global best: 0.004679149341945751, Runtime: 8.99392 seconds\n",
      "> Epoch: 1477, Current best: 0.004690480450658399, Global best: 0.004679149341945751, Runtime: 8.96585 seconds\n",
      "> Epoch: 1478, Current best: 0.004678087976007449, Global best: 0.004678087976007449, Runtime: 8.87547 seconds\n",
      "> Epoch: 1479, Current best: 0.004675420412192514, Global best: 0.004675420412192514, Runtime: 8.78266 seconds\n",
      "> Epoch: 1480, Current best: 0.004684816375091494, Global best: 0.004675420412192514, Runtime: 9.08500 seconds\n",
      "> Epoch: 1481, Current best: 0.004671331323970827, Global best: 0.004671331323970827, Runtime: 9.26402 seconds\n",
      "> Epoch: 1482, Current best: 0.004745710915543676, Global best: 0.004671331323970827, Runtime: 10.59072 seconds\n",
      "> Epoch: 1483, Current best: 0.004682315755828099, Global best: 0.004671331323970827, Runtime: 8.94318 seconds\n",
      "> Epoch: 1484, Current best: 0.004686282548525131, Global best: 0.004671331323970827, Runtime: 8.98983 seconds\n",
      "> Epoch: 1485, Current best: 0.004702539111667259, Global best: 0.004671331323970827, Runtime: 11.96902 seconds\n",
      "> Epoch: 1486, Current best: 0.004693052366132827, Global best: 0.004671331323970827, Runtime: 10.36737 seconds\n",
      "> Epoch: 1487, Current best: 0.004679973462831619, Global best: 0.004671331323970827, Runtime: 10.99413 seconds\n",
      "> Epoch: 1488, Current best: 0.004713269065381882, Global best: 0.004671331323970827, Runtime: 10.55563 seconds\n",
      "> Epoch: 1489, Current best: 0.004720075034811261, Global best: 0.004671331323970827, Runtime: 9.64975 seconds\n",
      "> Epoch: 1490, Current best: 0.004689205868932646, Global best: 0.004671331323970827, Runtime: 11.10725 seconds\n",
      "> Epoch: 1491, Current best: 0.004673147703150074, Global best: 0.004671331323970827, Runtime: 11.74597 seconds\n",
      "> Epoch: 1492, Current best: 0.004686344632154769, Global best: 0.004671331323970827, Runtime: 9.96390 seconds\n",
      "> Epoch: 1493, Current best: 0.004688003805151541, Global best: 0.004671331323970827, Runtime: 10.03341 seconds\n",
      "> Epoch: 1494, Current best: 0.004715041589768216, Global best: 0.004671331323970827, Runtime: 9.51137 seconds\n",
      "> Epoch: 1495, Current best: 0.004702856952695885, Global best: 0.004671331323970827, Runtime: 9.31948 seconds\n",
      "> Epoch: 1496, Current best: 0.004717979555257174, Global best: 0.004671331323970827, Runtime: 9.43557 seconds\n",
      "> Epoch: 1497, Current best: 0.004673476403971706, Global best: 0.004671331323970827, Runtime: 13.17206 seconds\n",
      "> Epoch: 1498, Current best: 0.0046803267893198094, Global best: 0.004671331323970827, Runtime: 10.83375 seconds\n",
      "> Epoch: 1499, Current best: 0.00467288890459487, Global best: 0.004671331323970827, Runtime: 10.71877 seconds\n",
      "> Epoch: 1500, Current best: 0.004659344660504336, Global best: 0.004659344660504336, Runtime: 9.99996 seconds\n",
      "> Epoch: 1501, Current best: 0.004665374884277142, Global best: 0.004659344660504336, Runtime: 11.77599 seconds\n",
      "> Epoch: 1502, Current best: 0.004655483798332861, Global best: 0.004655483798332861, Runtime: 10.12214 seconds\n",
      "> Epoch: 1503, Current best: 0.004668888757893152, Global best: 0.004655483798332861, Runtime: 9.48416 seconds\n",
      "> Epoch: 1504, Current best: 0.004664920408118476, Global best: 0.004655483798332861, Runtime: 10.75352 seconds\n",
      "> Epoch: 1505, Current best: 0.004658033130804224, Global best: 0.004655483798332861, Runtime: 12.20779 seconds\n",
      "> Epoch: 1506, Current best: 0.004717918453483827, Global best: 0.004655483798332861, Runtime: 12.01980 seconds\n",
      "> Epoch: 1507, Current best: 0.004692266792568353, Global best: 0.004655483798332861, Runtime: 9.86626 seconds\n",
      "> Epoch: 1508, Current best: 0.004652271362058821, Global best: 0.004652271362058821, Runtime: 11.43251 seconds\n",
      "> Epoch: 1509, Current best: 0.004645014086883917, Global best: 0.004645014086883917, Runtime: 9.73495 seconds\n",
      "> Epoch: 1510, Current best: 0.004714727358047959, Global best: 0.004645014086883917, Runtime: 10.39232 seconds\n",
      "> Epoch: 1511, Current best: 0.004626118738747339, Global best: 0.004626118738747339, Runtime: 9.41542 seconds\n",
      "> Epoch: 1512, Current best: 0.004634587618224503, Global best: 0.004626118738747339, Runtime: 9.43085 seconds\n",
      "> Epoch: 1513, Current best: 0.004844911837388894, Global best: 0.004626118738747339, Runtime: 9.58719 seconds\n",
      "> Epoch: 1514, Current best: 0.004643509929465309, Global best: 0.004626118738747339, Runtime: 12.00976 seconds\n",
      "> Epoch: 1515, Current best: 0.004629019844806702, Global best: 0.004626118738747339, Runtime: 9.41047 seconds\n",
      "> Epoch: 1516, Current best: 0.004670699289227839, Global best: 0.004626118738747339, Runtime: 9.36976 seconds\n",
      "> Epoch: 1517, Current best: 0.004643546619083816, Global best: 0.004626118738747339, Runtime: 9.07172 seconds\n",
      "> Epoch: 1518, Current best: 0.00462749268254202, Global best: 0.004626118738747339, Runtime: 9.10448 seconds\n",
      "> Epoch: 1519, Current best: 0.004628158882679161, Global best: 0.004626118738747339, Runtime: 9.30888 seconds\n",
      "> Epoch: 1520, Current best: 0.004639246144702066, Global best: 0.004626118738747339, Runtime: 9.06164 seconds\n",
      "> Epoch: 1521, Current best: 0.0046590475839094455, Global best: 0.004626118738747339, Runtime: 9.12051 seconds\n",
      "> Epoch: 1522, Current best: 0.004633254391455457, Global best: 0.004626118738747339, Runtime: 9.81449 seconds\n",
      "> Epoch: 1523, Current best: 0.004645921953247998, Global best: 0.004626118738747339, Runtime: 9.03970 seconds\n",
      "> Epoch: 1524, Current best: 0.004637289413764939, Global best: 0.004626118738747339, Runtime: 9.07181 seconds\n",
      "> Epoch: 1525, Current best: 0.0046440132128398965, Global best: 0.004626118738747339, Runtime: 9.60553 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1526, Current best: 0.004637961143152523, Global best: 0.004626118738747339, Runtime: 12.22116 seconds\n",
      "> Epoch: 1527, Current best: 0.004650016509141833, Global best: 0.004626118738747339, Runtime: 9.80031 seconds\n",
      "> Epoch: 1528, Current best: 0.004589126229725969, Global best: 0.004589126229725969, Runtime: 10.02281 seconds\n",
      "> Epoch: 1529, Current best: 0.0042170139126640254, Global best: 0.0042170139126640254, Runtime: 10.18819 seconds\n",
      "> Epoch: 1530, Current best: 0.0041967773864137175, Global best: 0.0041967773864137175, Runtime: 11.04925 seconds\n",
      "> Epoch: 1531, Current best: 0.004161502865831994, Global best: 0.004161502865831994, Runtime: 10.90726 seconds\n",
      "> Epoch: 1532, Current best: 0.004175386480081458, Global best: 0.004161502865831994, Runtime: 11.58717 seconds\n",
      "> Epoch: 1533, Current best: 0.004193328735918298, Global best: 0.004161502865831994, Runtime: 12.62445 seconds\n",
      "> Epoch: 1534, Current best: 0.004165962320575146, Global best: 0.004161502865831994, Runtime: 12.09365 seconds\n",
      "> Epoch: 1535, Current best: 0.004173107190646615, Global best: 0.004161502865831994, Runtime: 11.55908 seconds\n",
      "> Epoch: 1536, Current best: 0.00415122149511324, Global best: 0.00415122149511324, Runtime: 11.01388 seconds\n",
      "> Epoch: 1537, Current best: 0.004142994070153296, Global best: 0.004142994070153296, Runtime: 10.85320 seconds\n",
      "> Epoch: 1538, Current best: 0.0041483708202213355, Global best: 0.004142994070153296, Runtime: 10.08308 seconds\n",
      "> Epoch: 1539, Current best: 0.004123179226286217, Global best: 0.004123179226286217, Runtime: 11.44773 seconds\n",
      "> Epoch: 1540, Current best: 0.004115706737581208, Global best: 0.004115706737581208, Runtime: 11.38132 seconds\n",
      "> Epoch: 1541, Current best: 0.004133746535215337, Global best: 0.004115706737581208, Runtime: 11.84839 seconds\n",
      "> Epoch: 1542, Current best: 0.004110767206881, Global best: 0.004110767206881, Runtime: 11.14671 seconds\n",
      "> Epoch: 1543, Current best: 0.004128228801490638, Global best: 0.004110767206881, Runtime: 10.29768 seconds\n",
      "> Epoch: 1544, Current best: 0.004113007549846481, Global best: 0.004110767206881, Runtime: 9.49945 seconds\n",
      "> Epoch: 1545, Current best: 0.004113106040294902, Global best: 0.004110767206881, Runtime: 10.00589 seconds\n",
      "> Epoch: 1546, Current best: 0.0041058292262943375, Global best: 0.0041058292262943375, Runtime: 9.12476 seconds\n",
      "> Epoch: 1547, Current best: 0.004075649505235235, Global best: 0.004075649505235235, Runtime: 8.95802 seconds\n",
      "> Epoch: 1548, Current best: 0.004044845705717545, Global best: 0.004044845705717545, Runtime: 10.13309 seconds\n",
      "> Epoch: 1549, Current best: 0.003849639211458918, Global best: 0.003849639211458918, Runtime: 12.66661 seconds\n",
      "> Epoch: 1550, Current best: 0.003345676382434347, Global best: 0.003345676382434347, Runtime: 15.18409 seconds\n",
      "> Epoch: 1551, Current best: 0.003290923054918435, Global best: 0.003290923054918435, Runtime: 9.91613 seconds\n",
      "> Epoch: 1552, Current best: 0.0033049672363299537, Global best: 0.003290923054918435, Runtime: 10.09542 seconds\n",
      "> Epoch: 1553, Current best: 0.0032962697588109257, Global best: 0.003290923054918435, Runtime: 10.62038 seconds\n",
      "> Epoch: 1554, Current best: 0.0032597197787067843, Global best: 0.0032597197787067843, Runtime: 11.68001 seconds\n",
      "> Epoch: 1555, Current best: 0.003226606403970058, Global best: 0.003226606403970058, Runtime: 10.24928 seconds\n",
      "> Epoch: 1556, Current best: 0.003161311108756242, Global best: 0.003161311108756242, Runtime: 10.18691 seconds\n",
      "> Epoch: 1557, Current best: 0.0031528251350630573, Global best: 0.0031528251350630573, Runtime: 10.37159 seconds\n",
      "> Epoch: 1558, Current best: 0.00311538121497774, Global best: 0.00311538121497774, Runtime: 9.36568 seconds\n",
      "> Epoch: 1559, Current best: 0.0031092426254926532, Global best: 0.0031092426254926532, Runtime: 9.29161 seconds\n",
      "> Epoch: 1560, Current best: 0.003127056330450385, Global best: 0.0031092426254926532, Runtime: 9.20493 seconds\n",
      "> Epoch: 1561, Current best: 0.0031069216365809523, Global best: 0.0031069216365809523, Runtime: 9.41410 seconds\n",
      "> Epoch: 1562, Current best: 0.00309863245736961, Global best: 0.00309863245736961, Runtime: 9.55004 seconds\n",
      "> Epoch: 1563, Current best: 0.003132058434800717, Global best: 0.00309863245736961, Runtime: 9.08958 seconds\n",
      "> Epoch: 1564, Current best: 0.0030833993061838726, Global best: 0.0030833993061838726, Runtime: 9.08460 seconds\n",
      "> Epoch: 1565, Current best: 0.0030766197040234667, Global best: 0.0030766197040234667, Runtime: 9.00486 seconds\n",
      "> Epoch: 1566, Current best: 0.0031316174236382544, Global best: 0.0030766197040234667, Runtime: 9.46834 seconds\n",
      "> Epoch: 1567, Current best: 0.003085114899772592, Global best: 0.0030766197040234667, Runtime: 10.18891 seconds\n",
      "> Epoch: 1568, Current best: 0.003085809038943301, Global best: 0.0030766197040234667, Runtime: 11.88718 seconds\n",
      "> Epoch: 1569, Current best: 0.0030799012645609477, Global best: 0.0030766197040234667, Runtime: 11.10585 seconds\n",
      "> Epoch: 1570, Current best: 0.0030701646554103816, Global best: 0.0030701646554103816, Runtime: 11.19653 seconds\n",
      "> Epoch: 1571, Current best: 0.0030246322692752273, Global best: 0.0030246322692752273, Runtime: 9.95672 seconds\n",
      "> Epoch: 1572, Current best: 0.0030314516107063302, Global best: 0.0030246322692752273, Runtime: 10.60851 seconds\n",
      "> Epoch: 1573, Current best: 0.003030577416118447, Global best: 0.0030246322692752273, Runtime: 12.61867 seconds\n",
      "> Epoch: 1574, Current best: 0.003016760533991609, Global best: 0.003016760533991609, Runtime: 10.63802 seconds\n",
      "> Epoch: 1575, Current best: 0.0030186923504826667, Global best: 0.003016760533991609, Runtime: 11.40892 seconds\n",
      "> Epoch: 1576, Current best: 0.003006879311729072, Global best: 0.003006879311729072, Runtime: 11.55135 seconds\n",
      "> Epoch: 1577, Current best: 0.0030043465079683403, Global best: 0.0030043465079683403, Runtime: 12.09768 seconds\n",
      "> Epoch: 1578, Current best: 0.0030258808462026838, Global best: 0.0030043465079683403, Runtime: 10.80836 seconds\n",
      "> Epoch: 1579, Current best: 0.0030111738785235736, Global best: 0.0030043465079683403, Runtime: 11.89802 seconds\n",
      "> Epoch: 1580, Current best: 0.0029945178897519084, Global best: 0.0029945178897519084, Runtime: 9.74043 seconds\n",
      "> Epoch: 1581, Current best: 0.00300227141198857, Global best: 0.0029945178897519084, Runtime: 10.63787 seconds\n",
      "> Epoch: 1582, Current best: 0.0030312290114252356, Global best: 0.0029945178897519084, Runtime: 12.00951 seconds\n",
      "> Epoch: 1583, Current best: 0.0029980720380665383, Global best: 0.0029945178897519084, Runtime: 10.11316 seconds\n",
      "> Epoch: 1584, Current best: 0.0029842285506114508, Global best: 0.0029842285506114508, Runtime: 11.72092 seconds\n",
      "> Epoch: 1585, Current best: 0.002981904135179708, Global best: 0.002981904135179708, Runtime: 10.14712 seconds\n",
      "> Epoch: 1586, Current best: 0.002975690518424755, Global best: 0.002975690518424755, Runtime: 9.29666 seconds\n",
      "> Epoch: 1587, Current best: 0.0029780486543124858, Global best: 0.002975690518424755, Runtime: 9.22313 seconds\n",
      "> Epoch: 1588, Current best: 0.002972401577297295, Global best: 0.002972401577297295, Runtime: 9.58592 seconds\n",
      "> Epoch: 1589, Current best: 0.0029927056512709447, Global best: 0.002972401577297295, Runtime: 9.40154 seconds\n",
      "> Epoch: 1590, Current best: 0.0029704805619530524, Global best: 0.0029704805619530524, Runtime: 9.55702 seconds\n",
      "> Epoch: 1591, Current best: 0.0029773691652991443, Global best: 0.0029704805619530524, Runtime: 9.60187 seconds\n",
      "> Epoch: 1592, Current best: 0.002993336257371204, Global best: 0.0029704805619530524, Runtime: 12.48771 seconds\n",
      "> Epoch: 1593, Current best: 0.002972517196793139, Global best: 0.0029704805619530524, Runtime: 11.67152 seconds\n",
      "> Epoch: 1594, Current best: 0.002971418443950731, Global best: 0.0029704805619530524, Runtime: 9.88777 seconds\n",
      "> Epoch: 1595, Current best: 0.0029770412301661957, Global best: 0.0029704805619530524, Runtime: 9.16073 seconds\n",
      "> Epoch: 1596, Current best: 0.002966353225769005, Global best: 0.002966353225769005, Runtime: 9.42485 seconds\n",
      "> Epoch: 1597, Current best: 0.0029783025316922, Global best: 0.002966353225769005, Runtime: 11.30218 seconds\n",
      "> Epoch: 1598, Current best: 0.002968771505417889, Global best: 0.002966353225769005, Runtime: 12.33670 seconds\n",
      "> Epoch: 1599, Current best: 0.0029730732941696232, Global best: 0.002966353225769005, Runtime: 10.11664 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1600, Current best: 0.0029756081553669127, Global best: 0.002966353225769005, Runtime: 12.10748 seconds\n",
      "> Epoch: 1601, Current best: 0.003032695692155636, Global best: 0.002966353225769005, Runtime: 11.96737 seconds\n",
      "> Epoch: 1602, Current best: 0.0029644657548128957, Global best: 0.0029644657548128957, Runtime: 11.18556 seconds\n",
      "> Epoch: 1603, Current best: 0.002970572629621646, Global best: 0.0029644657548128957, Runtime: 11.77345 seconds\n",
      "> Epoch: 1604, Current best: 0.002970361320093365, Global best: 0.0029644657548128957, Runtime: 11.22351 seconds\n",
      "> Epoch: 1605, Current best: 0.0029680143397483373, Global best: 0.0029644657548128957, Runtime: 10.65948 seconds\n",
      "> Epoch: 1606, Current best: 0.0029269485262562894, Global best: 0.0029269485262562894, Runtime: 10.00253 seconds\n",
      "> Epoch: 1607, Current best: 0.0029250450677041724, Global best: 0.0029250450677041724, Runtime: 10.87945 seconds\n",
      "> Epoch: 1608, Current best: 0.002933529877968252, Global best: 0.0029250450677041724, Runtime: 12.32844 seconds\n",
      "> Epoch: 1609, Current best: 0.002928457879373853, Global best: 0.0029250450677041724, Runtime: 13.33354 seconds\n",
      "> Epoch: 1610, Current best: 0.002923451991719964, Global best: 0.002923451991719964, Runtime: 9.93580 seconds\n",
      "> Epoch: 1611, Current best: 0.0029090695587290242, Global best: 0.0029090695587290242, Runtime: 9.30088 seconds\n",
      "> Epoch: 1612, Current best: 0.0028959974241078087, Global best: 0.0028959974241078087, Runtime: 9.71204 seconds\n",
      "> Epoch: 1613, Current best: 0.002887988085298263, Global best: 0.002887988085298263, Runtime: 9.45462 seconds\n",
      "> Epoch: 1614, Current best: 0.0029132467591643646, Global best: 0.002887988085298263, Runtime: 9.41278 seconds\n",
      "> Epoch: 1615, Current best: 0.002889262638712516, Global best: 0.002887988085298263, Runtime: 9.27699 seconds\n",
      "> Epoch: 1616, Current best: 0.0028856043455518375, Global best: 0.0028856043455518375, Runtime: 9.35966 seconds\n",
      "> Epoch: 1617, Current best: 0.002889619267267037, Global best: 0.0028856043455518375, Runtime: 9.24620 seconds\n",
      "> Epoch: 1618, Current best: 0.0028914753914138247, Global best: 0.0028856043455518375, Runtime: 9.31594 seconds\n",
      "> Epoch: 1619, Current best: 0.002890378454625348, Global best: 0.0028856043455518375, Runtime: 9.58094 seconds\n",
      "> Epoch: 1620, Current best: 0.002887672138995752, Global best: 0.0028856043455518375, Runtime: 9.23909 seconds\n",
      "> Epoch: 1621, Current best: 0.002884975010470151, Global best: 0.002884975010470151, Runtime: 9.28193 seconds\n",
      "> Epoch: 1622, Current best: 0.0029131641139454245, Global best: 0.002884975010470151, Runtime: 9.22505 seconds\n",
      "> Epoch: 1623, Current best: 0.0028856301135204957, Global best: 0.002884975010470151, Runtime: 9.38806 seconds\n",
      "> Epoch: 1624, Current best: 0.0029000523207847154, Global best: 0.002884975010470151, Runtime: 12.13526 seconds\n",
      "> Epoch: 1625, Current best: 0.00288970461332285, Global best: 0.002884975010470151, Runtime: 10.62245 seconds\n",
      "> Epoch: 1626, Current best: 0.002884533774956893, Global best: 0.002884533774956893, Runtime: 9.51122 seconds\n",
      "> Epoch: 1627, Current best: 0.0027991589301716822, Global best: 0.0027991589301716822, Runtime: 9.39356 seconds\n",
      "> Epoch: 1628, Current best: 0.002771014703606637, Global best: 0.002771014703606637, Runtime: 10.11965 seconds\n",
      "> Epoch: 1629, Current best: 0.002779660282638029, Global best: 0.002771014703606637, Runtime: 11.59121 seconds\n",
      "> Epoch: 1630, Current best: 0.002760506676410422, Global best: 0.002760506676410422, Runtime: 10.78790 seconds\n",
      "> Epoch: 1631, Current best: 0.0027678036859811734, Global best: 0.002760506676410422, Runtime: 11.61712 seconds\n",
      "> Epoch: 1632, Current best: 0.002768098997106753, Global best: 0.002760506676410422, Runtime: 11.70687 seconds\n",
      "> Epoch: 1633, Current best: 0.002764143805324681, Global best: 0.002760506676410422, Runtime: 12.43721 seconds\n",
      "> Epoch: 1634, Current best: 0.0027619399047705143, Global best: 0.002760506676410422, Runtime: 15.06033 seconds\n",
      "> Epoch: 1635, Current best: 0.0027596139531702365, Global best: 0.0027596139531702365, Runtime: 12.19525 seconds\n",
      "> Epoch: 1636, Current best: 0.002755036789788025, Global best: 0.002755036789788025, Runtime: 12.32404 seconds\n",
      "> Epoch: 1637, Current best: 0.0027594909119709548, Global best: 0.002755036789788025, Runtime: 11.08547 seconds\n",
      "> Epoch: 1638, Current best: 0.002756715973795446, Global best: 0.002755036789788025, Runtime: 12.35009 seconds\n",
      "> Epoch: 1639, Current best: 0.0027615535932099485, Global best: 0.002755036789788025, Runtime: 11.50077 seconds\n",
      "> Epoch: 1640, Current best: 0.0027692361155018078, Global best: 0.002755036789788025, Runtime: 10.06432 seconds\n",
      "> Epoch: 1641, Current best: 0.002755925849196143, Global best: 0.002755036789788025, Runtime: 11.10384 seconds\n",
      "> Epoch: 1642, Current best: 0.002743068213676809, Global best: 0.002743068213676809, Runtime: 10.49887 seconds\n",
      "> Epoch: 1643, Current best: 0.0027374763091178285, Global best: 0.0027374763091178285, Runtime: 11.15622 seconds\n",
      "> Epoch: 1644, Current best: 0.002730907851690936, Global best: 0.002730907851690936, Runtime: 10.59853 seconds\n",
      "> Epoch: 1645, Current best: 0.0027247474632009186, Global best: 0.0027247474632009186, Runtime: 11.43883 seconds\n",
      "> Epoch: 1646, Current best: 0.0027172443941545442, Global best: 0.0027172443941545442, Runtime: 10.70721 seconds\n",
      "> Epoch: 1647, Current best: 0.00271524180546147, Global best: 0.00271524180546147, Runtime: 9.30311 seconds\n",
      "> Epoch: 1648, Current best: 0.002709815023242993, Global best: 0.002709815023242993, Runtime: 9.46382 seconds\n",
      "> Epoch: 1649, Current best: 0.0027081366730808596, Global best: 0.0027081366730808596, Runtime: 10.46932 seconds\n",
      "> Epoch: 1650, Current best: 0.002676917495978196, Global best: 0.002676917495978196, Runtime: 27.94943 seconds\n",
      "> Epoch: 1651, Current best: 0.002679004310763431, Global best: 0.002676917495978196, Runtime: 29.60874 seconds\n",
      "> Epoch: 1652, Current best: 0.0026794274990455534, Global best: 0.002676917495978196, Runtime: 29.16341 seconds\n",
      "> Epoch: 1653, Current best: 0.002658184052038832, Global best: 0.002658184052038832, Runtime: 29.69121 seconds\n",
      "> Epoch: 1654, Current best: 0.0026115308777601284, Global best: 0.0026115308777601284, Runtime: 29.60618 seconds\n",
      "> Epoch: 1655, Current best: 0.002603784128258893, Global best: 0.002603784128258893, Runtime: 29.09033 seconds\n",
      "> Epoch: 1656, Current best: 0.002607325827304261, Global best: 0.002603784128258893, Runtime: 29.74916 seconds\n",
      "> Epoch: 1657, Current best: 0.0026114060851965015, Global best: 0.002603784128258893, Runtime: 29.35062 seconds\n",
      "> Epoch: 1658, Current best: 0.002600359452338156, Global best: 0.002600359452338156, Runtime: 29.85882 seconds\n",
      "> Epoch: 1659, Current best: 0.002610084382541418, Global best: 0.002600359452338156, Runtime: 29.09050 seconds\n",
      "> Epoch: 1660, Current best: 0.0025995560956398153, Global best: 0.0025995560956398153, Runtime: 30.49317 seconds\n",
      "> Epoch: 1661, Current best: 0.0025891783354534893, Global best: 0.0025891783354534893, Runtime: 29.28473 seconds\n",
      "> Epoch: 1662, Current best: 0.0025899468565801077, Global best: 0.0025891783354534893, Runtime: 29.62132 seconds\n",
      "> Epoch: 1663, Current best: 0.002592994925840247, Global best: 0.0025891783354534893, Runtime: 30.69545 seconds\n",
      "> Epoch: 1664, Current best: 0.0025892452830986843, Global best: 0.0025891783354534893, Runtime: 30.00030 seconds\n",
      "> Epoch: 1665, Current best: 0.002596577554770871, Global best: 0.0025891783354534893, Runtime: 29.30869 seconds\n",
      "> Epoch: 1666, Current best: 0.0025893148150656887, Global best: 0.0025891783354534893, Runtime: 30.22448 seconds\n",
      "> Epoch: 1667, Current best: 0.0025927643055643386, Global best: 0.0025891783354534893, Runtime: 29.61900 seconds\n",
      "> Epoch: 1668, Current best: 0.0025904758502978357, Global best: 0.0025891783354534893, Runtime: 29.19396 seconds\n",
      "> Epoch: 1669, Current best: 0.0025893025674584226, Global best: 0.0025891783354534893, Runtime: 29.93296 seconds\n",
      "> Epoch: 1670, Current best: 0.0025852012247147885, Global best: 0.0025852012247147885, Runtime: 29.99671 seconds\n",
      "> Epoch: 1671, Current best: 0.002582802665742707, Global best: 0.002582802665742707, Runtime: 29.39392 seconds\n",
      "> Epoch: 1672, Current best: 0.0025838667784823673, Global best: 0.002582802665742707, Runtime: 29.09331 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1673, Current best: 0.0025849313993484, Global best: 0.002582802665742707, Runtime: 29.58914 seconds\n",
      "> Epoch: 1674, Current best: 0.002589816157725963, Global best: 0.002582802665742707, Runtime: 29.46825 seconds\n",
      "> Epoch: 1675, Current best: 0.002579566418527014, Global best: 0.002579566418527014, Runtime: 29.25220 seconds\n",
      "> Epoch: 1676, Current best: 0.002579212312241788, Global best: 0.002579212312241788, Runtime: 29.83481 seconds\n",
      "> Epoch: 1677, Current best: 0.002581025003271919, Global best: 0.002579212312241788, Runtime: 30.01120 seconds\n",
      "> Epoch: 1678, Current best: 0.0025795063837310154, Global best: 0.002579212312241788, Runtime: 29.29327 seconds\n",
      "> Epoch: 1679, Current best: 0.002591619815586869, Global best: 0.002579212312241788, Runtime: 29.23391 seconds\n",
      "> Epoch: 1680, Current best: 0.0025820772493387037, Global best: 0.002579212312241788, Runtime: 29.59157 seconds\n",
      "> Epoch: 1681, Current best: 0.002582165159067235, Global best: 0.002579212312241788, Runtime: 29.49096 seconds\n",
      "> Epoch: 1682, Current best: 0.0025691591263711034, Global best: 0.0025691591263711034, Runtime: 29.93827 seconds\n",
      "> Epoch: 1683, Current best: 0.0025347624460220273, Global best: 0.0025347624460220273, Runtime: 30.35767 seconds\n",
      "> Epoch: 1684, Current best: 0.0025340934032571545, Global best: 0.0025340934032571545, Runtime: 29.49978 seconds\n",
      "> Epoch: 1685, Current best: 0.0025156725707234542, Global best: 0.0025156725707234542, Runtime: 29.30988 seconds\n",
      "> Epoch: 1686, Current best: 0.002515433228960696, Global best: 0.002515433228960696, Runtime: 29.79436 seconds\n",
      "> Epoch: 1687, Current best: 0.0024962624882221935, Global best: 0.0024962624882221935, Runtime: 29.51352 seconds\n",
      "> Epoch: 1688, Current best: 0.0024973479066790595, Global best: 0.0024962624882221935, Runtime: 29.39118 seconds\n",
      "> Epoch: 1689, Current best: 0.0024969005562950975, Global best: 0.0024962624882221935, Runtime: 29.99213 seconds\n",
      "> Epoch: 1690, Current best: 0.0024906035029545466, Global best: 0.0024906035029545466, Runtime: 29.97044 seconds\n",
      "> Epoch: 1691, Current best: 0.002490763589861378, Global best: 0.0024906035029545466, Runtime: 29.56782 seconds\n",
      "> Epoch: 1692, Current best: 0.002480256128365982, Global best: 0.002480256128365982, Runtime: 29.27343 seconds\n",
      "> Epoch: 1693, Current best: 0.002484234554698895, Global best: 0.002480256128365982, Runtime: 29.72256 seconds\n",
      "> Epoch: 1694, Current best: 0.002485096546201059, Global best: 0.002480256128365982, Runtime: 29.47863 seconds\n",
      "> Epoch: 1695, Current best: 0.0024740554237669098, Global best: 0.0024740554237669098, Runtime: 30.24373 seconds\n",
      "> Epoch: 1696, Current best: 0.0024661624013162, Global best: 0.0024661624013162, Runtime: 29.78836 seconds\n",
      "> Epoch: 1697, Current best: 0.0024598460696199503, Global best: 0.0024598460696199503, Runtime: 29.62850 seconds\n",
      "> Epoch: 1698, Current best: 0.0024540406532773825, Global best: 0.0024540406532773825, Runtime: 29.19499 seconds\n",
      "> Epoch: 1699, Current best: 0.002440102020326925, Global best: 0.002440102020326925, Runtime: 30.04024 seconds\n",
      "> Epoch: 1700, Current best: 0.002439373530771205, Global best: 0.002439373530771205, Runtime: 29.58358 seconds\n",
      "> Epoch: 1701, Current best: 0.0024400287778220914, Global best: 0.002439373530771205, Runtime: 29.47192 seconds\n",
      "> Epoch: 1702, Current best: 0.0024389433021236536, Global best: 0.0024389433021236536, Runtime: 29.83592 seconds\n",
      "> Epoch: 1703, Current best: 0.002437324471732219, Global best: 0.002437324471732219, Runtime: 29.89823 seconds\n",
      "> Epoch: 1704, Current best: 0.0024358621240768074, Global best: 0.0024358621240768074, Runtime: 29.62970 seconds\n",
      "> Epoch: 1705, Current best: 0.0024264384043836133, Global best: 0.0024264384043836133, Runtime: 29.59633 seconds\n",
      "> Epoch: 1706, Current best: 0.002422169748731207, Global best: 0.002422169748731207, Runtime: 29.93156 seconds\n",
      "> Epoch: 1707, Current best: 0.0024109432320383387, Global best: 0.0024109432320383387, Runtime: 29.53429 seconds\n",
      "> Epoch: 1708, Current best: 0.0024113761630763894, Global best: 0.0024109432320383387, Runtime: 30.76230 seconds\n",
      "> Epoch: 1709, Current best: 0.00241052813452971, Global best: 0.00241052813452971, Runtime: 29.66409 seconds\n",
      "> Epoch: 1710, Current best: 0.002412911612195538, Global best: 0.00241052813452971, Runtime: 29.62465 seconds\n",
      "> Epoch: 1711, Current best: 0.002401825788224318, Global best: 0.002401825788224318, Runtime: 29.60038 seconds\n",
      "> Epoch: 1712, Current best: 0.0023954319107136147, Global best: 0.0023954319107136147, Runtime: 30.59967 seconds\n",
      "> Epoch: 1713, Current best: 0.002393716019042794, Global best: 0.002393716019042794, Runtime: 29.89381 seconds\n",
      "> Epoch: 1714, Current best: 0.002393539405069299, Global best: 0.002393539405069299, Runtime: 29.56254 seconds\n",
      "> Epoch: 1715, Current best: 0.0023950375220696913, Global best: 0.002393539405069299, Runtime: 30.06040 seconds\n",
      "> Epoch: 1716, Current best: 0.0023914917919130065, Global best: 0.0023914917919130065, Runtime: 30.33526 seconds\n",
      "> Epoch: 1717, Current best: 0.0023900718132522673, Global best: 0.0023900718132522673, Runtime: 30.23380 seconds\n",
      "> Epoch: 1718, Current best: 0.0023912749197290873, Global best: 0.0023900718132522673, Runtime: 29.55388 seconds\n",
      "> Epoch: 1719, Current best: 0.0023901898286064023, Global best: 0.0023900718132522673, Runtime: 29.76273 seconds\n",
      "> Epoch: 1720, Current best: 0.002389687682771811, Global best: 0.002389687682771811, Runtime: 29.52313 seconds\n",
      "> Epoch: 1721, Current best: 0.0023891096413333848, Global best: 0.0023891096413333848, Runtime: 30.19176 seconds\n",
      "> Epoch: 1722, Current best: 0.002391120413413962, Global best: 0.0023891096413333848, Runtime: 29.83327 seconds\n",
      "> Epoch: 1723, Current best: 0.0023895891032778157, Global best: 0.0023891096413333848, Runtime: 29.36194 seconds\n",
      "> Epoch: 1724, Current best: 0.00239369511623926, Global best: 0.0023891096413333848, Runtime: 29.64290 seconds\n",
      "> Epoch: 1725, Current best: 0.0023876708697378696, Global best: 0.0023876708697378696, Runtime: 29.65665 seconds\n",
      "> Epoch: 1726, Current best: 0.0023872866222298954, Global best: 0.0023872866222298954, Runtime: 29.65919 seconds\n",
      "> Epoch: 1727, Current best: 0.002388776115780138, Global best: 0.0023872866222298954, Runtime: 29.57541 seconds\n",
      "> Epoch: 1728, Current best: 0.002388547473660251, Global best: 0.0023872866222298954, Runtime: 30.06894 seconds\n",
      "> Epoch: 1729, Current best: 0.0023881494745774156, Global best: 0.0023872866222298954, Runtime: 29.47690 seconds\n",
      "> Epoch: 1730, Current best: 0.002391211098635408, Global best: 0.0023872866222298954, Runtime: 29.83496 seconds\n",
      "> Epoch: 1731, Current best: 0.0023879813268218144, Global best: 0.0023872866222298954, Runtime: 29.97612 seconds\n",
      "> Epoch: 1732, Current best: 0.0023767076239367834, Global best: 0.0023767076239367834, Runtime: 29.87010 seconds\n",
      "> Epoch: 1733, Current best: 0.002369186767357993, Global best: 0.002369186767357993, Runtime: 30.03969 seconds\n",
      "> Epoch: 1734, Current best: 0.0023617882206869007, Global best: 0.0023617882206869007, Runtime: 29.67940 seconds\n",
      "> Epoch: 1735, Current best: 0.002338001776910309, Global best: 0.002338001776910309, Runtime: 30.32941 seconds\n",
      "> Epoch: 1736, Current best: 0.002338168824838655, Global best: 0.002338001776910309, Runtime: 31.68830 seconds\n",
      "> Epoch: 1737, Current best: 0.0023395989830866633, Global best: 0.002338001776910309, Runtime: 31.18694 seconds\n",
      "> Epoch: 1738, Current best: 0.0023272323893327633, Global best: 0.0023272323893327633, Runtime: 31.36443 seconds\n",
      "> Epoch: 1739, Current best: 0.002320460430520935, Global best: 0.002320460430520935, Runtime: 31.41799 seconds\n",
      "> Epoch: 1740, Current best: 0.0023181135651146123, Global best: 0.0023181135651146123, Runtime: 31.44103 seconds\n",
      "> Epoch: 1741, Current best: 0.0023198747693224755, Global best: 0.0023181135651146123, Runtime: 30.40485 seconds\n",
      "> Epoch: 1742, Current best: 0.0023168533265028116, Global best: 0.0023168533265028116, Runtime: 29.55950 seconds\n",
      "> Epoch: 1743, Current best: 0.0023145813140533927, Global best: 0.0023145813140533927, Runtime: 30.29767 seconds\n",
      "> Epoch: 1744, Current best: 0.0023164004553202448, Global best: 0.0023145813140533927, Runtime: 30.28930 seconds\n",
      "> Epoch: 1745, Current best: 0.0023147735626519, Global best: 0.0023145813140533927, Runtime: 29.53021 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1746, Current best: 0.0023121154446791837, Global best: 0.0023121154446791837, Runtime: 29.90828 seconds\n",
      "> Epoch: 1747, Current best: 0.0023004159378564194, Global best: 0.0023004159378564194, Runtime: 29.66590 seconds\n",
      "> Epoch: 1748, Current best: 0.002298015309831259, Global best: 0.002298015309831259, Runtime: 30.30042 seconds\n",
      "> Epoch: 1749, Current best: 0.002298623697497244, Global best: 0.002298015309831259, Runtime: 30.35400 seconds\n",
      "> Epoch: 1750, Current best: 0.002298529079076894, Global best: 0.002298015309831259, Runtime: 29.73899 seconds\n",
      "> Epoch: 1751, Current best: 0.0022938283190285275, Global best: 0.0022938283190285275, Runtime: 30.42445 seconds\n",
      "> Epoch: 1752, Current best: 0.0022914372938639617, Global best: 0.0022914372938639617, Runtime: 29.86386 seconds\n",
      "> Epoch: 1753, Current best: 0.002288873292416526, Global best: 0.002288873292416526, Runtime: 30.04244 seconds\n",
      "> Epoch: 1754, Current best: 0.0022883679105722477, Global best: 0.0022883679105722477, Runtime: 29.67880 seconds\n",
      "> Epoch: 1755, Current best: 0.002290752974910291, Global best: 0.0022883679105722477, Runtime: 29.97765 seconds\n",
      "> Epoch: 1756, Current best: 0.0022912755469968655, Global best: 0.0022883679105722477, Runtime: 29.61494 seconds\n",
      "> Epoch: 1757, Current best: 0.0022891324750645044, Global best: 0.0022883679105722477, Runtime: 29.71396 seconds\n",
      "> Epoch: 1758, Current best: 0.002286110391617156, Global best: 0.002286110391617156, Runtime: 29.66848 seconds\n",
      "> Epoch: 1759, Current best: 0.0022852776139529957, Global best: 0.0022852776139529957, Runtime: 29.94091 seconds\n",
      "> Epoch: 1760, Current best: 0.002284857736346441, Global best: 0.002284857736346441, Runtime: 29.81044 seconds\n",
      "> Epoch: 1761, Current best: 0.002284623297865562, Global best: 0.002284623297865562, Runtime: 29.88711 seconds\n",
      "> Epoch: 1762, Current best: 0.002283936813132663, Global best: 0.002283936813132663, Runtime: 30.16471 seconds\n",
      "> Epoch: 1763, Current best: 0.002283199858592489, Global best: 0.002283199858592489, Runtime: 30.70676 seconds\n",
      "> Epoch: 1764, Current best: 0.0022779767147733418, Global best: 0.0022779767147733418, Runtime: 30.23539 seconds\n",
      "> Epoch: 1765, Current best: 0.0022769098802722905, Global best: 0.0022769098802722905, Runtime: 32.06422 seconds\n",
      "> Epoch: 1766, Current best: 0.0022746980453257778, Global best: 0.0022746980453257778, Runtime: 31.59972 seconds\n",
      "> Epoch: 1767, Current best: 0.002274040767755206, Global best: 0.002274040767755206, Runtime: 30.81058 seconds\n",
      "> Epoch: 1768, Current best: 0.002267286426560727, Global best: 0.002267286426560727, Runtime: 30.40764 seconds\n",
      "> Epoch: 1769, Current best: 0.0022596574377616646, Global best: 0.0022596574377616646, Runtime: 2040.17883 seconds\n",
      "> Epoch: 1770, Current best: 0.0022540063946093407, Global best: 0.0022540063946093407, Runtime: 15.42941 seconds\n",
      "> Epoch: 1771, Current best: 0.002253260162599828, Global best: 0.002253260162599828, Runtime: 15.37052 seconds\n",
      "> Epoch: 1772, Current best: 0.0022526255778500125, Global best: 0.0022526255778500125, Runtime: 16.54488 seconds\n",
      "> Epoch: 1773, Current best: 0.0022526480230057482, Global best: 0.0022526255778500125, Runtime: 16.41810 seconds\n",
      "> Epoch: 1774, Current best: 0.0022524206376738094, Global best: 0.0022524206376738094, Runtime: 17.43565 seconds\n",
      "> Epoch: 1775, Current best: 0.002246181427447479, Global best: 0.002246181427447479, Runtime: 26.61787 seconds\n",
      "> Epoch: 1776, Current best: 0.002244375715846521, Global best: 0.002244375715846521, Runtime: 30.20043 seconds\n",
      "> Epoch: 1777, Current best: 0.002234853514602946, Global best: 0.002234853514602946, Runtime: 29.81110 seconds\n",
      "> Epoch: 1778, Current best: 0.002233425625768139, Global best: 0.002233425625768139, Runtime: 29.61709 seconds\n",
      "> Epoch: 1779, Current best: 0.002232279511058243, Global best: 0.002232279511058243, Runtime: 31.00975 seconds\n",
      "> Epoch: 1780, Current best: 0.0022316130613170403, Global best: 0.0022316130613170403, Runtime: 29.51582 seconds\n",
      "> Epoch: 1781, Current best: 0.0022313117297617117, Global best: 0.0022313117297617117, Runtime: 29.81250 seconds\n",
      "> Epoch: 1782, Current best: 0.0022306249425361733, Global best: 0.0022306249425361733, Runtime: 30.09361 seconds\n",
      "> Epoch: 1783, Current best: 0.002230784277964966, Global best: 0.0022306249425361733, Runtime: 29.80007 seconds\n",
      "> Epoch: 1784, Current best: 0.0022300218698055006, Global best: 0.0022300218698055006, Runtime: 29.45060 seconds\n",
      "> Epoch: 1785, Current best: 0.0022298818091926165, Global best: 0.0022298818091926165, Runtime: 29.43498 seconds\n",
      "> Epoch: 1786, Current best: 0.0022296652924382854, Global best: 0.0022296652924382854, Runtime: 31.55389 seconds\n",
      "> Epoch: 1787, Current best: 0.0022292408892645435, Global best: 0.0022292408892645435, Runtime: 32.60617 seconds\n",
      "> Epoch: 1788, Current best: 0.0022287006053858744, Global best: 0.0022287006053858744, Runtime: 31.96088 seconds\n",
      "> Epoch: 1789, Current best: 0.0022285643426598074, Global best: 0.0022285643426598074, Runtime: 32.08900 seconds\n",
      "> Epoch: 1790, Current best: 0.002229676439905897, Global best: 0.0022285643426598074, Runtime: 31.59396 seconds\n",
      "> Epoch: 1791, Current best: 0.002224788327200199, Global best: 0.002224788327200199, Runtime: 15.82474 seconds\n",
      "> Epoch: 1792, Current best: 0.002225366315249981, Global best: 0.002224788327200199, Runtime: 10.48183 seconds\n",
      "> Epoch: 1793, Current best: 0.0022181965560998974, Global best: 0.0022181965560998974, Runtime: 10.35136 seconds\n",
      "> Epoch: 1794, Current best: 0.0022179603167150597, Global best: 0.0022179603167150597, Runtime: 10.54172 seconds\n",
      "> Epoch: 1795, Current best: 0.0022173670055887756, Global best: 0.0022173670055887756, Runtime: 10.38522 seconds\n",
      "> Epoch: 1796, Current best: 0.0022120326400935013, Global best: 0.0022120326400935013, Runtime: 10.97433 seconds\n",
      "> Epoch: 1797, Current best: 0.0022089948880641337, Global best: 0.0022089948880641337, Runtime: 10.67228 seconds\n",
      "> Epoch: 1798, Current best: 0.002205218163242109, Global best: 0.002205218163242109, Runtime: 10.59514 seconds\n",
      "> Epoch: 1799, Current best: 0.002203542350227351, Global best: 0.002203542350227351, Runtime: 10.97229 seconds\n",
      "> Epoch: 1800, Current best: 0.002202572061128532, Global best: 0.002202572061128532, Runtime: 10.65538 seconds\n",
      "> Epoch: 1801, Current best: 0.0021988483725787888, Global best: 0.0021988483725787888, Runtime: 10.99521 seconds\n",
      "> Epoch: 1802, Current best: 0.002191698411364959, Global best: 0.002191698411364959, Runtime: 10.59554 seconds\n",
      "> Epoch: 1803, Current best: 0.002186233668078589, Global best: 0.002186233668078589, Runtime: 10.16000 seconds\n",
      "> Epoch: 1804, Current best: 0.002182516073058882, Global best: 0.002182516073058882, Runtime: 10.65918 seconds\n",
      "> Epoch: 1805, Current best: 0.002182534911978355, Global best: 0.002182516073058882, Runtime: 10.77885 seconds\n",
      "> Epoch: 1806, Current best: 0.002182663540081433, Global best: 0.002182516073058882, Runtime: 10.61150 seconds\n",
      "> Epoch: 1807, Current best: 0.00218263740173945, Global best: 0.002182516073058882, Runtime: 10.20784 seconds\n",
      "> Epoch: 1808, Current best: 0.0021826504505709707, Global best: 0.002182516073058882, Runtime: 10.88265 seconds\n",
      "> Epoch: 1809, Current best: 0.002181576632082315, Global best: 0.002181576632082315, Runtime: 10.45401 seconds\n",
      "> Epoch: 1810, Current best: 0.0021814620695028417, Global best: 0.0021814620695028417, Runtime: 10.66930 seconds\n",
      "> Epoch: 1811, Current best: 0.0021814663068302135, Global best: 0.0021814620695028417, Runtime: 11.71380 seconds\n",
      "> Epoch: 1812, Current best: 0.002178322971018662, Global best: 0.002178322971018662, Runtime: 11.36692 seconds\n",
      "> Epoch: 1813, Current best: 0.0021772181896143154, Global best: 0.0021772181896143154, Runtime: 11.65101 seconds\n",
      "> Epoch: 1814, Current best: 0.002175230811838743, Global best: 0.002175230811838743, Runtime: 10.96234 seconds\n",
      "> Epoch: 1815, Current best: 0.002173889298897582, Global best: 0.002173889298897582, Runtime: 11.55736 seconds\n",
      "> Epoch: 1816, Current best: 0.0021736642141423706, Global best: 0.0021736642141423706, Runtime: 11.60419 seconds\n",
      "> Epoch: 1817, Current best: 0.002173425991616572, Global best: 0.002173425991616572, Runtime: 10.86657 seconds\n",
      "> Epoch: 1818, Current best: 0.002173087944772653, Global best: 0.002173087944772653, Runtime: 10.74009 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1819, Current best: 0.0021714907254490328, Global best: 0.0021714907254490328, Runtime: 11.64896 seconds\n",
      "> Epoch: 1820, Current best: 0.00217069621823814, Global best: 0.00217069621823814, Runtime: 11.33108 seconds\n",
      "> Epoch: 1821, Current best: 0.0021701179375623247, Global best: 0.0021701179375623247, Runtime: 11.55463 seconds\n",
      "> Epoch: 1822, Current best: 0.002162098283263673, Global best: 0.002162098283263673, Runtime: 11.29520 seconds\n",
      "> Epoch: 1823, Current best: 0.0021598320367722367, Global best: 0.0021598320367722367, Runtime: 12.05548 seconds\n",
      "> Epoch: 1824, Current best: 0.002155856943596628, Global best: 0.002155856943596628, Runtime: 12.86552 seconds\n",
      "> Epoch: 1825, Current best: 0.0021543964558682228, Global best: 0.0021543964558682228, Runtime: 11.25334 seconds\n",
      "> Epoch: 1826, Current best: 0.0021537187556979116, Global best: 0.0021537187556979116, Runtime: 11.03016 seconds\n",
      "> Epoch: 1827, Current best: 0.002153382984752522, Global best: 0.002153382984752522, Runtime: 10.20090 seconds\n",
      "> Epoch: 1828, Current best: 0.002152250702303292, Global best: 0.002152250702303292, Runtime: 10.03254 seconds\n",
      "> Epoch: 1829, Current best: 0.0021481098534339303, Global best: 0.0021481098534339303, Runtime: 9.84211 seconds\n",
      "> Epoch: 1830, Current best: 0.0021454090404166865, Global best: 0.0021454090404166865, Runtime: 10.12213 seconds\n",
      "> Epoch: 1831, Current best: 0.0021448240113930185, Global best: 0.0021448240113930185, Runtime: 10.02454 seconds\n",
      "> Epoch: 1832, Current best: 0.0021444628788581848, Global best: 0.0021444628788581848, Runtime: 9.84815 seconds\n",
      "> Epoch: 1833, Current best: 0.00214358730421748, Global best: 0.00214358730421748, Runtime: 10.34694 seconds\n",
      "> Epoch: 1834, Current best: 0.002144000095230286, Global best: 0.00214358730421748, Runtime: 10.03085 seconds\n",
      "> Epoch: 1835, Current best: 0.0021434687271271033, Global best: 0.0021434687271271033, Runtime: 10.08582 seconds\n",
      "> Epoch: 1836, Current best: 0.0021434583050207665, Global best: 0.0021434583050207665, Runtime: 10.21528 seconds\n",
      "> Epoch: 1837, Current best: 0.0021424319248514894, Global best: 0.0021424319248514894, Runtime: 9.83900 seconds\n",
      "> Epoch: 1838, Current best: 0.002140461697603611, Global best: 0.002140461697603611, Runtime: 9.99053 seconds\n",
      "> Epoch: 1839, Current best: 0.0021392407323830897, Global best: 0.0021392407323830897, Runtime: 10.64138 seconds\n",
      "> Epoch: 1840, Current best: 0.0021388262300791245, Global best: 0.0021388262300791245, Runtime: 9.73389 seconds\n",
      "> Epoch: 1841, Current best: 0.0021387038590765834, Global best: 0.0021387038590765834, Runtime: 9.92778 seconds\n",
      "> Epoch: 1842, Current best: 0.0021374842395900564, Global best: 0.0021374842395900564, Runtime: 9.60386 seconds\n",
      "> Epoch: 1843, Current best: 0.002137391712649048, Global best: 0.002137391712649048, Runtime: 9.77230 seconds\n",
      "> Epoch: 1844, Current best: 0.0021351605093774197, Global best: 0.0021351605093774197, Runtime: 9.66971 seconds\n",
      "> Epoch: 1845, Current best: 0.0021345958697943056, Global best: 0.0021345958697943056, Runtime: 10.08430 seconds\n",
      "> Epoch: 1846, Current best: 0.002133978080069387, Global best: 0.002133978080069387, Runtime: 9.85004 seconds\n",
      "> Epoch: 1847, Current best: 0.002133304400383288, Global best: 0.002133304400383288, Runtime: 9.47374 seconds\n",
      "> Epoch: 1848, Current best: 0.002128171371758565, Global best: 0.002128171371758565, Runtime: 9.53609 seconds\n",
      "> Epoch: 1849, Current best: 0.002119168285536261, Global best: 0.002119168285536261, Runtime: 9.47430 seconds\n",
      "> Epoch: 1850, Current best: 0.0021180234347771836, Global best: 0.0021180234347771836, Runtime: 10.01848 seconds\n",
      "> Epoch: 1851, Current best: 0.002116631872418522, Global best: 0.002116631872418522, Runtime: 9.73443 seconds\n",
      "> Epoch: 1852, Current best: 0.0021148918133218823, Global best: 0.0021148918133218823, Runtime: 9.62280 seconds\n",
      "> Epoch: 1853, Current best: 0.002114334306752884, Global best: 0.002114334306752884, Runtime: 9.40453 seconds\n",
      "> Epoch: 1854, Current best: 0.0021126338779639737, Global best: 0.0021126338779639737, Runtime: 9.62978 seconds\n",
      "> Epoch: 1855, Current best: 0.002105061927037576, Global best: 0.002105061927037576, Runtime: 9.67577 seconds\n",
      "> Epoch: 1856, Current best: 0.0020948376954016026, Global best: 0.0020948376954016026, Runtime: 9.38159 seconds\n",
      "> Epoch: 1857, Current best: 0.002094396656999351, Global best: 0.002094396656999351, Runtime: 9.83010 seconds\n",
      "> Epoch: 1858, Current best: 0.002091897060418962, Global best: 0.002091897060418962, Runtime: 9.69157 seconds\n",
      "> Epoch: 1859, Current best: 0.0020910691727663235, Global best: 0.0020910691727663235, Runtime: 9.37961 seconds\n",
      "> Epoch: 1860, Current best: 0.002089635197537227, Global best: 0.002089635197537227, Runtime: 9.49083 seconds\n",
      "> Epoch: 1861, Current best: 0.002088039287997798, Global best: 0.002088039287997798, Runtime: 9.65925 seconds\n",
      "> Epoch: 1862, Current best: 0.002086629427613421, Global best: 0.002086629427613421, Runtime: 9.72048 seconds\n",
      "> Epoch: 1863, Current best: 0.002086057981286303, Global best: 0.002086057981286303, Runtime: 10.16696 seconds\n",
      "> Epoch: 1864, Current best: 0.002051746135383849, Global best: 0.002051746135383849, Runtime: 9.99115 seconds\n",
      "> Epoch: 1865, Current best: 0.0020392233545028153, Global best: 0.0020392233545028153, Runtime: 9.52828 seconds\n",
      "> Epoch: 1866, Current best: 0.0020383569942452346, Global best: 0.0020383569942452346, Runtime: 9.56694 seconds\n",
      "> Epoch: 1867, Current best: 0.0020379648358966195, Global best: 0.0020379648358966195, Runtime: 9.63032 seconds\n",
      "> Epoch: 1868, Current best: 0.0020350870814594027, Global best: 0.0020350870814594027, Runtime: 9.50855 seconds\n",
      "> Epoch: 1869, Current best: 0.0020316957232989464, Global best: 0.0020316957232989464, Runtime: 9.77169 seconds\n",
      "> Epoch: 1870, Current best: 0.002029858784035294, Global best: 0.002029858784035294, Runtime: 9.81185 seconds\n",
      "> Epoch: 1871, Current best: 0.002026756115992777, Global best: 0.002026756115992777, Runtime: 9.53935 seconds\n",
      "> Epoch: 1872, Current best: 0.0020261391775301093, Global best: 0.0020261391775301093, Runtime: 9.46974 seconds\n",
      "> Epoch: 1873, Current best: 0.002024143561989408, Global best: 0.002024143561989408, Runtime: 9.83677 seconds\n",
      "> Epoch: 1874, Current best: 0.002023946953689635, Global best: 0.002023946953689635, Runtime: 12.73638 seconds\n",
      "> Epoch: 1875, Current best: 0.0020227007946028983, Global best: 0.0020227007946028983, Runtime: 10.08624 seconds\n",
      "> Epoch: 1876, Current best: 0.0020223636001742975, Global best: 0.0020223636001742975, Runtime: 9.95767 seconds\n",
      "> Epoch: 1877, Current best: 0.002022199537708774, Global best: 0.002022199537708774, Runtime: 9.61482 seconds\n",
      "> Epoch: 1878, Current best: 0.002022111625295047, Global best: 0.002022111625295047, Runtime: 9.77030 seconds\n",
      "> Epoch: 1879, Current best: 0.0020214792861265604, Global best: 0.0020214792861265604, Runtime: 9.84306 seconds\n",
      "> Epoch: 1880, Current best: 0.0020081322894299975, Global best: 0.0020081322894299975, Runtime: 9.54705 seconds\n",
      "> Epoch: 1881, Current best: 0.001999916470902248, Global best: 0.001999916470902248, Runtime: 10.54172 seconds\n",
      "> Epoch: 1882, Current best: 0.001996119021270523, Global best: 0.001996119021270523, Runtime: 9.78127 seconds\n",
      "> Epoch: 1883, Current best: 0.0019943592339735474, Global best: 0.0019943592339735474, Runtime: 9.69655 seconds\n",
      "> Epoch: 1884, Current best: 0.0019926658041884006, Global best: 0.0019926658041884006, Runtime: 9.53748 seconds\n",
      "> Epoch: 1885, Current best: 0.0019904721682704944, Global best: 0.0019904721682704944, Runtime: 9.59488 seconds\n",
      "> Epoch: 1886, Current best: 0.0019893644501116905, Global best: 0.0019893644501116905, Runtime: 9.44240 seconds\n",
      "> Epoch: 1887, Current best: 0.001987586334975442, Global best: 0.001987586334975442, Runtime: 9.66964 seconds\n",
      "> Epoch: 1888, Current best: 0.001987393898158845, Global best: 0.001987393898158845, Runtime: 9.94276 seconds\n",
      "> Epoch: 1889, Current best: 0.0019869544974357826, Global best: 0.0019869544974357826, Runtime: 9.51416 seconds\n",
      "> Epoch: 1890, Current best: 0.001985352995384082, Global best: 0.001985352995384082, Runtime: 9.40453 seconds\n",
      "> Epoch: 1891, Current best: 0.0019845434495234667, Global best: 0.0019845434495234667, Runtime: 9.49921 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1892, Current best: 0.0019837761447825666, Global best: 0.0019837761447825666, Runtime: 9.52014 seconds\n",
      "> Epoch: 1893, Current best: 0.0019825271755713864, Global best: 0.0019825271755713864, Runtime: 9.61881 seconds\n",
      "> Epoch: 1894, Current best: 0.0019796193261826618, Global best: 0.0019796193261826618, Runtime: 9.77528 seconds\n",
      "> Epoch: 1895, Current best: 0.0019784594503084512, Global best: 0.0019784594503084512, Runtime: 9.51715 seconds\n",
      "> Epoch: 1896, Current best: 0.0019779329054978494, Global best: 0.0019779329054978494, Runtime: 9.51616 seconds\n",
      "> Epoch: 1897, Current best: 0.001977868427917162, Global best: 0.001977868427917162, Runtime: 9.66247 seconds\n",
      "> Epoch: 1898, Current best: 0.0019761429536700923, Global best: 0.0019761429536700923, Runtime: 9.49323 seconds\n",
      "> Epoch: 1899, Current best: 0.001975243312452871, Global best: 0.001975243312452871, Runtime: 9.41450 seconds\n",
      "> Epoch: 1900, Current best: 0.0019746895869586223, Global best: 0.0019746895869586223, Runtime: 9.99848 seconds\n",
      "> Epoch: 1901, Current best: 0.0019740812592605283, Global best: 0.0019740812592605283, Runtime: 9.76839 seconds\n",
      "> Epoch: 1902, Current best: 0.0019717121480455208, Global best: 0.0019717121480455208, Runtime: 9.47031 seconds\n",
      "> Epoch: 1903, Current best: 0.0019708213216845035, Global best: 0.0019708213216845035, Runtime: 9.71250 seconds\n",
      "> Epoch: 1904, Current best: 0.001969193207788637, Global best: 0.001969193207788637, Runtime: 9.44938 seconds\n",
      "> Epoch: 1905, Current best: 0.001968314396346487, Global best: 0.001968314396346487, Runtime: 9.71648 seconds\n",
      "> Epoch: 1906, Current best: 0.001967684967989026, Global best: 0.001967684967989026, Runtime: 9.68658 seconds\n",
      "> Epoch: 1907, Current best: 0.0019668657423491517, Global best: 0.0019668657423491517, Runtime: 10.39328 seconds\n",
      "> Epoch: 1908, Current best: 0.001966385281646675, Global best: 0.001966385281646675, Runtime: 9.77702 seconds\n",
      "> Epoch: 1909, Current best: 0.0019652490620736427, Global best: 0.0019652490620736427, Runtime: 9.51820 seconds\n",
      "> Epoch: 1910, Current best: 0.001964360094062132, Global best: 0.001964360094062132, Runtime: 9.58596 seconds\n",
      "> Epoch: 1911, Current best: 0.001963561257828931, Global best: 0.001963561257828931, Runtime: 9.79237 seconds\n",
      "> Epoch: 1912, Current best: 0.0019629587215175635, Global best: 0.0019629587215175635, Runtime: 9.57799 seconds\n",
      "> Epoch: 1913, Current best: 0.0019625076218063283, Global best: 0.0019625076218063283, Runtime: 10.49996 seconds\n",
      "> Epoch: 1914, Current best: 0.0019622725881925213, Global best: 0.0019622725881925213, Runtime: 9.84181 seconds\n",
      "> Epoch: 1915, Current best: 0.0019622321536476046, Global best: 0.0019622321536476046, Runtime: 9.69361 seconds\n",
      "> Epoch: 1916, Current best: 0.001962015503903403, Global best: 0.001962015503903403, Runtime: 9.54126 seconds\n",
      "> Epoch: 1917, Current best: 0.0019603200060221765, Global best: 0.0019603200060221765, Runtime: 9.79725 seconds\n",
      "> Epoch: 1918, Current best: 0.001959912368909172, Global best: 0.001959912368909172, Runtime: 9.74539 seconds\n",
      "> Epoch: 1919, Current best: 0.0019598910779463343, Global best: 0.0019598910779463343, Runtime: 10.28359 seconds\n",
      "> Epoch: 1920, Current best: 0.001959699394829486, Global best: 0.001959699394829486, Runtime: 9.97163 seconds\n",
      "> Epoch: 1921, Current best: 0.001959461891625591, Global best: 0.001959461891625591, Runtime: 9.77935 seconds\n",
      "> Epoch: 1922, Current best: 0.0019590344309799845, Global best: 0.0019590344309799845, Runtime: 9.89546 seconds\n",
      "> Epoch: 1923, Current best: 0.0019588577068196953, Global best: 0.0019588577068196953, Runtime: 9.67163 seconds\n",
      "> Epoch: 1924, Current best: 0.0019584832171209267, Global best: 0.0019584832171209267, Runtime: 9.72845 seconds\n",
      "> Epoch: 1925, Current best: 0.0019579797116310855, Global best: 0.0019579797116310855, Runtime: 10.22281 seconds\n",
      "> Epoch: 1926, Current best: 0.001953747063347482, Global best: 0.001953747063347482, Runtime: 9.83110 seconds\n",
      "> Epoch: 1927, Current best: 0.0019498058172034293, Global best: 0.0019498058172034293, Runtime: 10.03841 seconds\n",
      "> Epoch: 1928, Current best: 0.0019487722448610827, Global best: 0.0019487722448610827, Runtime: 9.90187 seconds\n",
      "> Epoch: 1929, Current best: 0.0019482518546516988, Global best: 0.0019482518546516988, Runtime: 10.01249 seconds\n",
      "> Epoch: 1930, Current best: 0.001948232592223474, Global best: 0.001948232592223474, Runtime: 10.30153 seconds\n",
      "> Epoch: 1931, Current best: 0.0019386418049641035, Global best: 0.0019386418049641035, Runtime: 10.27262 seconds\n",
      "> Epoch: 1932, Current best: 0.001937129948477422, Global best: 0.001937129948477422, Runtime: 9.77927 seconds\n",
      "> Epoch: 1933, Current best: 0.0019364513853194515, Global best: 0.0019364513853194515, Runtime: 9.63912 seconds\n",
      "> Epoch: 1934, Current best: 0.0019359503135904376, Global best: 0.0019359503135904376, Runtime: 10.37428 seconds\n",
      "> Epoch: 1935, Current best: 0.0019356892820358179, Global best: 0.0019356892820358179, Runtime: 9.69356 seconds\n",
      "> Epoch: 1936, Current best: 0.0019350194346857779, Global best: 0.0019350194346857779, Runtime: 9.73342 seconds\n",
      "> Epoch: 1937, Current best: 0.0019310689246679108, Global best: 0.0019310689246679108, Runtime: 9.91781 seconds\n",
      "> Epoch: 1938, Current best: 0.0019292174626283495, Global best: 0.0019292174626283495, Runtime: 9.83415 seconds\n",
      "> Epoch: 1939, Current best: 0.001928668421181089, Global best: 0.001928668421181089, Runtime: 9.58393 seconds\n",
      "> Epoch: 1940, Current best: 0.0019278593853345033, Global best: 0.0019278593853345033, Runtime: 9.64971 seconds\n",
      "> Epoch: 1941, Current best: 0.0019274240747030973, Global best: 0.0019274240747030973, Runtime: 9.56200 seconds\n",
      "> Epoch: 1942, Current best: 0.0019271151788918753, Global best: 0.0019271151788918753, Runtime: 9.76145 seconds\n",
      "> Epoch: 1943, Current best: 0.0019268514889101568, Global best: 0.0019268514889101568, Runtime: 10.43811 seconds\n",
      "> Epoch: 1944, Current best: 0.001926434683935296, Global best: 0.001926434683935296, Runtime: 10.13917 seconds\n",
      "> Epoch: 1945, Current best: 0.0019261463645743762, Global best: 0.0019261463645743762, Runtime: 9.99117 seconds\n",
      "> Epoch: 1946, Current best: 0.001925795509405383, Global best: 0.001925795509405383, Runtime: 9.84008 seconds\n",
      "> Epoch: 1947, Current best: 0.0019253700657865126, Global best: 0.0019253700657865126, Runtime: 9.73941 seconds\n",
      "> Epoch: 1948, Current best: 0.0019250419335318978, Global best: 0.0019250419335318978, Runtime: 9.64871 seconds\n",
      "> Epoch: 1949, Current best: 0.001923654732640589, Global best: 0.001923654732640589, Runtime: 10.09622 seconds\n",
      "> Epoch: 1950, Current best: 0.0019234202016175986, Global best: 0.0019234202016175986, Runtime: 9.60984 seconds\n",
      "> Epoch: 1951, Current best: 0.0019231996166120635, Global best: 0.0019231996166120635, Runtime: 9.96119 seconds\n",
      "> Epoch: 1952, Current best: 0.00192308253681385, Global best: 0.00192308253681385, Runtime: 9.88734 seconds\n",
      "> Epoch: 1953, Current best: 0.0019227176101582664, Global best: 0.0019227176101582664, Runtime: 9.66718 seconds\n",
      "> Epoch: 1954, Current best: 0.0019222896391953284, Global best: 0.0019222896391953284, Runtime: 13.82132 seconds\n",
      "> Epoch: 1955, Current best: 0.0019219308700302875, Global best: 0.0019219308700302875, Runtime: 10.24952 seconds\n",
      "> Epoch: 1956, Current best: 0.001921604072174022, Global best: 0.001921604072174022, Runtime: 9.91183 seconds\n",
      "> Epoch: 1957, Current best: 0.0019212959091907734, Global best: 0.0019212959091907734, Runtime: 10.49093 seconds\n",
      "> Epoch: 1958, Current best: 0.001920805957588385, Global best: 0.001920805957588385, Runtime: 10.21720 seconds\n",
      "> Epoch: 1959, Current best: 0.0019206295704952902, Global best: 0.0019206295704952902, Runtime: 11.46664 seconds\n",
      "> Epoch: 1960, Current best: 0.0019202554402206876, Global best: 0.0019202554402206876, Runtime: 10.94290 seconds\n",
      "> Epoch: 1961, Current best: 0.0019194306099267766, Global best: 0.0019194306099267766, Runtime: 9.94378 seconds\n",
      "> Epoch: 1962, Current best: 0.0019143389929041946, Global best: 0.0019143389929041946, Runtime: 9.97762 seconds\n",
      "> Epoch: 1963, Current best: 0.0019119264371575326, Global best: 0.0019119264371575326, Runtime: 9.93669 seconds\n",
      "> Epoch: 1964, Current best: 0.0019109710619419162, Global best: 0.0019109710619419162, Runtime: 9.67562 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Epoch: 1965, Current best: 0.0019096686148377742, Global best: 0.0019096686148377742, Runtime: 10.46950 seconds\n",
      "> Epoch: 1966, Current best: 0.0019077812566029725, Global best: 0.0019077812566029725, Runtime: 10.42418 seconds\n",
      "> Epoch: 1967, Current best: 0.0019075739732865014, Global best: 0.0019075739732865014, Runtime: 10.27462 seconds\n",
      "> Epoch: 1968, Current best: 0.0019073937789818305, Global best: 0.0019073937789818305, Runtime: 10.27435 seconds\n",
      "> Epoch: 1969, Current best: 0.0019072663488398725, Global best: 0.0019072663488398725, Runtime: 10.15648 seconds\n",
      "> Epoch: 1970, Current best: 0.0019071129490537042, Global best: 0.0019071129490537042, Runtime: 10.14006 seconds\n",
      "> Epoch: 1971, Current best: 0.0019069773354752969, Global best: 0.0019069773354752969, Runtime: 10.12018 seconds\n",
      "> Epoch: 1972, Current best: 0.0019068474682134421, Global best: 0.0019068474682134421, Runtime: 10.02739 seconds\n",
      "> Epoch: 1973, Current best: 0.0019067262069095011, Global best: 0.0019067262069095011, Runtime: 9.89987 seconds\n",
      "> Epoch: 1974, Current best: 0.0019065798115561344, Global best: 0.0019065798115561344, Runtime: 10.46996 seconds\n",
      "> Epoch: 1975, Current best: 0.0019064682067231932, Global best: 0.0019064682067231932, Runtime: 10.43388 seconds\n",
      "> Epoch: 1976, Current best: 0.0019063907550302005, Global best: 0.0019063907550302005, Runtime: 10.28259 seconds\n",
      "> Epoch: 1977, Current best: 0.0019062828069564184, Global best: 0.0019062828069564184, Runtime: 10.09228 seconds\n",
      "> Epoch: 1978, Current best: 0.0019061540033129315, Global best: 0.0019061540033129315, Runtime: 10.58152 seconds\n",
      "> Epoch: 1979, Current best: 0.001906081345938097, Global best: 0.001906081345938097, Runtime: 9.80826 seconds\n",
      "> Epoch: 1980, Current best: 0.001905423434466027, Global best: 0.001905423434466027, Runtime: 9.70452 seconds\n",
      "> Epoch: 1981, Current best: 0.0019020458697313575, Global best: 0.0019020458697313575, Runtime: 10.00887 seconds\n",
      "> Epoch: 1982, Current best: 0.001901772644470507, Global best: 0.001901772644470507, Runtime: 9.86662 seconds\n",
      "> Epoch: 1983, Current best: 0.0018972071988289474, Global best: 0.0018972071988289474, Runtime: 9.84645 seconds\n",
      "> Epoch: 1984, Current best: 0.0018955871977822008, Global best: 0.0018955871977822008, Runtime: 10.44365 seconds\n",
      "> Epoch: 1985, Current best: 0.0018950612390934436, Global best: 0.0018950612390934436, Runtime: 9.84007 seconds\n",
      "> Epoch: 1986, Current best: 0.0018948632655715486, Global best: 0.0018948632655715486, Runtime: 10.50584 seconds\n",
      "> Epoch: 1987, Current best: 0.001892597253113166, Global best: 0.001892597253113166, Runtime: 10.46005 seconds\n",
      "> Epoch: 1988, Current best: 0.0018920147183684821, Global best: 0.0018920147183684821, Runtime: 10.09123 seconds\n",
      "> Epoch: 1989, Current best: 0.001891408495317093, Global best: 0.001891408495317093, Runtime: 10.00253 seconds\n",
      "> Epoch: 1990, Current best: 0.001891338615941101, Global best: 0.001891338615941101, Runtime: 10.07329 seconds\n",
      "> Epoch: 1991, Current best: 0.0018911500432962497, Global best: 0.0018911500432962497, Runtime: 9.87296 seconds\n",
      "> Epoch: 1992, Current best: 0.0018909384461885076, Global best: 0.0018909384461885076, Runtime: 10.14932 seconds\n",
      "> Epoch: 1993, Current best: 0.0018907879281656688, Global best: 0.0018907879281656688, Runtime: 10.21916 seconds\n",
      "> Epoch: 1994, Current best: 0.0018894275495732595, Global best: 0.0018894275495732595, Runtime: 10.52877 seconds\n",
      "> Epoch: 1995, Current best: 0.0018859541608247463, Global best: 0.0018859541608247463, Runtime: 10.49487 seconds\n",
      "> Epoch: 1996, Current best: 0.0018845484139109016, Global best: 0.0018845484139109016, Runtime: 10.24775 seconds\n",
      "> Epoch: 1997, Current best: 0.0018832762858250654, Global best: 0.0018832762858250654, Runtime: 10.01344 seconds\n",
      "> Epoch: 1998, Current best: 0.001882711068176342, Global best: 0.001882711068176342, Runtime: 10.02728 seconds\n",
      "> Epoch: 1999, Current best: 0.0018823213353696985, Global best: 0.0018823213353696985, Runtime: 9.88561 seconds\n",
      "> Epoch: 2000, Current best: 0.0018823213353696985, Global best: 0.0018823213353696985, Runtime: 10.14808 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 7.29216915e+00,  3.57598120e-03, -9.79810378e+00,  9.06213667e-01,\n",
       "         4.40119749e+00,  2.22029954e-01, -3.47704635e+00,  1.00000000e+01,\n",
       "         5.37921802e-01,  4.26314550e+00, -1.15398022e-01, -3.32616848e+00,\n",
       "        -5.77941754e+00, -2.80670232e+00, -3.76682702e-01,  6.96198251e+00,\n",
       "         9.67537591e+00,  1.54535044e+00, -1.29972881e-02, -1.41288487e-01,\n",
       "         1.79332585e-04,  1.85374163e+00, -8.48849048e-01, -5.40665766e+00,\n",
       "         4.68282588e+00,  3.07396915e+00, -9.35006831e+00,  1.84936190e+00,\n",
       "         1.13459393e-02,  7.50229143e+00]),\n",
       " 0.0018823213353696985)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def get_weights(w):\n",
    "        y_to_do = y_train_all_preds_array\n",
    "        y_true = y_clean\n",
    "        w_arit = w\n",
    "        #w_geom =  w[30:60]\n",
    "        #w_harm = w[60:90]\n",
    "        #w1 = w[90]\n",
    "        #w2 = w[91]\n",
    "        #w3 = w[93]\n",
    "        arit_preds = np.mean(w_arit * y_to_do, axis = 1)\n",
    "        #geom_preds = np.mean(w_geom * y_to_do, axis = 0)\n",
    "        #harm_preds = np.mean(harm_preds * y_to_do, axis = 0)\n",
    "        #all_pred = w1 * arit_preds + w2 * geom_preds + w3 * harm_preds\n",
    "        target_score = mean_squared_error(y_true, arit_preds)\n",
    "        return target_score\n",
    "\n",
    "\n",
    "\n",
    "from mealpy.bio_based import SMA\n",
    "from mealpy.evolutionary_based import DE\n",
    "from mealpy.math_based import HC\n",
    "\n",
    "problem_dict1 = {\n",
    "    \"obj_func\": get_weights,\n",
    "    \"lb\": [-10.0, ] * 30,\n",
    "    \"ub\": [10.0, ] * 30,\n",
    "    \"minmax\": \"min\",\n",
    "    \"verbose\": True}\n",
    "\n",
    "optmod = SMA.BaseSMA(problem_dict1, epoch = 2000, pop_size = 2000)\n",
    "optmod.solve()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = optmod.solution[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
